{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SampMark/ETL-de-dados-da-PNP/blob/main/GitHub_ETL_of_PNP_Data___Pipeline_for_BigQuery___public_servants_dimension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extração da Plataforma Nilo Peçanha (PNP), filtragem, tratamento e armazenamento em Big Query dos dados de Instituições Federais de Ensino (IFES) - tratamento para dimensão `servidores`**\n",
        "\n",
        "Este notebook automatiza a extração, tratamento e carga (ETL) dos microdados da Plataforma Nilo Peçanha (PNP) para o Google BigQuery. O fluxo foi projetado em etapas sequenciais para garantir a integridade e a qualidade dos dados.\n",
        "\n",
        "## **Fluxo de Execução**\n",
        "\n",
        "1. **Etapa 1: Instalação e Autenticação**: instalação das bibliotecas Python necessárias para a manipulação dos dados e conexão com os serviços Google (`gspread`, `pandas-gbq`, etc.). Autenticação do usuário e montagem do Google Drive para acesso e armazenamento dos arquivos de dados.\n",
        "2. **Etapa 2: Definição das Funções Principais**: carregamento das funções em memória que realizam as principais tarefas do pipeline: download, descompressão, análise de cabeçalhos, processamento e tratamento dos dados.\n",
        "3. **Etapa 3: Configuração do Pipeline**: o usuário define os parâmetros da extração através de uma interface interativa:\n",
        "\n",
        "    * **Período**: define o intervalo de anos (Ano Inicial e Final) para a extração.\n",
        "    * **Instituição(ões)**: filtra os dados para uma ou mais instituições específicas.\n",
        "    * **Força atualização**: opção para baixar novamente os arquivos da PNP, ignorando o cache local no Google Drive.\n",
        "\n",
        "4. **Etapa 4: Download e Análise de Cabeçalhos**: o script baixa os arquivos de dados compactados (`.gz`) da PNP para o Google Drive com base nas configurações da etapa anterior. Em seguida, os arquivos são descompactados e o script realiza uma análise comparativa dos cabeçalhos (colunas) de cada ano, exibindo uma tabela que destaca as diferenças.\n",
        "    * **Tabela com o comparativo de cabeçalhos dos CSV processados**: permite a analise prévia das colunas a serem extraídas do conjunto de dados (ex: matriculas, servidores).\n",
        "    * **Ponto de Decisão**: é gerada uma lista com as colunas comuns a todos os arquivos do período, que servirá de sugestão para a próxima etapa.\n",
        "\n",
        "5. **Etapa 5: Seleção de Colunas e Processamento dos Dados**: esta etapa demanda uma **Ação do Usuário** que deve copiar a lista de colunas sugerida e editá-la conforme a necessidade, em seguida inserir na célula de código desta etapa. O script então processa os arquivos CSV, unificando-os em um único DataFrame (`df_filtrado`), mantendo apenas as colunas selecionadas e aplicando o filtro de instituição.\n",
        "\n",
        "6. **Etapa 6: Tratamento e Limpeza dos Dados** é uma fase crucial do pipeline em que o DataFrame bruto (`df_filtrado`), que contém os dados extraídos de múltiplos anos, é transformado em um conjunto de dados \"limpo\", padronizado e enriquecido (`df_tratado`), pronto para análise, conforme implementação a seguir:\n",
        "    *  **Renomeação de Colunas:** a maioria da colunas foirenomeadas para um padrão consistente (removendo espaços e caracteres especiais) para garantir a compatibilidade com o BigQuery e facilitar o acesso no código. Por exemplo, `Jornada de Trabalho` foi alterado para `Jornada_de_Trabalho`.\n",
        "    *  **Padronização de Valores:** foram corrigidas inconsistências em colunas categóricas. Por exemplo, na coluna `Vinculo_Contrato`, o valor `Substituto/Temporario` foi corrigido para `Substituto/Temporário` , e na coluna `Titulacao`, `Aperfeicoamento` foi ajustado para `Aperfeiçoamento`.\n",
        "    * **Limpeza dos Nomes dos Campi:** Os nomes na coluna `Campus_do_IFRN` foram limpos, removendo prefixos como \"Campus \" e \"Campus Avançado \" para simplificar a visualização (ex: \"Campus Natal Central\" tornou-se \"Natal Central\").\n",
        "    *  **Criação de uma Nova Coluna (`Nivel_Educacional`):** Uma nova coluna chamada `Nivel_Educacional` foi criada a partir da coluna `Titulacao`, apenas para agrupar as diversas titulações em categorias hierárquicas mais amplas, como `(0) Educação Básica`, `(2) Educação Superior` e `(3) Pós-graduação`, facilitando análises agregadas sobre o nível de formação dos servidores.\n",
        "7. **Etapa 7: Análise e Exportação para o BigQuery**: são realizadas análises estatísticas descritivas e de _outliers_ sobre os dados tratados para verificar a qualidade final. Finalmente, o DataFrame (`df_tratado`) é exportado para uma tabela no Google BigQuery, finalizando o processo de ETL. O script gera um link direto para a tabela criada.\n",
        "\n",
        "<img src=\"https://www2.ifal.edu.br/noticias/ifal-se-destaca-na-eficiencia-academica-dos-institutos-federais-do-nordeste/plataforma-nilo-pecanha/@@images/98c1a2a4-6c59-436f-bdce-effa7ae4d539.jpeg\" alt=\"Logo da Plataforma Nilo Peçanha\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "sq1VuFUOqieo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Extração de microdados da PNP para Big Query\n",
        "\n",
        "Este notebook automatiza o fluxo de trabalho com os microdados extraídos da Plataforma Nilo Peçanha (PNP),\n",
        "permitindo a extração de diferentes tabelas e a análise de seus cabeçalhos em cada anos.\n",
        "\"\"\"\n",
        "\n",
        "# @title **ETAPA 1: Instalação de dependências, importações e autenticação do usuário no Google Drive**\n",
        "\n",
        "# Instalação de Dependências\n",
        "!pip install gspread gspread-dataframe oauth2client pandas-gbq --quiet\n",
        "print(\"Dependências instaladas com sucesso!\")"
      ],
      "metadata": {
        "id": "nYWoq3EYq7FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação de bibliotecas\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import gspread\n",
        "import gzip\n",
        "import shutil\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import auth, drive\n",
        "from google.auth import default\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Set"
      ],
      "metadata": {
        "id": "QmUMUGuKq9yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autenticação e Montagem do Google Drive\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"\\nAutenticação e montagem do Google Drive realizadas com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante a autenticação ou montagem do Drive: {e}\")"
      ],
      "metadata": {
        "id": "FJq__1gYq_UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 2: Definição das Funções Principais**\n",
        "\n",
        "def download_pnp_data(table_name: str, start_year: int, end_year: int, force_update: bool):\n",
        "    \"\"\"\n",
        "    Baixa os arquivos da PNP para uma pasta específica no Google Drive.\n",
        "    O nome do arquivo e a pasta de destino são baseados no table_name.\n",
        "    \"\"\"\n",
        "    drive_folder = Path(f'/content/drive/MyDrive/Coisas do IFRN/Prodes/Indicadores/PNP/{table_name.capitalize()}')\n",
        "    drive_folder.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Verificando arquivos na pasta do Google Drive: {drive_folder}\")\n",
        "\n",
        "    base_url = \"https://d236w85zd3t8iw.cloudfront.net/pnp-tests/microdados\"\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        file_name = f\"microdados_{table_name}_{year}.csv.gz\"\n",
        "        url = f\"{base_url}/{year}/{file_name}\"\n",
        "        destination = drive_folder / file_name\n",
        "\n",
        "        if not force_update and destination.exists():\n",
        "            print(f\"✔ O arquivo para {year} ('{file_name}') já existe. Usando o cache do Drive.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f\"⬇ Baixando dados para {year} de {url}...\")\n",
        "            with requests.get(url, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(destination, 'wb') as f:\n",
        "                    shutil.copyfileobj(r.raw, f)\n",
        "            print(f\"✔ Download de {year} concluído com sucesso.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Falha ao baixar o arquivo para {year}. Erro: {e}. O arquivo pode não existir para este ano.\")\n",
        "\n",
        "def decompress_gz_to_csv(gz_path: Path, out_dir: Path) -> Path:\n",
        "    \"\"\"Descompacta cada arquivo .gz para a pasta de trabalho\"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # Remove a extensão .gz para obter o nome do arquivo CSV\n",
        "    csv_out_path = out_dir / gz_path.with_suffix(\"\").name\n",
        "    print(f\"Descompactando: {gz_path.name} -> {csv_out_path.name}\")\n",
        "    with gzip.open(gz_path, \"rb\") as f_in, open(csv_out_path, \"wb\") as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "    return csv_out_path\n",
        "\n",
        "def get_header(path: Path, sep: str = ';') -> List[str]:\n",
        "    \"\"\"Lê o cabeçalho de cada arquivo CSV descompactado\"\"\"\n",
        "    try:\n",
        "        return list(pd.read_csv(path, nrows=0, sep=sep, engine='python').columns)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler o cabeçalho de {path.name}: {e}\")\n",
        "        return []\n",
        "\n",
        "def analyze_and_compare_headers(csv_paths: List[Path]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria um DataFrame comparativo de cabeçalhos e sugere colunas comuns.\n",
        "    \"\"\"\n",
        "    if not csv_paths:\n",
        "        print(\"Nenhum arquivo CSV para analisar.\")\n",
        "        return pd.DataFrame(), []\n",
        "\n",
        "    headers_dict = {path.name: get_header(path) for path in csv_paths}\n",
        "\n",
        "    # Identificar colunas comuns\n",
        "    sets_of_headers = [set(h) for h in headers_dict.values() if h]\n",
        "    if not sets_of_headers:\n",
        "        print(\"Não foi possível ler nenhum cabeçalho.\")\n",
        "        return pd.DataFrame(), []\n",
        "\n",
        "    common_columns = sorted(list(sets_of_headers[0].intersection(*sets_of_headers[1:])))\n",
        "\n",
        "    # Criar DataFrame para comparação visual\n",
        "    all_columns = sorted(list(set.union(*sets_of_headers)))\n",
        "    comparison_data = {}\n",
        "    for col in all_columns:\n",
        "        comparison_data[col] = [(\"✔\" if col in headers_dict.get(fname, []) else \"❌\") for fname in headers_dict.keys()]\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data, index=headers_dict.keys()).transpose()\n",
        "\n",
        "    print(\"\\n--- Análise de Cabeçalhos Concluída ---\")\n",
        "    print(\"A tabela abaixo mostra quais colunas estão presentes (✔) ou ausentes (❌) em cada arquivo.\")\n",
        "    display(HTML(comparison_df.to_html()))\n",
        "\n",
        "    print(\"\\n--- Sugestão de Colunas Comuns ---\")\n",
        "    print(f\"Foram encontradas {len(common_columns)} colunas presentes em TODOS os arquivos do período:\")\n",
        "    # Imprime a lista formatada para ser copiada e colada\n",
        "    print(\"\\nmanter_colunas = [\")\n",
        "    for col in common_columns:\n",
        "        print(f\"    '{col}',\")\n",
        "    print(\"]\")\n",
        "\n",
        "    return comparison_df, common_columns\n",
        "\n",
        "def process_to_dataframe(csv_paths: List[Path], columns_to_keep: List[str], institutions: List[str], chunksize: int = 100000, sep: str = ';'):\n",
        "    \"\"\"\n",
        "    Unifica, filtra e concatena os CSVs em um único DataFrame,\n",
        "    mantendo apenas a lista de colunas fornecida.\n",
        "    \"\"\"\n",
        "    if not csv_paths:\n",
        "        raise RuntimeError(\"Nenhum arquivo CSV para processar.\")\n",
        "    if not columns_to_keep:\n",
        "        raise ValueError(\"A lista 'columns_to_keep' não pode estar vazia.\")\n",
        "\n",
        "    print(f\"\\nProcessamento iniciado. Serão importadas {len(columns_to_keep)} colunas pré-definidas.\")\n",
        "\n",
        "    # Encontrar coluna da instituição (considerando inconsistências de codificação)\n",
        "    col_inst = None\n",
        "    if 'Instituição' in columns_to_keep:\n",
        "        col_inst = 'Instituição'\n",
        "    elif 'InstituiÃ§Ã£o' in columns_to_keep:\n",
        "        col_inst = 'InstituiÃ§Ã£o'\n",
        "\n",
        "    if institutions and col_inst:\n",
        "        print(f\"Filtrando pela coluna '{col_inst}' com os valores: {institutions}\")\n",
        "    elif institutions:\n",
        "        print(\"AVISO: Filtro de instituição solicitado, mas a coluna 'Instituição' não está na lista de colunas a serem mantidas.\")\n",
        "\n",
        "    institution_map = {'Instituto Federal do Rio Grande do Norte': 'IFRN'}\n",
        "    df_list = []\n",
        "\n",
        "    for csv_path in csv_paths:\n",
        "        print(f\"Processando e filtrando: {csv_path.name}\")\n",
        "        try:\n",
        "            # Lê o cabeçalho do arquivo para saber quais colunas ele realmente tem\n",
        "            actual_header = get_header(csv_path, sep)\n",
        "            # Usa apenas as colunas da nossa lista que existem neste arquivo\n",
        "            cols_to_read = [col for col in columns_to_keep if col in actual_header]\n",
        "\n",
        "            for chunk in pd.read_csv(csv_path, usecols=cols_to_read, chunksize=chunksize, sep=sep, engine='python', on_bad_lines='warn'):\n",
        "                if col_inst and col_inst in chunk.columns:\n",
        "                    chunk[col_inst] = chunk[col_inst].replace(institution_map)\n",
        "                    if institutions:\n",
        "                        chunk = chunk[chunk[col_inst].isin(institutions)]\n",
        "\n",
        "                if not chunk.empty:\n",
        "                    df_list.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"  ERRO ao processar o arquivo {csv_path.name}: {e}. Pulando este arquivo.\")\n",
        "            continue\n",
        "\n",
        "    if not df_list:\n",
        "        print(\"AVISO: Nenhum dado encontrado para as instituições selecionadas ou os arquivos estavam vazios.\")\n",
        "        return pd.DataFrame(columns=columns_to_keep)\n",
        "\n",
        "    final_df = pd.concat(df_list, ignore_index=True)\n",
        "    # Garante que o DataFrame final tenha todas as colunas da lista, preenchendo com NaN as que não existiam\n",
        "    final_df = final_df.reindex(columns=columns_to_keep)\n",
        "\n",
        "    print(f\"\\nProcesso concluído. DataFrame final criado com {len(final_df):,} linhas e {len(final_df.columns)} colunas.\")\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "jM4FIbiTrDQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 3: Configuração do Processo e Download (Opção para Código da Unidade)**\n",
        "\n",
        "# --- Interface Interativa de Configuração ---\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "# Seleção da tabela\n",
        "table_name_dropdown = widgets.Dropdown(\n",
        "    options=['matriculas', 'eficiencia_academica', 'financeiro', 'servidores'],\n",
        "    value='matriculas',\n",
        "    description='Tabela de Dados:',\n",
        "    style=style\n",
        ")\n",
        "\n",
        "# Período de anos\n",
        "start_year_slider = widgets.IntSlider(value=2017, min=2017, max=2024, step=1, description='Ano Inicial:', style=style)\n",
        "end_year_slider = widgets.IntSlider(value=2024, min=2017, max=2024, step=1, description='Ano Final:', style=style)\n",
        "\n",
        "# Opção de forçar atualização\n",
        "force_update_checkbox = widgets.Checkbox(value=False, description='Forçar atualização (baixar novamente os arquivos existentes)', style=style)\n",
        "\n",
        "# --- Filtro de instituições por código ---\n",
        "# O campo pede o código numérico da instituição.\n",
        "institution_code_text = widgets.Text(\n",
        "    value='26435', # O valor padrão está preenchido com o código do IFRN = '26435'\n",
        "    description='Códigos das Instituições (Co Inst):',\n",
        "    style=style,\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "print(\"--- Configure os Parâmetros do Pipeline ---\")\n",
        "display(table_name_dropdown)\n",
        "display(widgets.HBox([start_year_slider, end_year_slider]))\n",
        "display(force_update_checkbox)\n",
        "display(institution_code_text)"
      ],
      "metadata": {
        "id": "tJmLJZs1ec2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 3: Configuração do Processo e Download**\n",
        "\n",
        "# --- Interface Interativa de Configuração ---\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "# Seleção da tabela\n",
        "table_name_dropdown = widgets.Dropdown(\n",
        "    options=['matriculas', 'eficiencia_academica', 'financeiro', 'servidores'],\n",
        "    value='servidores',\n",
        "    description='Tabela de Dados:',\n",
        "    style=style\n",
        ")\n",
        "\n",
        "# Período de anos\n",
        "start_year_slider = widgets.IntSlider(value=2017, min=2017, max=2024, step=1, description='Ano Inicial:', style=style)\n",
        "end_year_slider = widgets.IntSlider(value=2024, min=2017, max=2024, step=1, description='Ano Final:', style=style)\n",
        "\n",
        "# Opção de forçar atualização\n",
        "force_update_checkbox = widgets.Checkbox(value=True, description='Forçar atualização (baixar novamente os arquivos existentes)', style=style)\n",
        "\n",
        "# --- Filtro de instituições por nome ---\n",
        "# O campo agora aceita um ou mais nomes de instituições, separados por vírgula.\n",
        "# O valor padrão já inclui as duas variações para o IFRN.\n",
        "institution_name_text = widgets.Text(\n",
        "    value='IFRN, Instituto Federal do Rio Grande do Norte',\n",
        "    description='Nome da Instituição (use vírgula para múltiplos nomes):',\n",
        "    style=style,\n",
        "    layout=widgets.Layout(width='70%') # Largura aumentada para melhor visualização\n",
        ")\n",
        "\n",
        "print(\"--- Configure os Parâmetros do Pipeline ---\")\n",
        "display(table_name_dropdown)\n",
        "display(widgets.HBox([start_year_slider, end_year_slider]))\n",
        "display(force_update_checkbox)\n",
        "display(institution_name_text)"
      ],
      "metadata": {
        "id": "2XCFScGZsmTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 4: Download dos arquivos `.gz` e análise dos cabeçalhos dos `.csv` extraídos**\n",
        "\n",
        "# 1. Pega os valores dos widgets da Etapa 3\n",
        "table_name = table_name_dropdown.value\n",
        "start_year = start_year_slider.value\n",
        "end_year = end_year_slider.value\n",
        "force_update = force_update_checkbox.value\n",
        "\n",
        "# Aplica o nome correto do widget e analisa as strings separadas por vírgulas\n",
        "institutions_str = institution_name_text.value\n",
        "institutions_list = [inst.strip() for inst in institutions_str.split(',') if inst.strip()]\n",
        "\n",
        "# 2. Executa o download\n",
        "download_pnp_data(table_name, start_year, end_year, force_update)\n",
        "\n",
        "# 3. Prepara os arquivos para a análise\n",
        "drive_folder = Path(f'/content/drive/MyDrive/Coisas do IFRN/Prodes/Indicadores/PNP/{table_name.capitalize()}')\n",
        "work_dir = Path.cwd() / \"extracted_csvs\"\n",
        "if work_dir.exists(): shutil.rmtree(work_dir)\n",
        "work_dir.mkdir()\n",
        "\n",
        "input_files_gz = [drive_folder / f\"microdados_{table_name}_{year}.csv.gz\" for year in range(start_year, end_year + 1)]\n",
        "input_files_gz_existing = [f for f in input_files_gz if f.exists()]\n",
        "\n",
        "all_csvs = []\n",
        "if input_files_gz_existing:\n",
        "    for gz_file in input_files_gz_existing:\n",
        "        all_csvs.append(decompress_gz_to_csv(gz_file, work_dir))\n",
        "else:\n",
        "    print(\"Nenhum arquivo .gz encontrado no Drive para o período e tabela selecionados.\")\n",
        "\n",
        "# 4. Analisa e compara os cabeçalhos\n",
        "if all_csvs:\n",
        "    comparison_df, common_columns = analyze_and_compare_headers(all_csvs)\n",
        "else:\n",
        "    print(\"Nenhuma análise de cabeçalho pôde ser feita, pois nenhum arquivo CSV foi descompactado.\")\n",
        "\n",
        "print(\"\\n\\n>>> AÇÃO NECESSÁRIA <<<\")\n",
        "print(\"Copie a lista de colunas comuns sugerida acima (ou edite-a conforme sua necessidade) e cole na célula da 'ETAPA 5' antes de executá-la.\")"
      ],
      "metadata": {
        "id": "6k14QyRSrMfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 5: Definição das colunas necessárias para extração de `df_filtrado`**\n",
        "\n",
        "# >>> LISTA DE COLUNAS PARA EXTRAÇÃO (COLE AQUI) <<<\n",
        "# Exemplo baseado na sugestão da etapa anterior.\n",
        "# Edite esta lista conforme a necessidade.\n",
        "# Lista de colunas a serem mantidas no DataFrame final, organizadas por categoria e finalidade.\n",
        "\n",
        "manter_colunas = [\n",
        "    'Classe',\n",
        "    'Cod Unidade',\n",
        "    'Código Municipio com DV',\n",
        "    'Código da Unidade de Ensino - SISTEC',\n",
        "    'Instituição',\n",
        "    'Jornada de Trabalho',\n",
        "    'Município',\n",
        "    'Número de registros',\n",
        "    'RSC',\n",
        "    'Região',\n",
        "    'Titulação',\n",
        "    'Unidade de Lotação',\n",
        "    'Vinculo Carreira',\n",
        "    'Vinculo Contrato',\n",
        "    'Vinculo Professor',\n",
        "]\n",
        "\n",
        "# --- Execução do Processamento ---\n",
        "df_filtrado = None # Inicializa a variável\n",
        "if not manter_colunas:\n",
        "    print(\"❌ ERRO: A lista 'manter_colunas' está vazia. Preencha-a com as colunas desejadas e execute novamente.\")\n",
        "elif not all_csvs:\n",
        "    print(\"❌ ERRO: Nenhum arquivo CSV foi encontrado para processar. Execute a Etapa 4 primeiro.\")\n",
        "else:\n",
        "    # Cria o DataFrame filtrado com base na seleção de colunas\n",
        "    df_filtrado = process_to_dataframe(\n",
        "        csv_paths=all_csvs,\n",
        "        columns_to_keep=manter_colunas,\n",
        "        institutions=institutions_list\n",
        "    )\n",
        "    display(df_filtrado.head())\n",
        "\n",
        "# --- Análise do DataFrame Gerado ---\n",
        "if df_filtrado is not None:\n",
        "    print(\"\\n--- Análise Detalhada do DataFrame Final ---\")\n",
        "    if df_filtrado.empty:\n",
        "        print(\"O DataFrame foi criado, mas está vazio (não contém linhas).\")\n",
        "    else:\n",
        "        num_rows, num_cols = df_filtrado.shape\n",
        "        print(f\"Dimensões: {num_rows:,} linhas e {num_cols} colunas.\")\n",
        "        print(\"\\nEstrutura e Tipos de Dados:\")\n",
        "        df_filtrado.info()\n",
        "else:\n",
        "    print(\"\\nO DataFrame 'df_filtrado' não foi criado. Verifique os erros nas etapas anteriores.\")"
      ],
      "metadata": {
        "id": "K6ZtJpH0rPJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Listagem e análise de valores únicos para as colunas indicadas em `df_filtrado`**\n",
        "if df_filtrado is not None:\n",
        "    columns_to_list = ['Classe', 'Jornada de Trabalho', 'Unidade de Lotação', 'RSC', 'Titulação', 'Vinculo Carreira', 'Vinculo Contrato',\n",
        "                       'Vinculo Professor']\n",
        "\n",
        "    for col in columns_to_list:\n",
        "        if col in df_filtrado.columns:\n",
        "            # Converte a coluna para o tipo 'string' antes de classificar valores exclusivos\n",
        "            unique_values = sorted(df_filtrado[col].astype(str).unique())\n",
        "            print(f\"Valores únicos em ordem alfabética na coluna '{col}':\")\n",
        "            for value in unique_values:\n",
        "                print(f\"- {value}\")\n",
        "            print(\"-\" * 20) # Separador\n",
        "        else:\n",
        "            print(f\"A coluna '{col}' não foi encontrada no DataFrame 'df_filtrado'.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "nns9fkpAmej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 6: Tratamento e Limpeza dos Dados em `df_tratado`**\n",
        "\n",
        "# Cria uma cópia de df_filtrado para os tratamentos necessários\n",
        "if df_filtrado is not None:\n",
        "    df_tratado = df_filtrado.copy()\n",
        "    print(\"DataFrame 'df_tratado' criado como cópia de 'df_filtrado'.\")\n",
        "\n",
        "    # Renomeia colunas adequadamente para o BigQuery\n",
        "    df_tratado = df_tratado.rename(columns={\n",
        "        'Cod Unidade': 'Cod_Unidade',\n",
        "        'Código Municipio com DV': 'Codigo_Municipio_com_DV',\n",
        "        'Código da Unidade de Ensino - SISTEC': 'Codigo_da_Unidade_de_Ensino_-_SISTEC',\n",
        "        'Instituição': 'Instituicao',\n",
        "        'Jornada de Trabalho': 'Jornada_de_Trabalho',\n",
        "        'Município': 'Municipio',\n",
        "        'Número de registros': 'Numero_de_registros',\n",
        "        'Região': 'Regiao',\n",
        "        'Titulação': 'Titulacao',\n",
        "        'Unidade de Lotação': 'Campus_do_IFRN',\n",
        "        'Vinculo Carreira': 'Vinculo_Carreira',\n",
        "        'Vinculo Contrato': 'Vinculo_Contrato',\n",
        "        'Vinculo Professor': 'Vinculo_Professor',\n",
        "    })\n",
        "\n",
        "    # Ajuste do parâmetro na coluna 'Vinculo_Contrato'\n",
        "    if 'Vinculo_Contrato' in df_tratado.columns:\n",
        "        df_tratado['Vinculo_Contrato'] = df_tratado['Vinculo_Contrato'].replace(\n",
        "            'Substituto/Temporario', 'Substituto/Temporário'\n",
        "        )\n",
        "        print(\"Parâmetro 'Substituto/Temporario' corrigido para 'Substituto/Temporário' na coluna 'Vinculo_Contrato'.\")\n",
        "    else:\n",
        "        print(\"Aviso: Coluna 'Vinculo_Contrato' não encontrada em 'df_tratado'.\")\n",
        "\n",
        "    # Ajuste de parâmetro na coluna 'Titulacao'\n",
        "    if 'Titulacao' in df_tratado.columns:\n",
        "        df_tratado['Titulacao'] = df_tratado['Titulacao'].replace(\n",
        "            'Aperfeicoamento', 'Aperfeiçoamento'\n",
        "        )\n",
        "        print(\"Parâmetro 'Aperfeicoamento' corrigido para 'Aperfeiçoamento' na coluna 'Titulacao'.\")\n",
        "    else:\n",
        "        print(\"Aviso: Coluna 'Titulacao' não encontrada em 'df_tratado'.\")\n",
        "\n",
        "    if df_tratado is not None and 'Titulacao' in df_tratado.columns:\n",
        "        print(\"Criando a coluna 'Nivel_Educacional' com base na coluna 'Titulacao'.\")\n",
        "\n",
        "        # Define o mapeamento de 'Titulacao' para uma nova coluna chamada 'Nivel_Educacional'\n",
        "        titulacao_para_nivel = {\n",
        "            'Não Informado': '(N/I) Não Informado',\n",
        "            'Ensino Fundamental': '(0) Educação Básica',\n",
        "            'Ensino Médio': '(0) Educação Básica',\n",
        "            'Técnico': '(1) Educação Profissional',\n",
        "            'Graduação': '(2) Educação Superior',\n",
        "            'Aperfeiçoamento': '(3) Pós-graduação',\n",
        "            'Especialização': '(3) Pós-graduação',\n",
        "            'Mestrado': '(3) Pós-graduação',\n",
        "            'Doutorado': '(3) Pós-graduação',\n",
        "            'Aperfeicoamento': '(3) Pós-graduação',\n",
        "            'Educação Básica': '(0) Educação Básica'\n",
        "        }\n",
        "\n",
        "        # Cria a nova coluna 'Nivel_Educacional' aplicando o mapeamento\n",
        "        # Usa .map() para mapear os valores e .fillna() para garantir que valores não mapeados se tornem NaN ou outro indicador se necessário\n",
        "        df_tratado['Nivel_Educacional'] = df_tratado['Titulacao'].map(titulacao_para_nivel).fillna('Outro/Não Mapeado') # Ajuste o fillna conforme a necessidade\n",
        "\n",
        "    # Ajustes e Renomeação na coluna 'Campus_do_IFRN'\n",
        "    if df_tratado is not None and 'Campus_do_IFRN' in df_tratado.columns:\n",
        "        print(\"Realizando ajustes e renomeação na coluna 'Unidade_de_Lotacao'.\")\n",
        "\n",
        "        # Remove as expressões 'Campus ' e 'Campus Avançado '\n",
        "        df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].str.replace('Campus Avançado ', '', regex=False)\n",
        "        df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].str.replace('Campus ', '', regex=False)\n",
        "        print(\"Expressões 'Campus ' e 'Campus Avançado ' removidas da coluna 'Campus_do_IFRN'.\")\n",
        "\n",
        "        # Renomeia parâmetros específicos em 'Campus_do_IFRN'\n",
        "        unidade_replacements = {\n",
        "            'Ceará-mirim': 'Ceará-Mirim',\n",
        "            'Pau Dos Ferros': 'Pau dos Ferros',\n",
        "            'Reitoria do Instituto Federal do Rio Grande do Norte': 'Reitoria'\n",
        "        }\n",
        "        df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].replace(unidade_replacements)\n",
        "        print(\"Parâmetros específicos renomeados na coluna 'Campus_do_IFRN'.\")\n",
        "\n",
        "    # Exibir informações sobre as colunas e tipos de dados de 'df_tratado'\n",
        "    print(\"\\nInformações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "QzniJhrcEJpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Exibe novamente os valores únicos para as colunas indicadas em `df_tratado`**\n",
        "if df_tratado is not None:\n",
        "    columns_to_list = ['Classe', 'Jornada_de_Trabalho', 'Campus_do_IFRN', 'RSC', 'Titulacao', 'Vinculo_Carreira', 'Vinculo_Contrato',\n",
        "                       'Vinculo_Professor', 'Nivel_Educacional']\n",
        "\n",
        "    for col in columns_to_list:\n",
        "        if col in df_tratado.columns:\n",
        "            # Converte a coluna para o tipo 'string' antes de classificar valores exclusivos\n",
        "            unique_values = sorted(df_tratado[col].astype(str).unique())\n",
        "            print(f\"Valores únicos em ordem alfabética na coluna '{col}':\")\n",
        "            for value in unique_values:\n",
        "                print(f\"- {value}\")\n",
        "            print(\"-\" * 20) # Separador\n",
        "        else:\n",
        "            print(f\"A coluna '{col}' não foi encontrada no DataFrame 'df_tratado'.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "VW8BPzNKHKYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"\\nContagem de valores na nova coluna 'Nivel_Educacional':\")\n",
        "    display(df_tratado['Nivel_Educacional'].value_counts())"
      ],
      "metadata": {
        "id": "1wQICqmiPpmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Exibir informações sobre as colunas e tipos de dados de `df_tratado`**\n",
        "if df_tratado is not None:\n",
        "    print(\"Informações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "YsF-NGGwRT_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 7: Exportação para o BigQuery**\n",
        "\n",
        "# --- Configurações de Destino do BigQuery ---\n",
        "import pandas_gbq\n",
        "\n",
        "project_id = \"pnp-data-extraction\" # Substitua pelo ID do seu projeto\n",
        "dataset_id = \"pnp_dados_IFRN\"      # Nome do conjunto de dados\n",
        "\n",
        "# O nome da tabela no BigQuery será o mesmo nome da tabela da PNP\n",
        "table_id = f\"df_{table_name}\"\n",
        "destination_table = f\"{dataset_id}.{table_id}\"\n",
        "\n",
        "# --- Execução da Exportação ---\n",
        "if df_tratado is not None and not df_tratado.empty:\n",
        "    print(f\"Iniciando a exportação de {len(df_tratado):,} linhas para o BigQuery...\")\n",
        "    print(f\"Destino: {project_id}.{destination_table}\")\n",
        "\n",
        "    # --- Renomear colunas para serem compatíveis com BigQuery ---\n",
        "    # Substitui espaços e '/' por '_'\n",
        "    df_tratado_bq = df_tratado.copy()\n",
        "    df_tratado_bq.columns = (df_tratado_bq.columns\n",
        "            .str.replace(' ', '_', regex=False)\n",
        "            .str.replace('/', '_', regex=False)\n",
        "            .str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "        )\n",
        "    print(\"Nomes das colunas padronizados para o BigQuery.\")\n",
        "\n",
        "    # --- Definição do Esquema da Tabela ---\n",
        "    # Defina o nome e o tipo de cada coluna que você quer controlar.\n",
        "    # Tipos comuns: 'STRING', 'INTEGER', 'FLOAT', 'NUMERIC', 'BOOLEAN', 'TIMESTAMP', 'DATE'.\n",
        "    table_schema = [\n",
        "        {\"name\": \"Classe\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Cod_Unidade\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_Municipio_com_DV\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_da_Unidade_de_Ensino_-_SISTEC\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Instituicao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Jornada_de_Trabalho\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Municipio\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Numero_de_registros\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"RSC\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Regiao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Titulacao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Campus_do_IFRN\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Vinculo_Carreira\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Vinculo_Contrato\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Vinculo_Professor\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Nivel_Educacional\", \"type\": \"STRING\"}\n",
        "    ]\n",
        "    # -----------------------------------------------------------------\n",
        "\n",
        "    # Envia o DataFrame para o BigQuery com o esquema especificado\n",
        "    try:\n",
        "        # Usando a função recomendada pandas_gbq.to_gbq\n",
        "        pandas_gbq.to_gbq(\n",
        "            df_tratado_bq, # Exporta o DataFrame com colunas renomeadas\n",
        "            destination_table=destination_table,\n",
        "            project_id=project_id,\n",
        "            if_exists='replace',  # Opções: 'fail', 'replace', 'append'\n",
        "            progress_bar=True\n",
        "        )\n",
        "        print(f\"\\n✔ DataFrame exportado com sucesso para o BigQuery!\")\n",
        "        print(f\"Link para a tabela: https://console.cloud.google.com/bigquery?project={project_id}&ws=!1m5!1m4!4m3!1s{project_id}!2s{dataset_id}!3s{table_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERRO durante a exportação para o BigQuery: {e}\")\n",
        "\n",
        "elif df_tratado is not None and df_tratado.empty:\n",
        "    print(\"AVISO: O DataFrame final está vazio. Nenhuma exportação foi realizada.\")\n",
        "else:\n",
        "    print(\"❌ ERRO: O DataFrame a ser exportado não foi encontrado. A exportação foi cancelada.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "H7wYuE0Dqco4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}