# **Pipeline de Extra√ß√£o, Tratamento e Carregamento para o BigQuery de Dados da Plataforma Nilo Pe√ßanha (PNP)**
<img src="https://novopnp.mec.gov.br/assets/logo-nilo-vertical-white-39a156b6.svg" alt="Logo da Plataforma Nilo Pe√ßanha" width="150"/>

## üìë Vis√£o Geral

Este projeto consiste em um pipeline de dados automatizado, desenvolvido em um notebook Google Colab, que permite a extra√ß√£o, tratamento e carregamento de dados da **Plataforma Nilo Pe√ßanha (PNP)**. 
O objetivo √© transformar e analisar grandes volumes de dados, disponibilizados anualmente em formato `.csv.gz`, em um DataFrame limpo, personalizado, padronizado e pronto para an√°lise. O armazenando do Data Frame Pandas final ser√£o tabelas do **Google BigQuery**, que podem ser conectadas a quaisquer ferramentas de _Business Intelligence_ (BI), como o **Google Looker Studio**, entre outras.

O pipeline √© configur√°vel atrav√©s de uma interface interativa, permitindo que o usu√°rio selecione a tabela de dados conforme a dimens√£o da PNP, o per√≠odo de an√°lise e as institui√ß√µes de ensino desejadas.

## ‚ú® Funcionalidades Principais

* **Download Automatizado:** Baixa os arquivos de microdados da PNP diretamente da fonte oficial para uma pasta no Google Drive do usu√°rio.
* **Interface Interativa:** Permite a configura√ß√£o de par√¢metros (tabela, ano inicial/final, institui√ß√£o) de forma amig√°vel, sem necessidade de alterar o c√≥digo.
* **An√°lise de Cabe√ßalhos:** Compara os cabe√ßalhos dos arquivos CSV de diferentes anos, identificando colunas comuns e inconsist√™ncias.
* **Processamento Eficiente de Big Data:** Utiliza a leitura de dados em _chunks_ (peda√ßos), permitindo processar arquivos grandes sem sobrecarregar a mem√≥ria.
* **Filtragem e Unifica√ß√£o:** Filtra os dados para uma ou mais institui√ß√µes espec√≠ficas e unifica os arquivos de m√∫ltiplos anos em um √∫nico DataFrame.
* **Limpeza e Padroniza√ß√£o:** Aplica um conjunto robusto de regras para limpar, padronizar e enriquecer os dados, com destaque para a normaliza√ß√£o de nomes de cursos.
* **Exporta√ß√£o para BigQuery:** Carrega o DataFrame final e tratado diretamente em uma tabela no Google BigQuery, deixando os dados prontos para consultas e visualiza√ß√µes.

## üöÄ Fluxo de Execu√ß√£o do Pipeline

O notebook √© organizado em etapas sequenciais. A seguir, uma descri√ß√£o de cada uma:

### **Etapa 1: Instala√ß√£o e Autentica√ß√£o**
* **O que faz:** Instala as bibliotecas Python necess√°rias (`pandas-gbq`, `gspread`, etc.) e realiza a autentica√ß√£o do usu√°rio para permitir o acesso ao Google Drive e aos servi√ßos Google Cloud.
* **A√ß√£o do Usu√°rio:** Executar a c√©lula e autorizar o acesso √† sua Conta Google quando solicitado.

### **Etapa 2: Defini√ß√£o das Fun√ß√µes Principais**
* **O que faz:** Carrega todas as fun√ß√µes que encapsulam a l√≥gica do pipeline (download, descompress√£o, an√°lise, processamento, etc.).
* **A√ß√£o do Usu√°rio:** Apenas executar a c√©lula. Nenhuma modifica√ß√£o √© necess√°ria.

### **Etapa 3: Configura√ß√£o do Processo**
* **O que faz:** Exibe uma interface interativa para o usu√°rio definir os par√¢metros da extra√ß√£o.
* **A√ß√£o do Usu√°rio:**
    * **Tabela de Dados:** Escolher a base de dados de interesse (ex: `matriculas`, `servidores`).
    * **Ano Inicial e Final:** Definir o per√≠odo da an√°lise.
    * **Nome da Institui√ß√£o:** Inserir o nome ou sigla da(s) institui√ß√£o(√µes) a serem filtradas, separados por v√≠rgula (ex: `IFRN, Instituto Federal do Rio Grande do Norte`).

### **Etapa 4: Download e An√°lise de Cabe√ßalhos**
* **O que faz:** Com base nos par√¢metros da Etapa 3, o script baixa os arquivos `.gz`, os descompacta e l√™ o cabe√ßalho de cada `.csv`. Ao final, exibe uma tabela comparativa e sugere uma lista de colunas que s√£o comuns a todos os arquivos do per√≠odo.
* **A√ß√£o do Usu√°rio:** Analisar a tabela de cabe√ßalhos e **copiar a lista de colunas sugerida (`final_columns_to_keep`)** para usar na pr√≥xima etapa.

### **Etapa 5: Processamento e Cria√ß√£o do DataFrame**
* **O que faz:** Utiliza a lista de colunas definida pelo usu√°rio para ler, filtrar (pela institui√ß√£o selecionada) e unificar todos os arquivos CSV em um √∫nico DataFrame Pandas (`df_final`).
* **A√ß√£o do Usu√°rio:** **Colar a lista de colunas** copiada da Etapa 4 no local indicado e executar a c√©lula.

### **Etapa 6: An√°lise Explorat√≥ria e Limpeza dos Dados**
* **O que faz:** Aplica uma s√©rie de transforma√ß√µes no DataFrame `df_final` para criar uma vers√£o limpa e padronizada, chamada `df_processed`. As principais a√ß√µes s√£o:
    1.  **Cria√ß√£o da coluna `Cursos`**: Adiciona prefixos como "Licenciatura em", "Tecnologia em" e "FIC" aos nomes dos cursos.
    2.  **Padroniza√ß√£o de Nomes de Cursos**: Executa uma limpeza profunda na coluna `Cursos`, removendo prefixos, corrigindo erros e aplicando regras de normaliza√ß√£o.
    3.  **Convers√£o para Booleano**: Trata a coluna `Matr√≠cula Atendida` para que contenha apenas valores `True` ou `False`.
    4.  **Ajustes Categ√≥ricos**: Padroniza valores em colunas como `Forma de ingresso`.
    5.  **Renomea√ß√£o de Colunas**: Renomeia todas as colunas para o padr√£o `snake_case`, ideal para bancos de dados.
    6.  **Convers√£o de Tipos**: Ajusta os tipos de dados de cada coluna (inteiro, data, string, etc.).
* **A√ß√£o do Usu√°rio:** Executar as c√©lulas em sequ√™ncia para aplicar todo o tratamento.

### **Etapa 7: Exporta√ß√£o para o BigQuery**
* **O que faz:** Realiza a √∫ltima etapa do pipeline, enviando o DataFrame final (`df_processed`) para uma tabela no Google BigQuery.
* **A√ß√£o do Usu√°rio:**
    1.  Definir as vari√°veis `project_id` e `dataset_id` com as informa√ß√µes do seu projeto no Google Cloud.
    2.  Executar a c√©lula para iniciar o upload.

## ‚öôÔ∏è Como Usar

1.  **Abra o Notebook:** Inicie o arquivo `.ipynb` no ambiente Google Colab.
2.  **Execute a Etapa 1:** Instale as depend√™ncias e autentique sua conta Google para acessar o Drive.
3.  **Execute a Etapa 2:** Carregue as fun√ß√µes do pipeline.
4.  **Configure na Etapa 3:** Preencha os campos da interface interativa com a tabela, per√≠odo e institui√ß√£o de seu interesse.
5.  **Execute a Etapa 4:** Aguarde o download e a an√°lise dos cabe√ßalhos. Ao final, copie a lista de colunas (`final_columns_to_keep`).
6.  **Cole na Etapa 5:** Cole a lista de colunas no local indicado e execute a c√©lula para criar o DataFrame bruto (`df_final`).
7.  **Execute a Etapa 6:** Rode todas as c√©lulas desta etapa para realizar a limpeza e o processamento dos dados.
8.  **Configure e Execute a Etapa 7:** Insira o ID do seu projeto e do seu dataset do BigQuery e execute a c√©lula para exportar a tabela final.

## üõ† Pr√©-requisitos

* Uma Conta Google com acesso ao Google Drive.
* Um projeto no **Google Cloud Platform** com a **API do BigQuery** ativada.
* Permiss√µes de Editor (`BigQuery Data Editor` e `BigQuery Job User`) no projeto do Google Cloud.
