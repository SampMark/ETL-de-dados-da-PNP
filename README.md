# **Pipeline de Extra√ß√£o, Tratamento e Carregamento para o BigQuery de Dados da Plataforma Nilo Pe√ßanha (PNP)**
<img src="https://novopnp.mec.gov.br/assets/logo-nilo-vertical-white-39a156b6.svg" alt="Logo da Plataforma Nilo Pe√ßanha" width="150"/>

## üìë Vis√£o Geral

Este projeto consiste em um pipeline de dados automatizado, desenvolvido em um [**notebook Google Colab**](https://github.com/SampMark/ETL-de-dados-da-PNP/blob/main/GitHub_ETL_of_PNP_Data___Pipeline_for_BigQuery.ipynb), que permite a extra√ß√£o, tratamento e carregamento de dados da **Plataforma Nilo Pe√ßanha (PNP)**. 
O objetivo √© transformar e analisar grandes volumes de dados, disponibilizados anualmente em formato `.csv.gz`, em um DataFrame limpo, personalizado, padronizado e pronto para an√°lise. O armazenando do Data Frame Pandas final ser√£o tabelas do **Google BigQuery**, que podem ser conectadas a quaisquer ferramentas de _Business Intelligence_ (BI), como o **Google Looker Studio**, entre outras. O pipeline √© configur√°vel atrav√©s de uma interface interativa, permitindo que o usu√°rio selecione a tabela de dados conforme a dimens√£o da PNP, o per√≠odo de an√°lise e as institui√ß√µes de ensino desejadas.

Visualize o [**Painel de An√°lise de Indicadores do IFRN extra√≠dos da PNP**](https://lookerstudio.google.com/reporting/10eeab08-4514-4251-bde9-7754fda7718f).

## ‚ú® Funcionalidades Principais

* **Download Automatizado:** Baixa os arquivos de microdados da PNP diretamente da fonte oficial para uma pasta no Google Drive do usu√°rio.
* **Interface Interativa:** Permite a configura√ß√£o de par√¢metros (tabela, ano inicial/final, institui√ß√£o) de forma amig√°vel, sem necessidade de alterar o c√≥digo.
* **An√°lise de Cabe√ßalhos:** Compara os cabe√ßalhos dos arquivos CSV de diferentes anos, identificando colunas comuns e inconsist√™ncias.
* **Processamento Eficiente de Big Data:** Utiliza a leitura de dados em _chunks_ (peda√ßos), permitindo processar arquivos grandes sem sobrecarregar a mem√≥ria.
* **Filtragem e Unifica√ß√£o:** Filtra os dados para uma ou mais institui√ß√µes espec√≠ficas e unifica os arquivos de m√∫ltiplos anos em um √∫nico DataFrame.
* **Limpeza e Padroniza√ß√£o:** Aplica um conjunto robusto de regras para limpar, padronizar e enriquecer os dados, com destaque para a normaliza√ß√£o de nomes de cursos.
* **Exporta√ß√£o para BigQuery:** Carrega o DataFrame final e tratado diretamente em uma tabela no Google BigQuery, deixando os dados prontos para consultas e visualiza√ß√µes.

## üöÄ Fluxo de Execu√ß√£o do Pipeline

Este notebook automatiza a extra√ß√£o, tratamento e carga (ETL) dos microdados da Plataforma Nilo Pe√ßanha (PNP) para o Google BigQuery. O fluxo foi projetado em etapas sequenciais para garantir a integridade e a qualidade dos dados.

### **Etapa 1: Instala√ß√£o e Autentica√ß√£o**
* Instala√ß√£o das bibliotecas Python necess√°rias (`pandas-gbq`, `gspread`, etc.).
* Autentica√ß√£o do usu√°rio para permitir o acesso ao Google Drive e aos servi√ßos Google Cloud.
* **A√ß√£o do Usu√°rio:** Executar a c√©lula e autorizar o acesso √† sua Conta Google quando solicitado.

### **Etapa 2: Defini√ß√£o das Fun√ß√µes Principais**
* Carregamento das fun√ß√µes em mem√≥ria que realizam as principais tarefas do pipeline: download, descompress√£o, an√°lise de cabe√ßalhos, processamento e tratamento dos dados.

### **Etapa 3: Configura√ß√£o do Processo**
* O usu√°rio define os par√¢metros da extra√ß√£o atrav√©s numa interface b√°sica interativa:
* **A√ß√£o do Usu√°rio:**
    * **Tabela de Dados:** Escolher a base de dados de interesse (ex: `matriculas`, `servidores`).
    * **Per√≠odo**: sefine o intervalo de anos (Ano Inicial e Final) para a extra√ß√£o.
    * **Nome da Institui√ß√£o:** Inserir o nome ou sigla da(s) institui√ß√£o(√µes) a serem filtradas, separados por v√≠rgula (ex: `IFRN, Instituto Federal do Rio Grande do Norte`).
    * **For√ßar Atualiza√ß√£o**: Op√ß√£o para baixar novamente os arquivos, ignorando o cache local no Google Drive.

### **Etapa 4: Download e An√°lise de Cabe√ßalhos**
* Com base nos par√¢metros da Etapa 3, o script baixa os arquivos `.gz`, os descompacta e l√™ o cabe√ßalho de cada `.csv`. Ao final, exibe uma tabela comparativa e sugere uma lista de colunas que s√£o comuns a todos os arquivos do per√≠odo.
* **Ponto de Decis√£o do Usu√°rio:** Analisar a tabela de cabe√ßalhos e **copiar e editar conforme a necessidade a lista de colunas sugerida em (`manter_colunas = [...]`)** a ser utilizada na pr√≥xima etapa.

### **Etapa 5: Processamento e Cria√ß√£o do DataFrame**
* O usu√°rio deve copiar a lista de colunas sugerida (e edit√°-la conforme a necessidade) e inseri-la na c√©lula de c√≥digo desta etapa
* O Script ir√° utilizar a lista de colunas definida pelo usu√°rio para ler, filtrar (pela institui√ß√£o selecionada) e unificar todos os arquivos CSV em um √∫nico DataFrame Pandas (`df_filtrado`).
* **A√ß√£o do Usu√°rio:** **Colar a lista de colunas** copiada da Etapa 4 no local indicado e executar a c√©lula.

### **Etapa 6 a 8: Tratamento, Limpeza e Cria√ß√£o de M√©tricas (ETL)**
* Uma sequ√™ncia de tratamentos √© aplicada ao DataFrame `df_filtrado` para criar uma vers√£o limpa e padronizada, chamada `df_tratado`. As principais a√ß√µes s√£o:
    1.  **Cria√ß√£o da coluna `Cursos`**: Adiciona prefixos como "Licenciatura em", "Tecnologia em" e "FIC" aos nomes dos cursos de modo a evitar que no passo seguinte, cursos com mesmo nome mas em n√≠veis distintos sejam padronizados equivocadamente.
    2.  **Padroniza√ß√£o de Nomes de Cursos**: Executa uma limpeza profunda na coluna `Cursos`, removendo prefixos, corrigindo erros e aplicando regras de normaliza√ß√£o.
    3.  **Convers√£o para Booleano**: Trata a coluna `Matr√≠cula Atendida` para que contenha apenas valores `True` ou `False`.
    4.  **Ajustes Categ√≥ricos**: Padroniza valores em colunas como `Forma de ingresso`.
    5.  **Renomea√ß√£o de Colunas**: Renomeia todas as colunas para o padr√£o `snake_case`, ideal para bancos de dados.
    6.  **Convers√£o de Tipos**: Ajusta os tipos de dados de cada coluna (inteiro, data, string, etc.).
    7.  **C√°lculo de M√©tricas**: Cria√ß√£o de colunas-chave para an√°lise, como `Vagas` e `Inscritos`, aplicando a l√≥gica de desduplica√ß√£o para evitar somas inflacionadas.
* **A√ß√£o do Usu√°rio:** Executar as c√©lulas em sequ√™ncia para aplicar todo o tratamento.

### ** Etapa 9 e 10: An√°lise Estat√≠stica e Exporta√ß√£o para o BigQuery**
* S√£o realizadas an√°lises estat√≠sticas descritivas e de outliers sobre os dados tratados para verificar a qualidade final.
* Finalmente, a √∫ltima etapa do pipeline, envio do DataFrame final (`df_tratado`) para uma tabela no Google BigQuery, finalizando o processo de ETL. O script gera um link direto para a tabela criada.
* **A√ß√£o do Usu√°rio:**
    1.  Definir as vari√°veis `project_id` e `dataset_id` com as informa√ß√µes do seu projeto no Google Cloud.
    2.  Executar a c√©lula para iniciar o upload.

## ‚öôÔ∏è Como Usar

1.  **Abra o Notebook:** Inicie o arquivo `.ipynb` no ambiente Google Colab.
2.  **Execute a Etapa 1:** Instale as depend√™ncias e autentique sua conta Google para acessar o Drive.
3.  **Execute a Etapa 2:** Carregue as fun√ß√µes do pipeline.
4.  **Configure na Etapa 3:** Preencha os campos da interface interativa com a tabela, per√≠odo e institui√ß√£o de seu interesse.
5.  **Execute a Etapa 4:** Aguarde o download e a an√°lise dos cabe√ßalhos. Ao final, copie a lista de colunas (`manter_colunas`).
6.  **Cole na Etapa 5:** Cole a lista de colunas no local indicado e execute a c√©lula para criar o DataFrame bruto (`df_final`).
7.  **Execute a Etapa 6:** Rode todas as c√©lulas desta etapa para realizar a limpeza e o processamento dos dados.
8.  **Configure e Execute a Etapa 7:** Insira o ID do seu projeto e do seu dataset do BigQuery e execute a c√©lula para exportar a tabela final.

## üõ† Pr√©-requisitos

* Uma Conta Google com acesso ao Google Drive.
* Um projeto no **Google Cloud Platform** com a **API do BigQuery** ativada.
* Permiss√µes de Editor (`BigQuery Data Editor` e `BigQuery Job User`) no projeto do Google Cloud.

## üìö Refer√™ncias

* MORAES, Gustavo Henrique et al. **Plataforma Nilo Pe√ßanha: guia de refer√™ncia metodol√≥gica**. Bras√≠lia, DF: Editora Evobiz, 2020. 131 p. E-book (PDF). Dispon√≠vel em: (https://dadosabertos.mec.gov.br/images/pdf/grm-2020-isbn-revisado.pdf).
* **Plataforma Nilo Pe√ßanha no Power BI**. Dispon√≠vel em: (https://app.powerbi.com/view?r=eyJrIjoiZDhkNGNiYzgtMjQ0My00OGVlLWJjNzYtZWQwYjI2OThhYWM1IiwidCI6IjllNjgyMzU5LWQxMjgtNGVkYi1iYjU4LTgyYjJhMTUzNDBmZiJ9).


