{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SampMark/ETL-de-dados-da-PNP/blob/main/GitHub_ETL_of_PNP_Data___Pipeline_for_BigQuery___academic_efficiency_dimension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extração, filtragem, tratamento e armazenamento em Big Query dos dados da da Plataforma Nilo Peçanha (PNP) de Instituições Federais de Ensino (IFES) - dimensão eficiência acadêmica**\n",
        "\n",
        "Este notebook automatiza a extração, tratamento e carga (ETL) dos microdados da Plataforma Nilo Peçanha (PNP) para o Google BigQuery. O fluxo foi projetado em etapas sequenciais para garantir a integridade e a qualidade dos dados.\n",
        "\n",
        "## **Fluxo de Execução**\n",
        "\n",
        "1. **Etapa 1: Instalação e Autenticação**: instalação das bibliotecas Python necessárias para a manipulação dos dados e conexão com os serviços Google (`gspread`, `pandas-gbq`, etc.). Autenticação do usuário e montagem do Google Drive para acesso e armazenamento dos arquivos de dados.\n",
        "2. **Etapa 2: Definição das Funções Principais**: carregamento das funções em memória que realizam as principais tarefas do pipeline: download, descompressão, análise de cabeçalhos, processamento e tratamento dos dados.\n",
        "3. **Etapa 3: Configuração do Pipeline**: o usuário define os parâmetros da extração através de uma interface interativa:\n",
        "\n",
        "    * **Período**: define o intervalo de anos (Ano Inicial e Final) para a extração.\n",
        "    * **Instituição(ões)**: filtra os dados para uma ou mais instituições específicas.\n",
        "    * **Força atualização**: opção para baixar novamente os arquivos da PNP, ignorando o cache local no Google Drive.\n",
        "\n",
        "4. **Etapa 4: Download e Análise de Cabeçalhos**: o script baixa os arquivos de dados compactados (`.gz`) da PNP para o Google Drive com base nas configurações da etapa anterior. Em seguida, os arquivos são descompactados e o script realiza uma análise comparativa dos cabeçalhos (colunas) de cada ano, exibindo uma tabela que destaca as diferenças.\n",
        "    * **Tabela com o comparativo de cabeçalhos dos CSV processados**: permite a analise prévia das colunas a serem extraídas do conjunto de dados (ex: matriculas, servidores).\n",
        "    * **Ponto de Decisão**: é gerada uma lista com as colunas comuns a todos os arquivos do período, que servirá de sugestão para a próxima etapa.\n",
        "\n",
        "5. **Etapa 5: Seleção de Colunas e Processamento dos Dados**: esta etapa demanda uma **Ação do Usuário** que deve copiar a lista de colunas sugerida e editá-la conforme a necessidade, em seguida inserir na célula de código desta etapa. O script então processa os arquivos CSV, unificando-os em um único DataFrame (`df_filtrado`), mantendo apenas as colunas selecionadas e aplicando o filtro de instituição.\n",
        "\n",
        "6. **Etapa 6 a 8: Tratamento, Limpeza e Criação de Métricas (ETL)**: uma sequência de tratamentos é aplicada ao DataFrame (`df_tratado`) unificado:\n",
        "    * **Padronização de `Nomes de Cursos`**: criação de uma coluna `Cursos` com nomes limpos e padronizados.\n",
        "    * **Ajustes de colunas**: correção de inconsistências, renomeação de colunas para compatibilidade com o BigQuery e tratamento de valores únicos.\n",
        "    * **Conversão de Tipos**: ajuste dos tipos de dados (numérico, texto, data) para garantir consistência.\n",
        "    * **Cálculo de Métricas**: criação de colunas-chave para análise, como `Vagas` e `Inscritos`, aplicando a lógica de desduplicação para evitar somas inflacionadas.\n",
        "7. **Etapa 9 e 10: Análise e Exportação para o BigQuery**: são realizadas análises estatísticas descritivas e de _outliers_ sobre os dados tratados para verificar a qualidade final. Finalmente, o DataFrame (`df_tratado`) é exportado para uma tabela no Google BigQuery, finalizando o processo de ETL. O script gera um link direto para a tabela criada.\n",
        "\n",
        "<img src=\"https://www2.ifal.edu.br/noticias/ifal-se-destaca-na-eficiencia-academica-dos-institutos-federais-do-nordeste/plataforma-nilo-pecanha/@@images/98c1a2a4-6c59-436f-bdce-effa7ae4d539.jpeg\" alt=\"Logo da Plataforma Nilo Peçanha\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "sq1VuFUOqieo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Extração de microdados da PNP para Big Query\n",
        "\n",
        "Este notebook automatiza o fluxo de trabalho com os microdados extraídos da Plataforma Nilo Peçanha (PNP),\n",
        "permitindo a extração de diferentes tabelas e a análise de seus cabeçalhos em cada anos.\n",
        "\"\"\"\n",
        "\n",
        "# @title **ETAPA 1: Instalação de dependências, importações e autenticação do usuário no Google Drive**\n",
        "\n",
        "# Instalação de Dependências\n",
        "!pip install gspread gspread-dataframe oauth2client pandas-gbq --quiet\n",
        "print(\"Dependências instaladas com sucesso!\")"
      ],
      "metadata": {
        "id": "nYWoq3EYq7FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação de bibliotecas\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import gspread\n",
        "import gzip\n",
        "import shutil\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import auth, drive\n",
        "from google.auth import default\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Set"
      ],
      "metadata": {
        "id": "QmUMUGuKq9yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autenticação e Montagem do Google Drive\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"\\nAutenticação e montagem do Google Drive realizadas com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante a autenticação ou montagem do Drive: {e}\")"
      ],
      "metadata": {
        "id": "FJq__1gYq_UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 2: Definição das Funções Principais**\n",
        "\n",
        "def download_pnp_data(table_name: str, start_year: int, end_year: int, force_update: bool):\n",
        "    \"\"\"\n",
        "    Baixa os arquivos da PNP para uma pasta específica no Google Drive.\n",
        "    O nome do arquivo e a pasta de destino são baseados no table_name.\n",
        "    \"\"\"\n",
        "    drive_folder = Path(f'/content/drive/MyDrive/Coisas do IFRN/Prodes/Indicadores/PNP/{table_name.capitalize()}')\n",
        "    drive_folder.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Verificando arquivos na pasta do Google Drive: {drive_folder}\")\n",
        "\n",
        "    base_url = \"https://d236w85zd3t8iw.cloudfront.net/pnp-tests/microdados\"\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        file_name = f\"microdados_{table_name}_{year}.csv.gz\"\n",
        "        url = f\"{base_url}/{year}/{file_name}\"\n",
        "        destination = drive_folder / file_name\n",
        "\n",
        "        if not force_update and destination.exists():\n",
        "            print(f\"✔ O arquivo para {year} ('{file_name}') já existe. Usando o cache do Drive.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f\"⬇ Baixando dados para {year} de {url}...\")\n",
        "            with requests.get(url, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(destination, 'wb') as f:\n",
        "                    shutil.copyfileobj(r.raw, f)\n",
        "            print(f\"✔ Download de {year} concluído com sucesso.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Falha ao baixar o arquivo para {year}. Erro: {e}. O arquivo pode não existir para este ano.\")\n",
        "\n",
        "def decompress_gz_to_csv(gz_path: Path, out_dir: Path) -> Path:\n",
        "    \"\"\"Descompacta cada arquivo .gz para a pasta de trabalho\"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # Remove a extensão .gz para obter o nome do arquivo CSV\n",
        "    csv_out_path = out_dir / gz_path.with_suffix(\"\").name\n",
        "    print(f\"Descompactando: {gz_path.name} -> {csv_out_path.name}\")\n",
        "    with gzip.open(gz_path, \"rb\") as f_in, open(csv_out_path, \"wb\") as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "    return csv_out_path\n",
        "\n",
        "def get_header(path: Path, sep: str = ';') -> List[str]:\n",
        "    \"\"\"Lê o cabeçalho de cada arquivo CSV descompactado\"\"\"\n",
        "    try:\n",
        "        return list(pd.read_csv(path, nrows=0, sep=sep, engine='python').columns)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler o cabeçalho de {path.name}: {e}\")\n",
        "        return []\n",
        "\n",
        "def analyze_and_compare_headers(csv_paths: List[Path]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria um DataFrame comparativo de cabeçalhos e sugere colunas comuns.\n",
        "    \"\"\"\n",
        "    if not csv_paths:\n",
        "        print(\"Nenhum arquivo CSV para analisar.\")\n",
        "        return pd.DataFrame(), []\n",
        "\n",
        "    headers_dict = {path.name: get_header(path) for path in csv_paths}\n",
        "\n",
        "    # Identificar colunas comuns\n",
        "    sets_of_headers = [set(h) for h in headers_dict.values() if h]\n",
        "    if not sets_of_headers:\n",
        "        print(\"Não foi possível ler nenhum cabeçalho.\")\n",
        "        return pd.DataFrame(), []\n",
        "\n",
        "    common_columns = sorted(list(sets_of_headers[0].intersection(*sets_of_headers[1:])))\n",
        "\n",
        "    # Criar DataFrame para comparação visual\n",
        "    all_columns = sorted(list(set.union(*sets_of_headers)))\n",
        "    comparison_data = {}\n",
        "    for col in all_columns:\n",
        "        comparison_data[col] = [(\"✔\" if col in headers_dict.get(fname, []) else \"❌\") for fname in headers_dict.keys()]\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data, index=headers_dict.keys()).transpose()\n",
        "\n",
        "    print(\"\\n--- Análise de Cabeçalhos Concluída ---\")\n",
        "    print(\"A tabela abaixo mostra quais colunas estão presentes (✔) ou ausentes (❌) em cada arquivo.\")\n",
        "    display(HTML(comparison_df.to_html()))\n",
        "\n",
        "    print(\"\\n--- Sugestão de Colunas Comuns ---\")\n",
        "    print(f\"Foram encontradas {len(common_columns)} colunas presentes em TODOS os arquivos do período:\")\n",
        "    # Imprime a lista formatada para ser copiada e colada\n",
        "    print(\"\\nmanter_colunas = [\")\n",
        "    for col in common_columns:\n",
        "        print(f\"    '{col}',\")\n",
        "    print(\"]\")\n",
        "\n",
        "    return comparison_df, common_columns\n",
        "\n",
        "def process_to_dataframe(csv_paths: List[Path], columns_to_keep: List[str], institutions: List[str], chunksize: int = 100000, sep: str = ';'):\n",
        "    \"\"\"\n",
        "    Unifica, filtra e concatena os CSVs em um único DataFrame,\n",
        "    mantendo apenas a lista de colunas fornecida.\n",
        "    \"\"\"\n",
        "    if not csv_paths:\n",
        "        raise RuntimeError(\"Nenhum arquivo CSV para processar.\")\n",
        "    if not columns_to_keep:\n",
        "        raise ValueError(\"A lista 'columns_to_keep' não pode estar vazia.\")\n",
        "\n",
        "    print(f\"\\nProcessamento iniciado. Serão importadas {len(columns_to_keep)} colunas pré-definidas.\")\n",
        "\n",
        "    # Encontrar coluna da instituição (considerando inconsistências de codificação)\n",
        "    col_inst = None\n",
        "    if 'Instituição' in columns_to_keep:\n",
        "        col_inst = 'Instituição'\n",
        "    elif 'InstituiÃ§Ã£o' in columns_to_keep:\n",
        "        col_inst = 'InstituiÃ§Ã£o'\n",
        "\n",
        "    if institutions and col_inst:\n",
        "        print(f\"Filtrando pela coluna '{col_inst}' com os valores: {institutions}\")\n",
        "    elif institutions:\n",
        "        print(\"AVISO: Filtro de instituição solicitado, mas a coluna 'Instituição' não está na lista de colunas a serem mantidas.\")\n",
        "\n",
        "    institution_map = {'Instituto Federal do Rio Grande do Norte': 'IFRN'}\n",
        "    df_list = []\n",
        "\n",
        "    for csv_path in csv_paths:\n",
        "        print(f\"Processando e filtrando: {csv_path.name}\")\n",
        "        try:\n",
        "            # Lê o cabeçalho do arquivo para saber quais colunas ele realmente tem\n",
        "            actual_header = get_header(csv_path, sep)\n",
        "            # Usa apenas as colunas da nossa lista que existem neste arquivo\n",
        "            cols_to_read = [col for col in columns_to_keep if col in actual_header]\n",
        "\n",
        "            for chunk in pd.read_csv(csv_path, usecols=cols_to_read, chunksize=chunksize, sep=sep, engine='python', on_bad_lines='warn'):\n",
        "                if col_inst and col_inst in chunk.columns:\n",
        "                    chunk[col_inst] = chunk[col_inst].replace(institution_map)\n",
        "                    if institutions:\n",
        "                        chunk = chunk[chunk[col_inst].isin(institutions)]\n",
        "\n",
        "                if not chunk.empty:\n",
        "                    df_list.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"  ERRO ao processar o arquivo {csv_path.name}: {e}. Pulando este arquivo.\")\n",
        "            continue\n",
        "\n",
        "    if not df_list:\n",
        "        print(\"AVISO: Nenhum dado encontrado para as instituições selecionadas ou os arquivos estavam vazios.\")\n",
        "        return pd.DataFrame(columns=columns_to_keep)\n",
        "\n",
        "    final_df = pd.concat(df_list, ignore_index=True)\n",
        "    # Garante que o DataFrame final tenha todas as colunas da lista, preenchendo com NaN as que não existiam\n",
        "    final_df = final_df.reindex(columns=columns_to_keep)\n",
        "\n",
        "    print(f\"\\nProcesso concluído. DataFrame final criado com {len(final_df):,} linhas e {len(final_df.columns)} colunas.\")\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "jM4FIbiTrDQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 3: Configuração do Processo e Download (Opção para Código da Unidade)**\n",
        "\n",
        "# --- Interface Interativa de Configuração ---\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "# Seleção da tabela\n",
        "table_name_dropdown = widgets.Dropdown(\n",
        "    options=['matriculas', 'eficiencia_academica', 'financeiro', 'servidores'],\n",
        "    value='eficiencia_academica',\n",
        "    description='Tabela de Dados:',\n",
        "    style=style\n",
        ")\n",
        "\n",
        "# Período de anos\n",
        "start_year_slider = widgets.IntSlider(value=2017, min=2017, max=2024, step=1, description='Ano Inicial:', style=style)\n",
        "end_year_slider = widgets.IntSlider(value=2024, min=2017, max=2024, step=1, description='Ano Final:', style=style)\n",
        "\n",
        "# Opção de forçar atualização\n",
        "force_update_checkbox = widgets.Checkbox(value=True, description='Forçar atualização (baixar novamente os arquivos existentes)', style=style)\n",
        "\n",
        "# --- Filtro de instituições por código ---\n",
        "# O campo pede o código numérico da instituição.\n",
        "institution_code_text = widgets.Text(\n",
        "    value='26435', # O valor padrão está preenchido com o código do IFRN = '26435'\n",
        "    description='Códigos das Instituições (Co Inst):',\n",
        "    style=style,\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "print(\"--- Configure os Parâmetros do Pipeline ---\")\n",
        "display(table_name_dropdown)\n",
        "display(widgets.HBox([start_year_slider, end_year_slider]))\n",
        "display(force_update_checkbox)\n",
        "display(institution_code_text)"
      ],
      "metadata": {
        "id": "tJmLJZs1ec2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 3: Configuração do Processo e Download**\n",
        "\n",
        "# --- Interface Interativa de Configuração ---\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "# Seleção da tabela\n",
        "table_name_dropdown = widgets.Dropdown(\n",
        "    options=['matriculas', 'eficiencia_academica', 'financeiro', 'servidores'],\n",
        "    value='eficiencia_academica',\n",
        "    description='Tabela de Dados:',\n",
        "    style=style\n",
        ")\n",
        "\n",
        "# Período de anos\n",
        "start_year_slider = widgets.IntSlider(value=2017, min=2017, max=2024, step=1, description='Ano Inicial:', style=style)\n",
        "end_year_slider = widgets.IntSlider(value=2024, min=2017, max=2024, step=1, description='Ano Final:', style=style)\n",
        "\n",
        "# Opção de forçar atualização\n",
        "force_update_checkbox = widgets.Checkbox(value=True, description='Forçar atualização (baixar novamente os arquivos existentes)', style=style)\n",
        "\n",
        "# --- Filtro de instituições por nome ---\n",
        "# O campo agora aceita um ou mais nomes de instituições, separados por vírgula.\n",
        "# O valor padrão já inclui as duas variações para o IFRN.\n",
        "institution_name_text = widgets.Text(\n",
        "    value='IFRN, Instituto Federal do Rio Grande do Norte',\n",
        "    description='Nome da Instituição (use vírgula para múltiplos nomes):',\n",
        "    style=style,\n",
        "    layout=widgets.Layout(width='70%') # Largura aumentada para melhor visualização\n",
        ")\n",
        "\n",
        "print(\"--- Configure os Parâmetros do Pipeline ---\")\n",
        "display(table_name_dropdown)\n",
        "display(widgets.HBox([start_year_slider, end_year_slider]))\n",
        "display(force_update_checkbox)\n",
        "display(institution_name_text)"
      ],
      "metadata": {
        "id": "2XCFScGZsmTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 4: Download dos arquivos `.gz` e análise dos cabeçalhos dos `.csv` extraídos**\n",
        "\n",
        "# 1. Pega os valores dos widgets da Etapa 3\n",
        "table_name = table_name_dropdown.value\n",
        "start_year = start_year_slider.value\n",
        "end_year = end_year_slider.value\n",
        "force_update = force_update_checkbox.value\n",
        "\n",
        "# Aplica o nome correto do widget e analisa as strings separadas por vírgulas\n",
        "institutions_str = institution_name_text.value\n",
        "institutions_list = [inst.strip() for inst in institutions_str.split(',') if inst.strip()]\n",
        "\n",
        "# 2. Executa o download\n",
        "download_pnp_data(table_name, start_year, end_year, force_update)\n",
        "\n",
        "# 3. Prepara os arquivos para a análise\n",
        "drive_folder = Path(f'/content/drive/MyDrive/Coisas do IFRN/Prodes/Indicadores/PNP/{table_name.capitalize()}')\n",
        "work_dir = Path.cwd() / \"extracted_csvs\"\n",
        "if work_dir.exists(): shutil.rmtree(work_dir)\n",
        "work_dir.mkdir()\n",
        "\n",
        "input_files_gz = [drive_folder / f\"microdados_{table_name}_{year}.csv.gz\" for year in range(start_year, end_year + 1)]\n",
        "input_files_gz_existing = [f for f in input_files_gz if f.exists()]\n",
        "\n",
        "all_csvs = []\n",
        "if input_files_gz_existing:\n",
        "    for gz_file in input_files_gz_existing:\n",
        "        all_csvs.append(decompress_gz_to_csv(gz_file, work_dir))\n",
        "else:\n",
        "    print(\"Nenhum arquivo .gz encontrado no Drive para o período e tabela selecionados.\")\n",
        "\n",
        "# 4. Analisa e compara os cabeçalhos\n",
        "if all_csvs:\n",
        "    comparison_df, common_columns = analyze_and_compare_headers(all_csvs)\n",
        "else:\n",
        "    print(\"Nenhuma análise de cabeçalho pôde ser feita, pois nenhum arquivo CSV foi descompactado.\")\n",
        "\n",
        "print(\"\\n\\n>>> AÇÃO NECESSÁRIA <<<\")\n",
        "print(\"Copie a lista de colunas comuns sugerida acima (ou edite-a conforme sua necessidade) e cole na célula da 'ETAPA 5' antes de executá-la.\")"
      ],
      "metadata": {
        "id": "6k14QyRSrMfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 5: Definição das colunas necessárias para extração de `df_filtrado`**\n",
        "\n",
        "# >>> LISTA DE COLUNAS PARA EXTRAÇÃO (COLE AQUI) <<<\n",
        "# Exemplo baseado na sugestão da etapa anterior.\n",
        "# Edite esta lista conforme a necessidade.\n",
        "# Lista de colunas a serem mantidas no DataFrame final, organizadas por categoria e finalidade.\n",
        "\n",
        "manter_colunas = [\n",
        "    # --- IDENTIFICADORES E CÓDIGOS ---\n",
        "    # Códigos únicos utilizados para referenciar instituições, unidades, matrículas e ciclos.\n",
        "    'Co Inst', 'Instituição', 'Cod Unidade', 'Unidade de Ensino', 'Código da Unidade de Ensino - SISTEC',\n",
        "    'Código do Ciclo Matricula', 'Código da Matricula',\n",
        "\n",
        "    # --- DADOS DE LOCALIZAÇÃO ---\n",
        "    # Variáveis de localização geográfica.\n",
        "    'Município', 'Código do Município com DV', 'UF', 'Região',\n",
        "\n",
        "    # --- DADOS DO CURSO ---\n",
        "    # Variáveis que descrevem as características do curso ofertado.\n",
        "    'Modalidade de Ensino', 'Tipo de Curso', 'Tipo de Oferta',  'Eixo Tecnológico', 'Subeixo Tecnológico',\n",
        "    'Nome de Curso', 'Turno', 'Carga Horaria', 'Carga Horaria Mínima', 'Fonte de Financiamento',\n",
        "    'Fator Esforço Curso', # Fator de ponderação para cálculo de Matrícula Equivalente\n",
        "\n",
        "    # --- DADOS DO ALUNO (DEMOGRÁFICOS E SOCIOECONÔMICOS) ---\n",
        "    # Variáveis com informações pessoais e socioeconômicas do estudante.\n",
        "    'Sexo', 'Cor / Raça', 'Idade', 'Faixa Etária', 'Renda Familiar',\n",
        "\n",
        "    # --- DADOS DA MATRÍCULA ---\n",
        "    # Variáveis que detalham o status e o período do vínculo do aluno com o curso.\n",
        "    'Ano', 'Matrícula Atendida', 'Situação de Matrícula', 'Categoria da Situação',\n",
        "    'Data de Inicio do Ciclo', 'Data de Fim Previsto do Ciclo',\n",
        "    'Data de Ocorrencia da Matricula', 'Mês De Ocorrência da Situação',\n",
        "\n",
        "    # --- DADOS DE VAGAS E INGRESSO ---\n",
        "    # Variáveis relacionadas ao processo seletivo, número de vagas e forma de entrada.\n",
        "    'Total de Inscritos', 'Forma de ingresso', 'Vagas Ofertadas',\n",
        "    'Vagas Regulares AC', 'Vagas Regulares l1', 'Vagas Regulares l2', 'Vagas Regulares l5', 'Vagas Regulares l6',\n",
        "    'Vagas Regulares l9', 'Vagas Regulares l10', 'Vagas Regulares l13', 'Vagas Regulares l14',\n",
        "    'Vagas Extraordinárias AC', 'Vagas Extraordinárias l1', 'Vagas Extraordinárias l2',\n",
        "    'Vagas Extraordinárias l5', 'Vagas Extraordinárias l6', 'Vagas Extraordinárias l9',\n",
        "    'Vagas Extraordinárias l10', 'Vagas Extraordinárias l13', 'Vagas Extraordinárias l14'\n",
        "]\n",
        "\n",
        "# --- Execução do Processamento ---\n",
        "df_filtrado = None # Inicializa a variável\n",
        "if not manter_colunas:\n",
        "    print(\"❌ ERRO: A lista 'manter_colunas' está vazia. Preencha-a com as colunas desejadas e execute novamente.\")\n",
        "elif not all_csvs:\n",
        "    print(\"❌ ERRO: Nenhum arquivo CSV foi encontrado para processar. Execute a Etapa 4 primeiro.\")\n",
        "else:\n",
        "    # Cria o DataFrame filtrado com base na seleção de colunas\n",
        "    df_filtrado = process_to_dataframe(\n",
        "        csv_paths=all_csvs,\n",
        "        columns_to_keep=manter_colunas,\n",
        "        institutions=institutions_list\n",
        "    )\n",
        "    display(df_filtrado.head())\n",
        "\n",
        "# --- Análise do DataFrame Gerado ---\n",
        "if df_filtrado is not None:\n",
        "    print(\"\\n--- Análise Detalhada do DataFrame Final ---\")\n",
        "    if df_filtrado.empty:\n",
        "        print(\"O DataFrame foi criado, mas está vazio (não contém linhas).\")\n",
        "    else:\n",
        "        num_rows, num_cols = df_filtrado.shape\n",
        "        print(f\"Dimensões: {num_rows:,} linhas e {num_cols} colunas.\")\n",
        "        print(\"\\nEstrutura e Tipos de Dados:\")\n",
        "        df_filtrado.info()\n",
        "else:\n",
        "    print(\"\\nO DataFrame 'df_filtrado' não foi criado. Verifique os erros nas etapas anteriores.\")"
      ],
      "metadata": {
        "id": "K6ZtJpH0rPJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Listagem e análise de valores únicos para as colunas indicadas em `df_filtrado`**\n",
        "if df_filtrado is not None:\n",
        "    columns_to_list = ['Ano', 'Unidade de Ensino', 'Eixo Tecnológico', 'Subeixo Tecnológico', 'Matrícula Atendida',\n",
        "                       'Forma de ingresso', 'Faixa Etária', 'Renda Familiar', 'Fonte de Financiamento',\n",
        "                       'Tipo de Oferta', 'Tipo de Curso', 'Situação de Matrícula', 'Categoria da Situação',\n",
        "                       ]\n",
        "\n",
        "    for col in columns_to_list:\n",
        "        if col in df_filtrado.columns:\n",
        "            # Converte a coluna para o tipo 'string' antes de classificar valores exclusivos\n",
        "            unique_values = sorted(df_filtrado[col].astype(str).unique())\n",
        "            print(f\"Valores únicos em ordem alfabética na coluna '{col}':\")\n",
        "            for value in unique_values:\n",
        "                print(f\"- {value}\")\n",
        "            print(\"-\" * 20) # Separador\n",
        "        else:\n",
        "            print(f\"A coluna '{col}' não foi encontrada no DataFrame 'df_filtrado'.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "nns9fkpAmej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Visualização dos valores únicos da coluna 'Nome de Curso' em `df_filtrado`**\n",
        "\n",
        "if 'df_filtrado' in locals() and df_filtrado is not None and 'Nome de Curso' in df_filtrado.columns:\n",
        "    unique_nomes_curso = sorted(df_filtrado['Nome de Curso'].unique())\n",
        "    print(\"Valores únicos na coluna 'Nome de Curso' (df_filtrado):\")\n",
        "    print(f\"Total de valores únicos: {len(unique_nomes_curso)}\")\n",
        "    for nome in unique_nomes_curso:\n",
        "        print(f\"- {nome}\")\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Nome de Curso' não encontrada no DataFrame 'df_filtrado'.\")"
      ],
      "metadata": {
        "id": "JQANcTG-l40Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Alicação de prefixos aos nomes de cursos para evitar padronização de nomes iguais, mas de diferentes modalidades/tipos**\n",
        "def renomear_cursos(row):\n",
        "    \"\"\"\n",
        "    Verifica o 'Tipo de Curso' em uma linha do DataFrame e adiciona o\n",
        "    prefixo correspondente ao 'Nome de Curso'.\n",
        "    \"\"\"\n",
        "    tipo_curso = row['Tipo de Curso']\n",
        "    nome_curso = row['Nome de Curso']\n",
        "\n",
        "    if tipo_curso == 'Licenciatura':\n",
        "        return f'Licenciatura em {nome_curso}'\n",
        "    elif tipo_curso == 'Qualificação Profissional (FIC)':\n",
        "        # Adiciona o prefixo 'FIC em' em vez de 'Qualificação Profissional (FIC) em'\n",
        "        return f'FIC {nome_curso}'\n",
        "    elif tipo_curso == 'Tecnologia':\n",
        "        return f'Tecnologia em {nome_curso}'\n",
        "    else:\n",
        "        # Se não for nenhum dos tipos especificados, retorna o nome original\n",
        "        return nome_curso\n",
        "\n",
        "# --- Criação da nova coluna 'Cursos' ---\n",
        "# A função 'renomear_cursos' é aplicada a cada linha (axis=1) do DataFrame.\n",
        "# O resultado é armazenado na nova coluna 'Cursos'.\n",
        "df_filtrado['Cursos'] = df_filtrado.apply(renomear_cursos, axis=1)\n",
        "\n",
        "# --- Exibição do DataFrame final com a nova coluna ---\n",
        "print(\"--- DataFrame com a Nova Coluna 'Cursos' ---\")\n",
        "display(df_filtrado)"
      ],
      "metadata": {
        "id": "lddPlmDplP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir valores únicos da coluna 'Cursos' em df_filtrado\n",
        "if 'df_filtrado' in locals() and df_filtrado is not None and 'Cursos' in df_filtrado.columns:\n",
        "    unique_cursos = sorted(df_filtrado['Cursos'].unique())\n",
        "    print(\"Valores únicos na coluna 'Cursos' (df_filtrado):\")\n",
        "    print(f\"Total de valores únicos: {len(unique_cursos)}\")\n",
        "    for nome in unique_cursos:\n",
        "        print(f\"- {nome}\")\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Cursos' não encontrada no DataFrame 'df_filtrado'.\")"
      ],
      "metadata": {
        "id": "dXbDqNUfwjhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular e exibir a diferença em valores únicos dos cursos antes e depois do tratamento\n",
        "if ('df_filtrado' in locals() and df_filtrado is not None and\n",
        "    'Nome de Curso' in df_filtrado.columns and 'Cursos' in df_filtrado.columns):\n",
        "\n",
        "    unique_nomes_curso = df_filtrado['Nome de Curso'].unique()\n",
        "    unique_cursos = df_filtrado['Cursos'].unique()\n",
        "\n",
        "    num_unique_nomes_curso = len(unique_nomes_curso)\n",
        "    num_unique_cursos = len(unique_cursos)\n",
        "    difference = num_unique_nomes_curso - num_unique_cursos\n",
        "\n",
        "    print(f\"Total de valores únicos na coluna 'Nome de Curso' (df_filtrado): {num_unique_nomes_curso}\")\n",
        "    print(f\"Total de valores únicos na coluna 'Cursos' (df_filtrado): {num_unique_cursos}\")\n",
        "    print(f\"Diferença no número de valores únicos: {difference}\")\n",
        "\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"As colunas 'Nome de Curso' ou 'Cursos' não foram encontradas no DataFrame 'df_filtrado'.\")"
      ],
      "metadata": {
        "id": "5QolQ07hC6ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 6: Script para tratamento detalhado dos parâmetros da coluna 'Cursos'**\n",
        "\n",
        "def padronizar_nome_curso(df, nome_coluna='Cursos'):\n",
        "    \"\"\"\n",
        "    Aplica uma série de regras de padronização a uma coluna de nomes de cursos em um DataFrame do pandas.\n",
        "\n",
        "    As etapas de limpeza incluem:\n",
        "    1. Remoção de informações redundantes (prefixos, sufixos de modalidade, etc.).\n",
        "    2. Normalização de variações complexas e padronização de nomes específicos.\n",
        "    3. Correção de erros de digitação e variações simples.\n",
        "    4. Padronização da capitalização para Title Case, preservando conectivos em minúsculo.\n",
        "    5. Remoção de espaços em branco extras.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): O DataFrame contendo os dados.\n",
        "        nome_coluna (str): O nome da coluna com os nomes dos cursos a serem padronizados.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: O DataFrame com a coluna de nomes de cursos padronizada.\n",
        "    \"\"\"\n",
        "    # Copia a coluna para evitar alterar o DataFrame original durante o processo\n",
        "    cursos_padronizados = df[nome_coluna].copy().astype(str)\n",
        "\n",
        "    # --- Etapa 1: Remover informações redundantes (prefixos e sufixos) ---\n",
        "    prefixos_para_remover = [\n",
        "        r'^fic\\s*-\\s*',\n",
        "        r'^pós graduação em\\s*',\n",
        "        r'^pós-graduação em\\s*',\n",
        "        r'^especialização \\(lato sensu\\)\\s*-\\s*',\n",
        "        r'^especialização\\s*-\\s*',\n",
        "        r'^mestrado\\s*-\\s*',\n",
        "        r'^doutorado\\s*-\\s*',\n",
        "        r'^mestrado profissional\\s*-\\s*',\n",
        "        r'^fic qualificação profissional\\s*-\\s*',\n",
        "        r'curso de Pós-graduação Lato Sensu '\n",
        "    ]\n",
        "    for prefixo in prefixos_para_remover:\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(prefixo, '', regex=True, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove sufixos de modalidade\n",
        "    sufixos_para_remover = [\n",
        "        r',\\s*na modalidade ead$',\n",
        "        r',\\s*na modalidade semipresencial$'\n",
        "    ]\n",
        "    for sufixo in sufixos_para_remover:\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(sufixo, '', regex=True, flags=re.IGNORECASE)\n",
        "\n",
        "    # --- Etapa 2: Normalização de variações complexas ---\n",
        "    # Este dicionário usa regex para encontrar padrões e substituir pelo nome padronizado.\n",
        "    # A ordem é importante: regras mais específicas devem vir antes.\n",
        "    regras_de_normalizacao = {\n",
        "        # --- Doutorados ---\n",
        "        # --- AJUSTE ---: Adicionada regra para Doutorado Acadêmico em Educação Profissional.\n",
        "        r'.*Doutorado Acad[êe]mico em Educaç[ãa]o Profissional.*': 'Doutorado em Educação Profissional e Tecnológica',\n",
        "        # --- AJUSTE ---: Adicionada regra para Doutorado em Desenvolvimento Educacional e Social.\n",
        "        # r'.*Doutorado.*Desenvolvimento Educacional e Social.*': 'Doutorado em Desenvolvimento Educacional e Social',\n",
        "        r'.*Doutorado em Ensino.*(Rede Nordeste|renoen).*': 'Doutorado em Ensino (RENOEN)',\n",
        "        r'Rede Nordeste de Ensino - RENOEN': 'Doutorado em Ensino (RENOEN)',\n",
        "\n",
        "        # --- Mestrados ---\n",
        "        r'.*Mestrado.*Educação Profissional e Tecnológica.*': 'Mestrado em Educação Profissional e Tecnológica (ProfEPT)',\n",
        "        r'Mestrado Acadêmico em Educação Profissional': 'Mestrado em Educação Profissional e Tecnológica (ProfEPT)',\n",
        "        r'Mestrado em Ensino': 'Mestrado em Ensino (Posensino)',\n",
        "        r'Mestrado Profissional - Recursos Naturais': 'Mestrado Profissional em Uso Sustentável dos Recursos Naturais',\n",
        "        r'Uso Sustentável dos Recursos Naturais': 'Mestrado Profissional em Uso Sustentável dos Recursos Naturais',\n",
        "\n",
        "        # --- Especializações ---\n",
        "        r'Especialização em Educação Ambiental e Geografia do Semi-Árido': 'Especialização em Educação Ambiental e Geografia do Semiárido',\n",
        "        r'Educação Ambiental e Geografia do Semiarido': 'Especialização em Educação Ambiental e Geografia do Semiárido',\n",
        "        r'.*Ci[êe]ncias Humanas e Saberes Contempor[âa]neos.*': 'Especialização em Ciências Humanas e Saberes Contemporâneos para a Educação',\n",
        "        r'Ciências Humanas e Competências Contemporâneas para a Educação': 'Especialização em Ciências Humanas e Competências Contemporâneas para a Educação',\n",
        "        r'Docência para a Educação Profissional e Tecnológica (DocentEPT)': 'Especialização em Docência na Educação Profissional e Tecnológica (DocentEPT)',\n",
        "        r'Especialização em Eja No Contexto da Diversidade': 'Especialização em Educação de Jovens e Adultos no Contexto da Diversidade',\n",
        "        r'Engenharia de Segurança do Trabalho': 'Especialização em Engenharia de Segurança do Trabalho',\n",
        "        r'Ensino de Ciências Biológicas': 'Especialização em Ensino de Ciências Biológicas',\n",
        "        r'Ensino de Ciências Naturais e Matemática': 'Especialização em Ensino de Ciências Naturais e Matemática',\n",
        "        r'Ensino de Geociências': 'Especialização em Ensino de Geociências',\n",
        "        r'Ensino de Teatro': 'Especialização em Ensino de Teatro',\n",
        "        r'Especialização em Ciência e Tecnologia de Alimentos, Na Modalidade Ead': 'Especialização em Ciência e Tecnologia de Alimentos',\n",
        "        r'Especialização em Ensino de Língua Portuguesa e Matemática Transdisciplinar': 'Especialização em Ensino da Língua Portuguesa e Matemática numa Abordagem Transdisciplinar',\n",
        "        r'Especialização em Docência para a Educação Profissional e Tecnológica – Docentept - Uab': 'Especialização em Docência na Educação Profissional e Tecnológica (DocentEPT)',\n",
        "        r'Estudos Linguísticos e Literários': 'Especialização em Estudos Linguísticos e Literários',\n",
        "        r'Especialização em Práticas Assertivas da Educação Profissional Integrada À Educação de Jovens e Adultos - Com Ênfase em Didática': 'Especialização em Práticas Assertivas em Didática e Gestão da Educação Profissional integrada à EJA',\n",
        "        r'^Gestão Ambiental$': 'Especialização em Gestão Ambiental',\n",
        "        r'Licenciatura em Licenciatura para a Educação Profissional, Científica e Tecnológica': 'Licenciatura em Formação Pedagógica para a Educação Básica, Profissional e Tecnológica'\n",
        "     }\n",
        "\n",
        "    for padrao, nome_correto in regras_de_normalizacao.items():\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(padrao, nome_correto, regex=True, flags=re.IGNORECASE)\n",
        "\n",
        "    # --- Etapa 3: Corrigir erros de digitação e variações simples ---\n",
        "    correcoes_simples = {\n",
        "        'Conteporaneidade': 'Contemporaneidade',\n",
        "        'À': 'à', # Corrige capitalização de crase\n",
        "        'Semi-Árido': 'Semiárido',\n",
        "        'Eja No Contexto da Diversidade': 'EJA no Contexto da Diversidade',\n",
        "        'para O Ensino Médio': 'para o Ensino Médio',\n",
        "        'desportiva e de Lazer': 'Desportiva e de Lazer'\n",
        "    }\n",
        "    for erro, correcao in correcoes_simples.items():\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(erro, correcao, regex=False)\n",
        "\n",
        "    # --- Etapa 4: Padronização da capitalização ---\n",
        "    # Converte para Title Case\n",
        "    cursos_padronizados = cursos_padronizados.str.title()\n",
        "\n",
        "    # Lista de conectivos e palavras curtas que devem ser minúsculas\n",
        "    conectivos_minusculos = ['de', 'da', 'do', 'dos', 'das', 'em', 'e', 'para', 'o', 'a', 'à', 'os', 'as', 'no', 'na', 'nos', 'nas', 'com']\n",
        "\n",
        "    def apply_title_case_with_exceptions(name):\n",
        "        # Garante que siglas como IFRN, EJA, EAD, etc., fiquem em maiúsculas\n",
        "        name = re.sub(r'\\b(If(rn)?|Eja|Ead|Fic|Ppi|Pcd|Renoen|Profept|Docentept|Posensino)\\b', lambda m: m.group().upper(), name, flags=re.IGNORECASE)\n",
        "        words = name.split()\n",
        "        processed_words = [word.lower() if word.lower() in conectivos_minusculos and i > 0 else word for i, word in enumerate(words)]\n",
        "        return ' '.join(processed_words)\n",
        "\n",
        "    cursos_padronizados = cursos_padronizados.apply(apply_title_case_with_exceptions)\n",
        "\n",
        "    # --- Etapa 5: Remover espaços em branco extras ---\n",
        "    cursos_padronizados = cursos_padronizados.str.strip()\n",
        "    cursos_padronizados = cursos_padronizados.str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    # Atribui a coluna padronizada de volta ao DataFrame\n",
        "    df_tratado = df.copy()\n",
        "    df_tratado[nome_coluna] = cursos_padronizados\n",
        "\n",
        "    return df_tratado\n",
        "\n",
        "# Aplica a função de padronização à coluna 'Cursos'\n",
        "df_tratado = padronizar_nome_curso(df_filtrado.copy(), 'Cursos')\n",
        "\n",
        "if df_tratado is not None:\n",
        "    num_rows, num_cols = df_tratado.shape\n",
        "    print(f\"DataFrame 'df_tratado' criado como uma cópia de 'df_filtrado' tem {num_rows:,} linhas e {num_cols} colunas.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")\n",
        "\n",
        "# Exibir valores únicos em ordem alfabética na coluna 'Cursos'\n",
        "if df_tratado is not None and 'Cursos' in df_tratado.columns:\n",
        "    unique_cursos = sorted(df_tratado['Cursos'].unique())\n",
        "    print(\"Valores únicos na coluna 'Cursos':\")\n",
        "    print(f\"Total de valores únicos na coluna 'Cursos' após limpeza: {len(unique_cursos)}\")\n",
        "    for curso in unique_cursos:\n",
        "        print(f\"- {curso}\")\n",
        "elif df_tratado is None:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Cursos' não encontrada no DataFrame.\")\n"
      ],
      "metadata": {
        "id": "KRIFNHQ80BVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Calcular e exibir a diferença em valores únicos antes e após tratamento dos nomes de cursos**\n",
        "if ('df_filtrado' in locals() and df_filtrado is not None and 'Nome de Curso' in df_filtrado.columns and\n",
        "    'df_tratado' in locals() and df_tratado is not None and 'Cursos' in df_tratado.columns):\n",
        "\n",
        "    unique_nomes_curso_filtrado = df_filtrado['Nome de Curso'].unique()\n",
        "    unique_cursos_tratado = df_tratado['Cursos'].unique()\n",
        "\n",
        "    num_unique_nomes_curso_filtrado = len(unique_nomes_curso_filtrado)\n",
        "    num_unique_cursos_tratado = len(unique_cursos_tratado)\n",
        "    difference = num_unique_nomes_curso_filtrado - num_unique_cursos_tratado\n",
        "\n",
        "    print(f\"Total de valores únicos na coluna 'Nome de Curso' (df_filtrado): {num_unique_nomes_curso_filtrado}\")\n",
        "    print(f\"Total de valores únicos na coluna 'Cursos' (df_tratado): {num_unique_cursos_tratado}\")\n",
        "    print(f\"Diferença no número de valores únicos: {difference}\")\n",
        "\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None or 'Nome de Curso' not in df_filtrado.columns:\n",
        "    print(\"DataFrame 'df_filtrado' ou a coluna 'Nome de Curso' não encontrada. Execute as etapas anteriores.\")\n",
        "elif 'df_tratado' not in locals() or df_tratado is None or 'Cursos' not in df_tratado.columns:\n",
        "     print(\"DataFrame 'df_tratado' ou a coluna 'Cursos' não encontrada. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "cY4IteLXDnDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Mapear os valores em 'Matrícula Atendida' e converter para `booleano`**\n",
        "if 'df_tratado' in locals() and df_tratado is not None and 'Matrícula Atendida' in df_tratado.columns:\n",
        "    print(\"Convertendo a coluna 'Matrícula Atendida' para booleano...\")\n",
        "    mapping = {\"Sim\": 1, \"Y\": 1, \"Não Informado\": 0}\n",
        "    # Certifica-se de que a coluna é do tipo objeto para lidar com tipos mistos antes do mapeamento\n",
        "    df_tratado['Matrícula Atendida'] = df_tratado['Matrícula Atendida'].astype(str).map(mapping).astype(bool)\n",
        "    print(\"Conversão concluída.\")\n",
        "    print(\"\\nContagem dos valores na coluna 'Matrícula Atendida' após conversão:\")\n",
        "    display(df_tratado['Matrícula Atendida'].value_counts())\n",
        "elif 'df_tratado' not in locals() or df_tratado is None:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Matrícula Atendida' não encontrada no DataFrame 'df_tratado'.\")"
      ],
      "metadata": {
        "id": "SMJGe5UGP883"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Ajuste/correção de parâmetros em diversas colunas, renomeia adequadamente as colunas em `df_tratado` a ser exportado para o BigQuery**\n",
        "\n",
        "# Renomeia colunas adequadamente para o BigQuery para evitar 'KeyErrors'\n",
        "df_tratado = df_tratado.rename(columns={\n",
        "    'Carga Horaria': 'Carga_Horaria',\n",
        "    'Carga Horaria Mínima': 'Carga_Horaria_Minima',\n",
        "    'Categoria da Situação': 'Categoria_da_Situacao',\n",
        "    'Co Inst': 'Co_Inst',\n",
        "    'Cod Unidade': 'Cod_Unidade',\n",
        "    'Cor / Raça': 'Cor_Raça',\n",
        "    'Código da Matricula': 'Codigo_da_Matricula',\n",
        "    'Código da Unidade de Ensino - SISTEC': 'Codigo_da_Unidade_de_Ensino_-_SISTEC',\n",
        "    'Código do Ciclo Matricula': 'Codigo_do_Ciclo_Matricula',\n",
        "    'Código do Município com DV': 'Codigo_do_Municipio_com_DV',\n",
        "    'Data de Fim Previsto do Ciclo': 'Data_de_Fim_Previsto_do_Ciclo',\n",
        "    'Data de Inicio do Ciclo': 'Data_de_Inicio_do_Ciclo',\n",
        "    'Data de Ocorrencia da Matricula': 'Data_de_Ocorrencia_da_Matricula',\n",
        "    'Situação de Matrícula': 'Situacao_de_Matricula',\n",
        "    'Subeixo Tecnológico': 'Subeixo_Tecnologico',\n",
        "    'Matrícula Atendida': 'Matricula_Atendida',\n",
        "    'Eixo Tecnológico': 'Eixo_Tecnologico',\n",
        "    'Faixa Etária': 'Faixa_Etaria',\n",
        "    'Fator Esforço Curso': 'Fator_Esforco_Curso',\n",
        "    'Fonte de Financiamento': 'Fonte_de_Financiamento',\n",
        "    'Forma de ingresso': 'Forma_de_Ingresso',\n",
        "    'Instituição': 'Instituicao',\n",
        "    'Mês De Ocorrência da Situação': 'Mes_De_Ocorrencia_da_Situacao',\n",
        "    'Município': 'Municipio',\n",
        "    'Modalidade de Ensino': 'Modalidade_Educacional',\n",
        "    'Tipo de Curso': 'Nivel_Educacional',\n",
        "    'Tipo de Oferta': 'Modalidade_de_Curso',\n",
        "    'Nome de Curso': 'Nome_de_Curso',\n",
        "    'Total de Inscritos': 'Total_de_Inscritos',\n",
        "    'Região': 'Regiao',\n",
        "    'Renda Familiar': 'Renda_Familiar',\n",
        "    'Sexo': 'Sexo',\n",
        "    'Unidade de Ensino': 'Campus_do_IFRN',\n",
        "    'Vagas Extraordinárias AC': 'Vagas_Extraordinarias_AC',\n",
        "    'Vagas Extraordinárias l1': 'Vagas_Extraordinarias_l1',\n",
        "    'Vagas Extraordinárias l10': 'Vagas_Extraordinarias_l10',\n",
        "    'Vagas Extraordinárias l13': 'Vagas_Extraordinarias_l13',\n",
        "    'Vagas Extraordinárias l14': 'Vagas_Extraordinarias_l14',\n",
        "    'Vagas Extraordinárias l2': 'Vagas_Extraordinarias_l2',\n",
        "    'Vagas Extraordinárias l5': 'Vagas_Extraordinarias_l5',\n",
        "    'Vagas Extraordinárias l6': 'Vagas_Extraordinarias_l6',\n",
        "    'Vagas Extraordinárias l9': 'Vagas_Extraordinarias_l9',\n",
        "    'Vagas Ofertadas': 'Vagas_Ofertadas',\n",
        "    'Vagas Regulares AC': 'Vagas_Regulares_AC',\n",
        "    'Vagas Regulares l1': 'Vagas_Regulares_l1',\n",
        "    'Vagas Regulares l10': 'Vagas_Regulares_l10',\n",
        "    'Vagas Regulares l13': 'Vagas_Regulares_l13',\n",
        "    'Vagas Regulares l14': 'Vagas_Regulares_l14',\n",
        "    'Vagas Regulares l2': 'Vagas_Regulares_l2',\n",
        "    'Vagas Regulares l5': 'Vagas_Regulares_l5',\n",
        "    'Vagas Regulares l6': 'Vagas_Regulares_l6',\n",
        "    'Vagas Regulares l9': 'Vagas_Regulares_l9',\n",
        "})\n",
        "\n",
        "\n",
        "# Correção de parâmetros na coluna 'Eixo Tecnológico'\n",
        "replacements_eixo = {\n",
        "    'GESTÃO E NEGÓCIOS': 'Gestão e Negócios',\n",
        "    'DESENVOLVIMENTO EDUCACIONAL E SOCIAL': 'Desenvolvimento Educacional e Social'\n",
        "}\n",
        "if 'Eixo_Tecnologico' in df_tratado.columns:\n",
        "  df_tratado['Eixo_Tecnologico'] = df_tratado['Eixo_Tecnologico'].replace(replacements_eixo)\n",
        "\n",
        "# Correção de parâmetros na coluna 'Subeixo Tecnológico'\n",
        "replacements_subeixo = {\n",
        "    'AMBIENTE E SAÚDE': 'Ambiente e Saúde'\n",
        "}\n",
        "if 'Subeixo_Tecnologico' in df_tratado.columns:\n",
        "  df_tratado['Subeixo_Tecnologico'] = df_tratado['Subeixo_Tecnologico'].replace(replacements_subeixo) # Changed from replacements_eixo\n",
        "\n",
        "# Correção de parâmetros em 'Campus_do_IFRN'\n",
        "replacements_unidade = {\n",
        "    'Campus Ceará-mirim': 'Ceará-Mirim',\n",
        "    'Campus Pau Dos Ferros': 'Pau dos Ferros'\n",
        "}\n",
        "if 'Campus_do_IFRN' in df_tratado.columns:\n",
        "    # Aplicação de regras\n",
        "    df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].replace(replacements_unidade)\n",
        "    # Remoção d palavras 'Campus ' e 'Campus Avançado ' de 'Campus_do_IFRN'\n",
        "    df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].str.replace('Campus Avançado ', '', regex=False)\n",
        "    df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].str.replace('Campus ', '', regex=False)\n",
        "\n",
        "# Renomeia parâmetros na coluna 'Forma de ingresso'\n",
        "replacements_ingresso = {\n",
        "    'AC': 'AC: Ampla Concorrência',\n",
        "    'l1': 'L1: Renda Baixa',\n",
        "    'l2': 'L2: Renda Baixa + PPI',\n",
        "    'l5': 'L5: Ensino Médio em Escolas Pública',\n",
        "    'l6': 'L6: PPI (Pretos, Pardos e Indígenas)',\n",
        "    'l9': 'L9: Renda Baixa + PcD',\n",
        "    'l10': 'L10: Renda Baixa + PPI + PcD',\n",
        "    'l13': 'L13: PcD (Independente de Renda)',\n",
        "    'l14': 'L14: PcD + PPI (Independente de Renda)',\n",
        "    'LB_EP': 'LB EP: Baixa Renda + Ensino Médio em Escola Pública',\n",
        "    'LB_PCD': 'LB PCD: Baixa Renda + PcD',\n",
        "    'LB_PPI': 'LB PPI: Baixa Renda + PPI',\n",
        "    'LB_Q': 'LB Q: Baixa Renda + Quilombolas + Ensino Médio em Escola Pública',\n",
        "    'LI_EP': 'LI EP: Ensino Médio em Escola Pública (Independentemente da Renda)',\n",
        "    'LI_PCD': 'LI PCD: PcD (Independente de Renda)',\n",
        "    'LI_PPI': 'LI PPI: PPI (Independente de Renda)',\n",
        "    'LI_Q': 'LI Q: Quilombolas + Ensino Médio em Escola Pública (Independente da Renda)',\n",
        "    'nan': 'NI: Não informado'\n",
        "}\n",
        "\n",
        "if 'Forma_de_Ingresso' in df_tratado.columns:\n",
        "    # Certifica que a coluna seja do tipo objeto para lidar com substituições de strings, incluindo 'nan'\n",
        "    df_tratado['Forma_de_Ingresso'] = df_tratado['Forma_de_Ingresso'].astype(str).replace(replacements_ingresso)\n",
        "\n",
        "# Adiciona padronização em parâmetro da coluna 'Categoria da Situação'\n",
        "replacements_categoria_situacao = {\n",
        "    'Em curso': 'Em Curso',\n",
        "}\n",
        "if 'Categoria_da_Situacao' in df_tratado.columns:\n",
        "    df_tratado['Categoria_da_Situacao'] = df_tratado['Categoria_da_Situacao'].replace(replacements_categoria_situacao)\n",
        "\n",
        "# Substituir em 'Renda_Familiar' as ocorrências de 'S/I' para 'Não declarada'\n",
        "df_tratado['Renda_Familiar'] = df_tratado['Renda_Familiar'].replace('S/I', 'Não declarada')\n",
        "\n",
        "# Renomear categorias em 'Renda_Familiar'\n",
        "renomear_renda = {\n",
        "    '0<RFP<=0,5': '(1) 0<RFP<=0,5',\n",
        "    '0,5<RFP<=1':  '(2) 0,5<RFP<=1',\n",
        "    '1<RFP<=1,5': '(3) 1<RFP<=1,5',\n",
        "    '1,5<RFP<=2,5': '(4) 1,5<RFP<=2,5',\n",
        "    '2,5<RFP<=3,5': '(5) 2,5<RFP<=3,5',\n",
        "    'RFP>3,5': '(6) RFP>3,5',\n",
        "    'Não declarada': '(7) Não declarada'\n",
        "}\n",
        "\n",
        "# Aplicar a renomeação utilizando o método .replace() à coluna\n",
        "df_tratado['Renda_Familiar'] = df_tratado['Renda_Familiar'].replace(renomear_renda)\n",
        "\n",
        "\n",
        "# Reposiona 'Cursos' depois de 'Nome_de_Curso'\n",
        "if 'Cursos' in df_tratado.columns and 'Nome_de_Curso' in df_tratado.columns:\n",
        "    cols = df_tratado.columns.tolist()\n",
        "    # Find the index of 'Cursos' and 'Nome_de_Curso'\n",
        "    cursos_index = cols.index('Cursos')\n",
        "    nome_curso_index = cols.index('Nome_de_Curso')\n",
        "\n",
        "    # Remove 'Cursos' from its current position\n",
        "    cursos_col = cols.pop(cursos_index)\n",
        "\n",
        "    # Insert 'Cursos' after 'Nome_de_Curso'\n",
        "    cols.insert(nome_curso_index + 1, cursos_col)\n",
        "\n",
        "    # Reindex the DataFrame with the new column order\n",
        "    df_tratado = df_tratado[cols]\n",
        "    print(\"Coluna 'Cursos' reposicionada após 'Nome_de_Curso'.\")\n",
        "elif 'Cursos' not in df_tratado.columns:\n",
        "    print(\"Aviso: Coluna 'Cursos' não encontrada para reposicionamento.\")\n",
        "elif 'Nome_de_Curso' not in df_tratado.columns:\n",
        "    print(\"Aviso: Coluna 'Nome_de_Curso' não encontrada para servir de referência.\")\n",
        "\n",
        "\n",
        "# Exibir informações sobre as colunas e tipos de dados de 'df_tratado'\n",
        "if df_tratado is not None:\n",
        "    print(\"Informações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "sbLxpB5GtpqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Verificação de valores únicos para as colunas indicadas**\n",
        "if df_tratado is not None:\n",
        "    columns_to_list = ['Campus_do_IFRN', 'Eixo_Tecnologico', 'Forma_de_Ingresso', 'Renda_Familiar', 'Categoria_da_Situacao', 'Situacao_de_Matricula']\n",
        "\n",
        "    for col in columns_to_list:\n",
        "        if col in df_tratado.columns:\n",
        "            # Converte a coluna para o tipo 'string' antes de classificar valores exclusivos\n",
        "            unique_values = sorted(df_tratado[col].astype(str).unique())\n",
        "            print(f\"Valores únicos em ordem alfabética na coluna '{col}':\")\n",
        "            for value in unique_values:\n",
        "                print(f\"- {value}\")\n",
        "            print(\"-\" * 20) # Separador\n",
        "        else:\n",
        "            print(f\"A coluna '{col}' não foi encontrada no DataFrame 'df_tratado'.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "_So1hD4dltnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Tratamento dos tipos de dados das colunas de `df_tratado`**\n",
        "def converter_tipos_de_dados(df: pd.DataFrame, colunas_para_int: list, colunas_para_str: list, colunas_para_data: dict, colunas_para_float: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converte os tipos de dados de colunas especificadas em um DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): O DataFrame a ser modificado.\n",
        "        colunas_para_int (list): Uma lista de nomes de colunas para converter para o tipo Int64 (inteiro que suporta nulos).\n",
        "        colunas_para_str (list): Uma lista de nomes de colunas para converter para o tipo string.\n",
        "        colunas_para_data (dict): Um dicionário onde as chaves são os nomes das colunas\n",
        "                                  para converter para data e os valores são os formatos esperados (ex: {'coluna': '%d/%m/%Y'}).\n",
        "        colunas_para_float (dict): Um dicionário onde as chaves são os nomes das colunas\n",
        "                                   para converter para float e os valores são os caracteres a serem substituídos\n",
        "                                   antes da conversão (ex: {'coluna': ','}).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Uma cópia do DataFrame com os tipos de dados convertidos.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"Erro: O objeto fornecido não é um DataFrame do pandas.\")\n",
        "        return None\n",
        "\n",
        "    # Cria uma cópia para evitar modificar o DataFrame original\n",
        "    df_processado = df.copy()\n",
        "\n",
        "    print(\"--- Iniciando conversão de tipos de dados ---\")\n",
        "\n",
        "    # --- Conversão para Inteiro (Int64) ---\n",
        "    print(\"\\nConvertendo colunas para Inteiro:\")\n",
        "    for col in colunas_para_int:\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                # Converte para float primeiro para lidar com NaN, depois para Int64 (que suporta NaN)\n",
        "                df_processado[col] = pd.to_numeric(df_processado[col], errors='coerce').astype('Int64')\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para Int64.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}': {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    # --- Conversão para String ---\n",
        "    print(\"\\nConvertendo colunas para String:\")\n",
        "    for col in colunas_para_str:\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                # Converte valores nulos para uma string vazia antes de mudar o tipo\n",
        "                df_processado[col] = df_processado[col].fillna('').astype(str)\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para string.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}': {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    # --- Conversão para Data ---\n",
        "    print(\"\\nConvertendo colunas para Data:\")\n",
        "    for col, fmt in colunas_para_data.items():\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                df_processado[col] = pd.to_datetime(df_processado[col], format=fmt, errors='coerce')\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para data com formato '{fmt}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}' para data: {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    # --- Conversão para Float (float64) ---\n",
        "    print(\"\\nConvertendo colunas para Float:\")\n",
        "    for col, char_to_replace in colunas_para_float.items():\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                # Substitui o caractere e converte para numérico\n",
        "                df_processado[col] = df_processado[col].astype(str).str.replace(char_to_replace, '.', regex=False)\n",
        "                df_processado[col] = pd.to_numeric(df_processado[col], errors='coerce').astype('float64')\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para float64.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}' para float64: {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    print(\"\\n--- Conversão concluída ---\")\n",
        "    return df_processado\n",
        "\n",
        "# Define as listas de colunas\n",
        "colunas_int = [\n",
        "    'Carga_Horaria_Minima', 'Idade', 'Total_de_Inscritos',\n",
        "    'Vagas_Extraordinarias_AC', 'Vagas_Extraordinarias_l1', 'Vagas_Extraordinarias_l10',\n",
        "    'Vagas_Extraordinarias_l13', 'Vagas_Extraordinarias_l14', 'Vagas_Extraordinarias_l2',\n",
        "    'Vagas_Extraordinarias_l5', 'Vagas_Extraordinarias_l6', 'Vagas_Extraordinarias_l9',\n",
        "    'Vagas_Ofertadas', 'Vagas_Regulares_AC', 'Vagas_Regulares_l1', 'Vagas_Regulares_l10',\n",
        "    'Vagas_Regulares_l13', 'Vagas_Regulares_l14', 'Vagas_Regulares_l2', 'Vagas_Regulares_l5',\n",
        "    'Vagas_Regulares_l6', 'Vagas_Regulares_l9'\n",
        "]\n",
        "colunas_str = [\n",
        "    'Cod_Unidade',\n",
        "    'Co_Inst',\n",
        "    'Codigo_da_Matricula',\n",
        "    'Codigo_da_Unidade_de_Ensino_-_SISTEC',\n",
        "    'Codigo_do_Ciclo_Matricula',\n",
        "    'Codigo_do_Municipio_com_DV',\n",
        "    'Eixo_Tecnologico',\n",
        "    'Forma_de_Ingresso',\n",
        "    'Instituicao',\n",
        "    'Modalidade_Educacional',\n",
        "    'Nivel_Educacional',\n",
        "    'Modalidade_de_Curso',\n",
        "    'Matricula_Atendida',\n",
        "    'Renda_Familiar',\n",
        "    'Subeixo_Tecnologico',\n",
        "    'Categoria_da_Situacao',\n",
        "]\n",
        "colunas_data = {\n",
        "    'Data_de_Fim_Previsto_do_Ciclo': '%d/%m/%Y',\n",
        "    'Data_de_Inicio_do_Ciclo': '%d/%m/%Y',\n",
        "    'Data_de_Ocorrencia_da_Matricula': '%d/%m/%Y'\n",
        "}\n",
        "\n",
        "colunas_float = {\n",
        "    'Fator_Esforco_Curso': ',' # Substituir vírgula por ponto\n",
        "}\n",
        "\n",
        "# Chama a função e armazena o resultado em um novo DataFrame\n",
        "df_tratado = converter_tipos_de_dados(df_tratado, colunas_int, colunas_str, colunas_data, colunas_float)\n",
        "\n",
        "# Verifica os tipos de dados após a conversão (opcional)\n",
        "if df_tratado is not None:\n",
        "  print(\"\\nTipos de dados após a conversão:\")\n",
        "  colunas_convertidas = colunas_int + colunas_str + list(colunas_data.keys()) + list(colunas_float.keys())\n",
        "  print(df_tratado[colunas_convertidas].dtypes)"
      ],
      "metadata": {
        "id": "UOm4infX6cc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Exibir informações sobre as colunas e tipos de dados de `df_tratado`**\n",
        "if df_tratado is not None:\n",
        "    print(\"Informações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "YsF-NGGwRT_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 8: Cálculo de métricas específicas a partir dos microdados da PNP**\n",
        "\n",
        "# Código para calcular os totais de 'Vagas' e 'Inscritos' a partir dos microdados da PNP,\n",
        "# preparando-os para agregação em ferramentas de BI como Looker Studio ou Power BI.\n",
        "\n",
        "# Reconstrução dos dados a partir das colunas que detalham a distribuição de vagas por cotas.\n",
        "COL_VAGAS_REG = [\n",
        "    \"Vagas_Regulares_AC\",\"Vagas_Regulares_l1\",\"Vagas_Regulares_l2\",\n",
        "    \"Vagas_Regulares_l5\",\"Vagas_Regulares_l6\",\"Vagas_Regulares_l9\",\n",
        "    \"Vagas_Regulares_l10\",\"Vagas_Regulares_l13\",\"Vagas_Regulares_l14\",\n",
        "]\n",
        "COL_VAGAS_EXT = [\n",
        "    \"Vagas_Extraordinarias_AC\",\"Vagas_Extraordinarias_l1\",\"Vagas_Extraordinarias_l2\",\n",
        "    \"Vagas_Extraordinarias_l5\",\"Vagas_Extraordinarias_l6\",\"Vagas_Extraordinarias_l9\",\n",
        "    \"Vagas_Extraordinarias_l10\",\"Vagas_Extraordinarias_l13\",\"Vagas_Extraordinarias_l14\",\n",
        "]\n",
        "COL_VAGAS_TODAS = COL_VAGAS_REG + COL_VAGAS_EXT\n",
        "\n",
        "# --- PASSO 1: Reconstrução do Total de Vagas (Fallback para dados ausentes) ---\n",
        "# Esta etapa resolve um problema comum de qualidade nos dados da PNP, em que a coluna\n",
        "# principal 'Vagas_Ofertadas' pode estar nula ou zerada, enquanto os detalhes por cota estão preenchidos.\n",
        "# O código soma os detalhes para criar um total confiável.\n",
        "\n",
        "# Certifica se as colunas relevantes são numéricas antes de somar (opcional)\n",
        "for col in COL_VAGAS_TODAS:\n",
        "    if col in df_tratado.columns:\n",
        "        df_tratado[col] = pd.to_numeric(df_tratado[col], errors='coerce').astype('Int64')\n",
        "\n",
        "df_tratado[\"Vagas_categorias_soma\"] = df_tratado[COL_VAGAS_TODAS].fillna(0).sum(axis=1).astype('Int64')\n",
        "\n",
        "# --- PASSO 2: Criação de uma Coluna de Vagas Confiável ---\n",
        "# Criação de uma nova coluna ('Vagas_Ofertadas_calc') que prioriza o valor oficial de # 'Vagas_Ofertadas',\n",
        "# mas utiliza a soma reconstruída da etapa 1 como alternativa (fallback) caso o valor oficial não seja válido.\n",
        "# Isto garante que não haja perda de dados.\n",
        "# Verifica se 'Vagas_Ofertadas' é do tipo Int64 antes do cálculo\n",
        "if 'Vagas_Ofertadas' in df_tratado.columns:\n",
        "    df_tratado['Vagas_Ofertadas'] = pd.to_numeric(df_tratado['Vagas_Ofertadas'], errors='coerce').astype('Int64')\n",
        "\n",
        "# Aplica o método .where() do Pandas para manipulação correta de 'Int64'\n",
        "# Condição: 'Vagas_Ofertadas' is not null AND > 0\n",
        "condition = df_tratado[\"Vagas_Ofertadas\"].notna() & (df_tratado[\"Vagas_Ofertadas\"] > 0)\n",
        "df_tratado[\"Vagas_Ofertadas_calc\"] = df_tratado[\"Vagas_Ofertadas\"].where(\n",
        "    condition,\n",
        "    df_tratado[\"Vagas_categorias_soma\"]\n",
        ").astype(\"Int64\")\n",
        "\n",
        "# --- PASSO 3: Desduplicação dos Valores por Ciclo de Matrícula ---\n",
        "# Nos dados brutos, o número de vagas e inscritos de um ciclo se repete para CADA aluno matriculado,\n",
        "# inflacionando a soma direta. O código abaixo resolve isso atribuindo o valor total de vagas e inscritos\n",
        "# APENAS à primeira linha de cada ciclo, zerando as demais. Assim, uma simples SOMA dessas novas colunas\n",
        "# no Looker Studio resultará no valor correto.\n",
        "\n",
        "# Certifica se 'Total_de_Inscritos' é 'Int64'\n",
        "if 'Total_de_Inscritos' in df_tratado.columns:\n",
        "     df_tratado['Total_de_Inscritos'] = pd.to_numeric(df_tratado['Total_de_Inscritos'], errors='coerce').astype('Int64')\n",
        "\n",
        "# Agrupa o DataFrame por ciclo de matrícula.\n",
        "g = df_tratado.groupby(\"Codigo_do_Ciclo_Matricula\", dropna=False)\n",
        "\n",
        "# Para cada ciclo, encontra o valor máximo (e correto) de vagas e inscritos.\n",
        "# A função 'transform' aplica esse valor a todas as linhas pertencentes ao mesmo ciclo.\n",
        "vagas_ciclo = g[\"Vagas_Ofertadas_calc\"].transform(\"max\")\n",
        "inscritos_ciclo = g[\"Total_de_Inscritos\"].transform(\"max\")\n",
        "\n",
        "# Identifica qual linha é a primeira de cada ciclo de matrícula.\n",
        "primeira_linha_ciclo = ~df_tratado.duplicated(subset=[\"Codigo_do_Ciclo_Matricula\"], keep=\"first\")\n",
        "\n",
        "# Cria as colunas finais 'Vagas' e 'Inscritos' com o dtype correto, atribuição usando .loc\n",
        "df_tratado[\"Vagas\"] = pd.Series(0, index=df_tratado.index, dtype='Int64') # Inicializar com '0' do tipo correto\n",
        "df_tratado.loc[primeira_linha_ciclo, \"Vagas\"] = vagas_ciclo[primeira_linha_ciclo] # Atribuir diretamente da série filtrada\n",
        "\n",
        "df_tratado[\"Inscritos\"] = pd.Series(0, index=df_tratado.index, dtype='Int64') # Inicializar com '0' do tipo correto\n",
        "df_tratado.loc[primeira_linha_ciclo, \"Inscritos\"] = inscritos_ciclo[primeira_linha_ciclo] # Atribuir diretamente da série filtrada\n",
        "\n",
        "# --- PASSO 4: Padronização da Coluna 'Ano' ---\n",
        "# O campo 'Ano' original da PNP se refere ao ano da extração.\n",
        "# Este código cria um novo campo 'Ano_de_início_do_ciclo' baseado na data de início do ciclo, o que é\n",
        "# metodologicamente mais correto para analisar coortes e comparar vagas/inscritos.\n",
        "df_tratado[\"Ano_de_início_do_ciclo\"] = pd.to_datetime(df_tratado[\"Data_de_Inicio_do_Ciclo\"], errors='coerce').dt.year.astype('Int64')\n",
        "\n",
        "# Exibição de ´df_tratado´\n",
        "print(df_tratado[['Codigo_do_Ciclo_Matricula', 'Vagas_Ofertadas_calc', 'Vagas', 'Total_de_Inscritos', 'Inscritos', 'Ano']].head(10))\n",
        "print(f\"Soma correta de Vagas: {df_tratado['Vagas'].sum()}\")\n",
        "print(f\"Soma correta de Inscritos: {df_tratado['Inscritos'].sum()}\")"
      ],
      "metadata": {
        "id": "YvmarUCvhp7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 9: Exibição de estatísticas para colunas selecionadas de `df_tratado`**\n",
        "\n",
        "if df_tratado is not None and not df_tratado.empty:\n",
        "    columns_to_describe = [\n",
        "        'Idade',\n",
        "        'Total_de_Inscritos',\n",
        "        'Vagas_Extraordinarias_AC',\n",
        "        'Vagas_Extraordinarias_l1',\n",
        "        'Vagas_Extraordinarias_l10',\n",
        "        'Vagas_Extraordinarias_l13',\n",
        "        'Vagas_Extraordinarias_l14',\n",
        "        'Vagas_Extraordinarias_l2',\n",
        "        'Vagas_Extraordinarias_l5',\n",
        "        'Vagas_Extraordinarias_l6',\n",
        "        'Vagas_Extraordinarias_l9',\n",
        "        'Vagas_Ofertadas',\n",
        "        'Vagas_Regulares_AC',\n",
        "        'Vagas_Regulares_l1',\n",
        "        'Vagas_Regulares_l10',\n",
        "        'Vagas_Regulares_l13',\n",
        "        'Vagas_Regulares_l14',\n",
        "        'Vagas_Regulares_l2',\n",
        "        'Vagas_Regulares_l5',\n",
        "        'Vagas_Regulares_l6',\n",
        "        'Vagas_Regulares_l9',\n",
        "    ]\n",
        "\n",
        "    # Filtrar apenas colunas existentes no DataFrame\n",
        "    existing_columns = [col for col in columns_to_describe if col in df_tratado.columns]\n",
        "\n",
        "    if existing_columns:\n",
        "        print(\"Estatísticas Descritivas para Colunas Selecionadas:\")\n",
        "        # Tente descrever colunas numéricas\n",
        "        try:\n",
        "            # Definir opção de formato 'float' para exibição\n",
        "            pd.options.display.float_format = '{:,.4f}'.format\n",
        "            display(df_tratado[existing_columns].describe())\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar estatísticas para colunas numéricas selecionadas: {e}\")\n",
        "\n",
        "        # Tente descrever colunas de objetos\n",
        "        try:\n",
        "            object_columns = df_tratado[existing_columns].select_dtypes(include='object').columns.tolist()\n",
        "            if object_columns:\n",
        "                 print(\"\\nEstatísticas Descritivas para Colunas de Objeto Selecionadas:\")\n",
        "                 display(df_tratado[object_columns].describe())\n",
        "        except Exception as e:\n",
        "             print(f\"Erro ao gerar estatísticas para colunas de objetos selecionadas: {e}\")\n",
        "\n",
        "        # Remove a opção de formato float após a exibição, se desejar, para não afetar outras exibições\n",
        "        pd.options.display.float_format = None\n",
        "\n",
        "    else:\n",
        "        print(\"Nenhuma das colunas selecionadas foi encontrada no DataFrame.\")\n",
        "\n",
        "elif df_tratado is not None and df_tratado.empty:\n",
        "    print(\"O DataFrame 'df_tratado' está vazio, nenhuma estatística para exibir.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado, execute os passos anteriores.\")"
      ],
      "metadata": {
        "id": "qWHcOrazU0ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Análise Estatística de _outliers_ em colunas selecionadas de `df_tratado`**\n",
        "\n",
        "def analyze_outliers_iqr(df: pd.DataFrame, columns: list):\n",
        "    \"\"\"\n",
        "    Calcula e exibe os limites superior e inferior para identificação de outliers\n",
        "    usando o método IQR para as colunas numéricas especificadas.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): O DataFrame a ser analisado.\n",
        "        columns (list): Uma lista de nomes de colunas numéricas para análise.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(\"DataFrame vazio ou não encontrado. Nenhuma análise de outlier realizada.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Análise de Outliers (Método IQR) ---\")\n",
        "\n",
        "    for col in columns:\n",
        "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # Remove valores nulos para o cálculo do IQR\n",
        "            data = df[col].dropna()\n",
        "\n",
        "            if len(data) < 4: # Precisa de pelo menos 4 pontos para calcular Q1, Q3 e IQR\n",
        "                print(f\"  - Coluna '{col}': Não há dados suficientes para análise de outlier.\")\n",
        "                continue\n",
        "\n",
        "            Q1 = data.quantile(0.25)\n",
        "            Q3 = data.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            print(f\"\\n--- Coluna: '{col}' ---\")\n",
        "            print(f\"  Q1 (25º Percentil): {Q1:,.4f}\")\n",
        "            print(f\"  Q3 (75º Percentil): {Q3:,.4f}\")\n",
        "            print(f\"  IQR (Intervalo Interquartil): {IQR:,.4f}\")\n",
        "            print(f\"  Limite Inferior (Q1 - 1.5*IQR): {lower_bound:,.4f}\")\n",
        "            print(f\"  Limite Superior (Q3 + 1.5*IQR): {upper_bound:,.4f}\")\n",
        "\n",
        "            # Opcional: Contar e exibir outliers\n",
        "            outliers_lower = data[data < lower_bound]\n",
        "            outliers_upper = data[data > upper_bound]\n",
        "            total_outliers = len(outliers_lower) + len(outliers_upper)\n",
        "\n",
        "            print(f\"  Número de potenciais outliers abaixo do limite inferior: {len(outliers_lower):,}\")\n",
        "            print(f\"  Número de potenciais outliers acima do limite superior: {len(outliers_upper):,}\")\n",
        "            print(f\"  Total de potenciais outliers: {total_outliers:,}\")\n",
        "\n",
        "        elif col in df.columns:\n",
        "             print(f\"  - Coluna '{col}': Não é um tipo de dado numérico. Pulando análise de outlier.\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "# Lista de colunas para analisar outliers\n",
        "columns_for_outlier_analysis = [\n",
        "    'Idade',\n",
        "    'Total_de_Inscritos',\n",
        "    'Vagas_Extraordinarias_AC',\n",
        "    'Vagas_Extraordinarias_l1',\n",
        "    'Vagas_Extraordinarias_l10',\n",
        "    'Vagas_Extraordinarias_l13',\n",
        "    'Vagas_Extraordinarias_l14',\n",
        "    'Vagas_Extraordinarias_l2',\n",
        "    'Vagas_Extraordinarias_l5',\n",
        "    'Vagas_Extraordinarias_l6',\n",
        "    'Vagas_Extraordinarias_l9',\n",
        "    'Vagas_Ofertadas',\n",
        "    'Vagas_Regulares_AC',\n",
        "    'Vagas_Regulares_l1',\n",
        "    'Vagas_Regulares_l10',\n",
        "    'Vagas_Regulares_l13',\n",
        "    'Vagas_Regulares_l14',\n",
        "    'Vagas_Regulares_l2',\n",
        "    'Vagas_Regulares_l5',\n",
        "    'Vagas_Regulares_l6',\n",
        "    'Vagas_Regulares_l9',\n",
        "]\n",
        "\n",
        "# Executa a análise\n",
        "analyze_outliers_iqr(df_tratado, columns_for_outlier_analysis)"
      ],
      "metadata": {
        "id": "03Z99XBZFpVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Exibição das informações sobre as colunas e tipos de dados de `df_tratado` finalizado para exportação**\n",
        "if df_tratado is not None:\n",
        "    print(\"Informações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "_mUaL9ExVh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 10: Exportação para o BigQuery**\n",
        "\n",
        "# --- Configurações de Destino do BigQuery ---\n",
        "import pandas_gbq\n",
        "\n",
        "project_id = \"pnp-data-extraction\" # Substitua pelo ID do seu projeto\n",
        "dataset_id = \"pnp_dados_IFRN\"      # Nome do conjunto de dados\n",
        "\n",
        "# O nome da tabela no BigQuery será o mesmo nome da tabela da PNP\n",
        "table_id = f\"df_{table_name}\"\n",
        "destination_table = f\"{dataset_id}.{table_id}\"\n",
        "\n",
        "# --- Execução da Exportação ---\n",
        "if df_tratado is not None and not df_tratado.empty:\n",
        "    print(f\"Iniciando a exportação de {len(df_tratado):,} linhas para o BigQuery...\")\n",
        "    print(f\"Destino: {project_id}.{destination_table}\")\n",
        "\n",
        "    # --- Renomear colunas para serem compatíveis com BigQuery ---\n",
        "    # Substitui espaços e '/' por '_'\n",
        "    df_tratado_bq = df_tratado.copy()\n",
        "    df_tratado_bq.columns = (df_tratado_bq.columns\n",
        "            .str.replace(' ', '_', regex=False)\n",
        "            .str.replace('/', '_', regex=False)\n",
        "            .str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "        )\n",
        "    print(\"Nomes das colunas padronizados para o BigQuery.\")\n",
        "\n",
        "    # --- Definição do Esquema da Tabela ---\n",
        "    # Defina o nome e o tipo de cada coluna que você quer controlar.\n",
        "    # Tipos comuns: 'STRING', 'INTEGER', 'FLOAT', 'NUMERIC', 'BOOLEAN', 'TIMESTAMP', 'DATE'.\n",
        "    table_schema = [\n",
        "        {\"name\": \"Co_Inst\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Instituicao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Cod_Unidade\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Campus_do_IFRN\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_da_Unidade_de_Ensino_-_SISTEC\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_do_Ciclo_Matricula\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_da_Matricula\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Municipio\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_do_Municipio_com_DV\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"UF\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Regiao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Modalidade_Educacional\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Nivel_Educacional\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Modalidade_de_Curso\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Eixo_Tecnologico\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Subeixo_Tecnologico\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Nome_de_Curso\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Cursos\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Turno\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Carga_Horaria\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Carga_Horaria_Minima\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Fonte_de_Financiamento\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Fator_Esforco_Curso\", \"type\": \"FLOAT\"},\n",
        "        {\"name\": \"Sexo\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Cor_Raça\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Idade\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Faixa_Etaria\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Renda_Familiar\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Ano\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Matricula_Atendida\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Situacao_de_Matricula\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Categoria_da_Situacao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Data_de_Inicio_do_Ciclo\", \"type\": \"DATETIME\"},\n",
        "        {\"name\": \"Data_de_Fim_Previsto_do_Ciclo\", \"type\": \"DATETIME\"},\n",
        "        {\"name\": \"Data_de_Ocorrencia_da_Matricula\", \"type\": \"DATETIME\"},\n",
        "        {\"name\": \"Mes_De_Ocorrencia_da_Situacao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Total_de_Inscritos\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Forma_de_Ingresso\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Vagas_Ofertadas\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_AC\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l1\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l2\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l5\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l6\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l9\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l10\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l13\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l14\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_AC\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l1\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l2\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l5\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l6\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l9\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l10\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l13\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_categorias_soma\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Ofertadas_calc\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l14\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Inscritos\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Ano_de_início_do_ciclo\", \"type\": \"INTEGER\"}\n",
        "    ]\n",
        "    # -----------------------------------------------------------------\n",
        "\n",
        "    # Envia o DataFrame para o BigQuery com o esquema especificado\n",
        "    try:\n",
        "        # Usando a função recomendada pandas_gbq.to_gbq\n",
        "        pandas_gbq.to_gbq(\n",
        "            df_tratado_bq, # Exporta o DataFrame com colunas renomeadas\n",
        "            destination_table=destination_table,\n",
        "            project_id=project_id,\n",
        "            if_exists='replace',  # Opções: 'fail', 'replace', 'append'\n",
        "            progress_bar=True\n",
        "        )\n",
        "        print(f\"\\n✔ DataFrame exportado com sucesso para o BigQuery!\")\n",
        "        print(f\"Link para a tabela: https://console.cloud.google.com/bigquery?project={project_id}&ws=!1m5!1m4!4m3!1s{project_id}!2s{dataset_id}!3s{table_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERRO durante a exportação para o BigQuery: {e}\")\n",
        "\n",
        "elif df_tratado is not None and df_tratado.empty:\n",
        "    print(\"AVISO: O DataFrame final está vazio. Nenhuma exportação foi realizada.\")\n",
        "else:\n",
        "    print(\"❌ ERRO: O DataFrame a ser exportado não foi encontrado. A exportação foi cancelada.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "H7wYuE0Dqco4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}