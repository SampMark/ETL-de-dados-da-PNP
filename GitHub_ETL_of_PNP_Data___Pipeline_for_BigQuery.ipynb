{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SampMark/ETL-de-dados-da-PNP/blob/main/GitHub_ETL_of_PNP_Data___Pipeline_for_BigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extração da Plataforma Nilo Peçanha (PNP), filtragem, tratamento e armazenamento em Big Query dos dados de Instituições Federais de Ensino (IFES)**\n",
        "\n",
        "## **Fluxo de Execução**\n",
        "1. **Etapa 1: Instalação e Autenticação**: Instalação de bibliotecas e conecção ao Google Drive.\n",
        "2. **Etapa 2: Configuração do Pipeline**: definição dos parâmetros principais, como os tipos de dado da tabela, o período da extração (anos) e a(as) instituição(ões) a ser(em) filtrada(s).\n",
        "3. **Etapa 3: Download dos dados compactados da PNP**: nessa etapa é efetuado o download os arquivos compactados (**`.gz`**) da PNP para o Google Drive com base nas configurações da etapa anterior.\n",
        "4. **Etapa 4: Análise dos cabeçalhos dos CSVs ao longo dos anos**: nesta etapa crucial o script descompacta os arquivos, lê o cabeçalho de cada um e os exibe em uma tabela comparativa. Por fim, sugere uma lista de colunas que são comuns a todos os arquivos **`.csv`**, para decisão sobre quais variáveis manter.\n",
        "5. **Etapa 5: Processamento da extração do Big Data**: após a definição da lista final de colunas a serem mantida é executado o processamento, que resulta na unificação e filtragem dos dados num _DataFrame_ Pandas.\n",
        "6. **Etapa 6: Limpeza e tratamento de dados Específicos**: em seguida, são aplicadas um conjunto de funções para limpeza e padronizações dos dados.\n",
        "7. **Etapa 7: Exportação para BigQuery**: por fim, o _DataFrame_ tratado é salvo numa tabela do banco de dados do **Google BigQuery**.\n",
        "<img src=\"https://www2.ifal.edu.br/noticias/ifal-se-destaca-na-eficiencia-academica-dos-institutos-federais-do-nordeste/plataforma-nilo-pecanha/@@images/98c1a2a4-6c59-436f-bdce-effa7ae4d539.jpeg\" alt=\"Logo da Plataforma Nilo Peçanha\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "sq1VuFUOqieo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Extração de microdados da PNP para Big Query\n",
        "\n",
        "Este notebook automatiza o fluxo de trabalho com os microdados extraídos da Plataforma Nilo Peçanha (PNP),\n",
        "permitindo a extração de diferentes tabelas e a análise de seus cabeçalhos em cada anos.\n",
        "\"\"\"\n",
        "\n",
        "# @title **ETAPA 1: Instalação de dependências, importações e autenticação do usuário no Google Drive**\n",
        "\n",
        "# Instalação de Dependências\n",
        "!pip install gspread gspread-dataframe oauth2client pandas-gbq --quiet\n",
        "print(\"Dependências instaladas com sucesso!\")"
      ],
      "metadata": {
        "id": "nYWoq3EYq7FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação de bibliotecas\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import gspread\n",
        "import gzip\n",
        "import shutil\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import auth, drive\n",
        "from google.auth import default\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Set"
      ],
      "metadata": {
        "id": "QmUMUGuKq9yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autenticação e Montagem do Google Drive\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"\\nAutenticação e montagem do Google Drive realizadas com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante a autenticação ou montagem do Drive: {e}\")"
      ],
      "metadata": {
        "id": "FJq__1gYq_UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 2: Definição das Funções Principais**\n",
        "\n",
        "def download_pnp_data(table_name: str, start_year: int, end_year: int, force_update: bool):\n",
        "    \"\"\"\n",
        "    Baixa os arquivos da PNP para uma pasta específica no Google Drive.\n",
        "    O nome do arquivo e a pasta de destino são baseados no table_name.\n",
        "    \"\"\"\n",
        "    drive_folder = Path(f'/content/drive/MyDrive/Coisas do IFRN/Prodes/Indicadores/PNP/{table_name.capitalize()}')\n",
        "    drive_folder.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Verificando arquivos na pasta do Google Drive: {drive_folder}\")\n",
        "\n",
        "    base_url = \"https://d236w85zd3t8iw.cloudfront.net/pnp-tests/microdados\"\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        file_name = f\"microdados_{table_name}_{year}.csv.gz\"\n",
        "        url = f\"{base_url}/{year}/{file_name}\"\n",
        "        destination = drive_folder / file_name\n",
        "\n",
        "        if not force_update and destination.exists():\n",
        "            print(f\"✔ O arquivo para {year} ('{file_name}') já existe. Usando o cache do Drive.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f\"⬇ Baixando dados para {year} de {url}...\")\n",
        "            with requests.get(url, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(destination, 'wb') as f:\n",
        "                    shutil.copyfileobj(r.raw, f)\n",
        "            print(f\"✔ Download de {year} concluído com sucesso.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Falha ao baixar o arquivo para {year}. Erro: {e}. O arquivo pode não existir para este ano.\")\n",
        "\n",
        "def decompress_gz_to_csv(gz_path: Path, out_dir: Path) -> Path:\n",
        "    \"\"\"Descompacta cada arquivo .gz para a pasta de trabalho\"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # Remove a extensão .gz para obter o nome do arquivo CSV\n",
        "    csv_out_path = out_dir / gz_path.with_suffix(\"\").name\n",
        "    print(f\"Descompactando: {gz_path.name} -> {csv_out_path.name}\")\n",
        "    with gzip.open(gz_path, \"rb\") as f_in, open(csv_out_path, \"wb\") as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "    return csv_out_path\n",
        "\n",
        "def get_header(path: Path, sep: str = ';') -> List[str]:\n",
        "    \"\"\"Lê o cabeçalho de cada arquivo CSV descompactado\"\"\"\n",
        "    try:\n",
        "        return list(pd.read_csv(path, nrows=0, sep=sep, engine='python').columns)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler o cabeçalho de {path.name}: {e}\")\n",
        "        return []\n",
        "\n",
        "def analyze_and_compare_headers(csv_paths: List[Path]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria um DataFrame comparativo de cabeçalhos e sugere colunas comuns.\n",
        "    \"\"\"\n",
        "    if not csv_paths:\n",
        "        print(\"Nenhum arquivo CSV para analisar.\")\n",
        "        return pd.DataFrame(), []\n",
        "\n",
        "    headers_dict = {path.name: get_header(path) for path in csv_paths}\n",
        "\n",
        "    # Identificar colunas comuns\n",
        "    sets_of_headers = [set(h) for h in headers_dict.values() if h]\n",
        "    if not sets_of_headers:\n",
        "        print(\"Não foi possível ler nenhum cabeçalho.\")\n",
        "        return pd.DataFrame(), []\n",
        "\n",
        "    common_columns = sorted(list(sets_of_headers[0].intersection(*sets_of_headers[1:])))\n",
        "\n",
        "    # Criar DataFrame para comparação visual\n",
        "    all_columns = sorted(list(set.union(*sets_of_headers)))\n",
        "    comparison_data = {}\n",
        "    for col in all_columns:\n",
        "        comparison_data[col] = [(\"✔\" if col in headers_dict.get(fname, []) else \"❌\") for fname in headers_dict.keys()]\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data, index=headers_dict.keys()).transpose()\n",
        "\n",
        "    print(\"\\n--- Análise de Cabeçalhos Concluída ---\")\n",
        "    print(\"A tabela abaixo mostra quais colunas estão presentes (✔) ou ausentes (❌) em cada arquivo.\")\n",
        "    display(HTML(comparison_df.to_html()))\n",
        "\n",
        "    print(\"\\n--- Sugestão de Colunas Comuns ---\")\n",
        "    print(f\"Foram encontradas {len(common_columns)} colunas presentes em TODOS os arquivos do período:\")\n",
        "    # Imprime a lista formatada para ser copiada e colada\n",
        "    print(\"\\nmanter_colunas = [\")\n",
        "    for col in common_columns:\n",
        "        print(f\"    '{col}',\")\n",
        "    print(\"]\")\n",
        "\n",
        "    return comparison_df, common_columns\n",
        "\n",
        "def process_to_dataframe(csv_paths: List[Path], columns_to_keep: List[str], institutions: List[str], chunksize: int = 100000, sep: str = ';'):\n",
        "    \"\"\"\n",
        "    Unifica, filtra e concatena os CSVs em um único DataFrame,\n",
        "    mantendo apenas a lista de colunas fornecida.\n",
        "    \"\"\"\n",
        "    if not csv_paths:\n",
        "        raise RuntimeError(\"Nenhum arquivo CSV para processar.\")\n",
        "    if not columns_to_keep:\n",
        "        raise ValueError(\"A lista 'columns_to_keep' não pode estar vazia.\")\n",
        "\n",
        "    print(f\"\\nProcessamento iniciado. Serão importadas {len(columns_to_keep)} colunas pré-definidas.\")\n",
        "\n",
        "    # Encontrar coluna da instituição (considerando inconsistências de codificação)\n",
        "    col_inst = None\n",
        "    if 'Instituição' in columns_to_keep:\n",
        "        col_inst = 'Instituição'\n",
        "    elif 'InstituiÃ§Ã£o' in columns_to_keep:\n",
        "        col_inst = 'InstituiÃ§Ã£o'\n",
        "\n",
        "    if institutions and col_inst:\n",
        "        print(f\"Filtrando pela coluna '{col_inst}' com os valores: {institutions}\")\n",
        "    elif institutions:\n",
        "        print(\"AVISO: Filtro de instituição solicitado, mas a coluna 'Instituição' não está na lista de colunas a serem mantidas.\")\n",
        "\n",
        "    institution_map = {'Instituto Federal do Rio Grande do Norte': 'IFRN'}\n",
        "    df_list = []\n",
        "\n",
        "    for csv_path in csv_paths:\n",
        "        print(f\"Processando e filtrando: {csv_path.name}\")\n",
        "        try:\n",
        "            # Lê o cabeçalho do arquivo para saber quais colunas ele realmente tem\n",
        "            actual_header = get_header(csv_path, sep)\n",
        "            # Usa apenas as colunas da nossa lista que existem neste arquivo\n",
        "            cols_to_read = [col for col in columns_to_keep if col in actual_header]\n",
        "\n",
        "            for chunk in pd.read_csv(csv_path, usecols=cols_to_read, chunksize=chunksize, sep=sep, engine='python', on_bad_lines='warn'):\n",
        "                if col_inst and col_inst in chunk.columns:\n",
        "                    chunk[col_inst] = chunk[col_inst].replace(institution_map)\n",
        "                    if institutions:\n",
        "                        chunk = chunk[chunk[col_inst].isin(institutions)]\n",
        "\n",
        "                if not chunk.empty:\n",
        "                    df_list.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"  ERRO ao processar o arquivo {csv_path.name}: {e}. Pulando este arquivo.\")\n",
        "            continue\n",
        "\n",
        "    if not df_list:\n",
        "        print(\"AVISO: Nenhum dado encontrado para as instituições selecionadas ou os arquivos estavam vazios.\")\n",
        "        return pd.DataFrame(columns=columns_to_keep)\n",
        "\n",
        "    final_df = pd.concat(df_list, ignore_index=True)\n",
        "    # Garante que o DataFrame final tenha todas as colunas da lista, preenchendo com NaN as que não existiam\n",
        "    final_df = final_df.reindex(columns=columns_to_keep)\n",
        "\n",
        "    print(f\"\\nProcesso concluído. DataFrame final criado com {len(final_df):,} linhas e {len(final_df.columns)} colunas.\")\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "jM4FIbiTrDQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **ETAPA 3: Configuração do Processo e Download (Opcional)**\n",
        "\n",
        "# --- Interface Interativa de Configuração ---\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "# Seleção da tabela\n",
        "table_name_dropdown = widgets.Dropdown(\n",
        "    options=['matriculas', 'eficiencia_academica', 'financeiro', 'servidores'],\n",
        "    value='matriculas',\n",
        "    description='Tabela de Dados:',\n",
        "    style=style\n",
        ")\n",
        "\n",
        "# Período de anos\n",
        "start_year_slider = widgets.IntSlider(value=2017, min=2017, max=2024, step=1, description='Ano Inicial:', style=style)\n",
        "end_year_slider = widgets.IntSlider(value=2024, min=2017, max=2024, step=1, description='Ano Final:', style=style)\n",
        "\n",
        "# Opção de forçar atualização\n",
        "force_update_checkbox = widgets.Checkbox(value=False, description='Forçar atualização (baixar novamente os arquivos existentes)', style=style)\n",
        "\n",
        "# --- Filtro de instituições por código ---\n",
        "# O campo pede o código numérico da instituição.\n",
        "institution_code_text = widgets.Text(\n",
        "    value='26435', # O valor padrão está preenchido com o código do IFRN = '26435'\n",
        "    description='Códigos das Instituições (Co Inst):',\n",
        "    style=style,\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "print(\"--- Configure os Parâmetros do Pipeline ---\")\n",
        "display(table_name_dropdown)\n",
        "display(widgets.HBox([start_year_slider, end_year_slider]))\n",
        "display(force_update_checkbox)\n",
        "display(institution_code_text)"
      ],
      "metadata": {
        "id": "tJmLJZs1ec2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 3: Configuração do Processo e Download**\n",
        "\n",
        "# --- Interface Interativa de Configuração ---\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "# Seleção da tabela\n",
        "table_name_dropdown = widgets.Dropdown(\n",
        "    options=['matriculas', 'eficiencia_academica', 'financeiro', 'servidores'],\n",
        "    value='matriculas',\n",
        "    description='Tabela de Dados:',\n",
        "    style=style\n",
        ")\n",
        "\n",
        "# Período de anos\n",
        "start_year_slider = widgets.IntSlider(value=2017, min=2017, max=2024, step=1, description='Ano Inicial:', style=style)\n",
        "end_year_slider = widgets.IntSlider(value=2024, min=2017, max=2024, step=1, description='Ano Final:', style=style)\n",
        "\n",
        "# Opção de forçar atualização\n",
        "force_update_checkbox = widgets.Checkbox(value=False, description='Forçar atualização (baixar novamente os arquivos existentes)', style=style)\n",
        "\n",
        "# --- Filtro de instituições por nome ---\n",
        "# O campo agora aceita um ou mais nomes de instituições, separados por vírgula.\n",
        "# O valor padrão já inclui as duas variações para o IFRN.\n",
        "institution_name_text = widgets.Text(\n",
        "    value='IFRN, Instituto Federal do Rio Grande do Norte',\n",
        "    description='Nome da Instituição (use vírgula para múltiplos nomes):',\n",
        "    style=style,\n",
        "    layout=widgets.Layout(width='70%') # Largura aumentada para melhor visualização\n",
        ")\n",
        "\n",
        "print(\"--- Configure os Parâmetros do Pipeline ---\")\n",
        "display(table_name_dropdown)\n",
        "display(widgets.HBox([start_year_slider, end_year_slider]))\n",
        "display(force_update_checkbox)\n",
        "display(institution_name_text)"
      ],
      "metadata": {
        "id": "2XCFScGZsmTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 4: Download dos arquivos `.gz` e análise dos cabeçalhos dos `.csv` extraídos**\n",
        "\n",
        "# 1. Pega os valores dos widgets da Etapa 3\n",
        "table_name = table_name_dropdown.value\n",
        "start_year = start_year_slider.value\n",
        "end_year = end_year_slider.value\n",
        "force_update = force_update_checkbox.value\n",
        "\n",
        "# Aplica o nome correto do widget e analisa as strings separadas por vírgulas\n",
        "institutions_str = institution_name_text.value\n",
        "institutions_list = [inst.strip() for inst in institutions_str.split(',') if inst.strip()]\n",
        "\n",
        "# 2. Executa o download\n",
        "download_pnp_data(table_name, start_year, end_year, force_update)\n",
        "\n",
        "# 3. Prepara os arquivos para a análise\n",
        "drive_folder = Path(f'/content/drive/MyDrive/Coisas do IFRN/Prodes/Indicadores/PNP/{table_name.capitalize()}')\n",
        "work_dir = Path.cwd() / \"extracted_csvs\"\n",
        "if work_dir.exists(): shutil.rmtree(work_dir)\n",
        "work_dir.mkdir()\n",
        "\n",
        "input_files_gz = [drive_folder / f\"microdados_{table_name}_{year}.csv.gz\" for year in range(start_year, end_year + 1)]\n",
        "input_files_gz_existing = [f for f in input_files_gz if f.exists()]\n",
        "\n",
        "all_csvs = []\n",
        "if input_files_gz_existing:\n",
        "    for gz_file in input_files_gz_existing:\n",
        "        all_csvs.append(decompress_gz_to_csv(gz_file, work_dir))\n",
        "else:\n",
        "    print(\"Nenhum arquivo .gz encontrado no Drive para o período e tabela selecionados.\")\n",
        "\n",
        "# 4. Analisa e compara os cabeçalhos\n",
        "if all_csvs:\n",
        "    comparison_df, common_columns = analyze_and_compare_headers(all_csvs)\n",
        "else:\n",
        "    print(\"Nenhuma análise de cabeçalho pôde ser feita, pois nenhum arquivo CSV foi descompactado.\")\n",
        "\n",
        "print(\"\\n\\n>>> AÇÃO NECESSÁRIA <<<\")\n",
        "print(\"Copie a lista de colunas comuns sugerida acima (ou edite-a conforme sua necessidade) e cole na célula da 'ETAPA 5' antes de executá-la.\")"
      ],
      "metadata": {
        "id": "6k14QyRSrMfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 5: Definição das colunas necessárias para extração do `df_filtrado`**\n",
        "\n",
        "# >>> LISTA DE COLUNAS PARA EXTRAÇÃO (COLE AQUI) <<<\n",
        "# Exemplo baseado na sugestão da etapa anterior.\n",
        "# Edite esta lista conforme a necessidade.\n",
        "# Lista de colunas a serem mantidas no DataFrame final, organizadas por categoria e finalidade.\n",
        "\n",
        "manter_colunas = [\n",
        "\n",
        "    # --- IDENTIFICADORES E CÓDIGOS ---\n",
        "    # Códigos únicos utilizados para referenciar instituições, unidades, matrículas e ciclos.\n",
        "    'Co Inst', 'Instituição', 'Cod Unidade', 'Unidade de Ensino', 'Código da Unidade de Ensino - SISTEC',\n",
        "    'Código do Ciclo Matricula', 'Código da Matricula',\n",
        "\n",
        "    # --- DADOS DE LOCALIZAÇÃO ---\n",
        "    # Variáveis de localização geográfica.\n",
        "    'Município', 'Código do Município com DV', 'UF', 'Região',\n",
        "\n",
        "    # --- DADOS DO CURSO ---\n",
        "    # Variáveis que descrevem as características do curso ofertado.\n",
        "    'Modalidade de Ensino', 'Tipo de Curso', 'Tipo de Oferta',  'Eixo Tecnológico', 'Subeixo Tecnológico',\n",
        "    'Nome de Curso', 'Turno', 'Carga Horaria', 'Carga Horaria Mínima', 'Fonte de Financiamento',\n",
        "    'Fator Esforço Curso', # Fator de ponderação para cálculo de Matrícula Equivalente\n",
        "\n",
        "    # --- DADOS DO ALUNO (DEMOGRÁFICOS E SOCIOECONÔMICOS) ---\n",
        "    # Variáveis com informações pessoais e socioeconômicas do estudante.\n",
        "    'Sexo', 'Cor / Raça', 'Idade', 'Faixa Etária', 'Renda Familiar',\n",
        "\n",
        "    # --- DADOS DA MATRÍCULA ---\n",
        "    # Variáveis que detalham o status e o período do vínculo do aluno com o curso.\n",
        "    'Ano', 'Matrícula Atendida', 'Situação de Matrícula', 'Categoria da Situação',\n",
        "    'Data de Inicio do Ciclo', 'Data de Fim Previsto do Ciclo',\n",
        "    'Data de Ocorrencia da Matricula', 'Mês De Ocorrência da Situação',\n",
        "\n",
        "    # --- DADOS DE VAGAS E INGRESSO ---\n",
        "    # Variáveis relacionadas ao processo seletivo, número de vagas e forma de entrada.\n",
        "    'Total de Inscritos', 'Forma de ingresso', 'Vagas Ofertadas', 'Vagas Regulares AC',\n",
        "    'Vagas Regulares l1', 'Vagas Regulares l2', 'Vagas Regulares l5', 'Vagas Regulares l6',\n",
        "    'Vagas Regulares l9', 'Vagas Regulares l10', 'Vagas Regulares l13', 'Vagas Regulares l14',\n",
        "    'Vagas Extraordinárias AC', 'Vagas Extraordinárias l1', 'Vagas Extraordinárias l2',\n",
        "    'Vagas Extraordinárias l5', 'Vagas Extraordinárias l6', 'Vagas Extraordinárias l9',\n",
        "    'Vagas Extraordinárias l10', 'Vagas Extraordinárias l13', 'Vagas Extraordinárias l14'\n",
        "]\n",
        "\n",
        "# --- Execução do Processamento ---\n",
        "df_filtrado = None # Inicializa a variável\n",
        "if not manter_colunas:\n",
        "    print(\"❌ ERRO: A lista 'manter_colunas' está vazia. Preencha-a com as colunas desejadas e execute novamente.\")\n",
        "elif not all_csvs:\n",
        "    print(\"❌ ERRO: Nenhum arquivo CSV foi encontrado para processar. Execute a Etapa 4 primeiro.\")\n",
        "else:\n",
        "    # Cria o DataFrame filtrado com base na seleção de colunas\n",
        "    df_filtrado = process_to_dataframe(\n",
        "        csv_paths=all_csvs,\n",
        "        columns_to_keep=manter_colunas,\n",
        "        institutions=institutions_list\n",
        "    )\n",
        "    display(df_filtrado.head())\n",
        "\n",
        "# --- Análise do DataFrame Gerado ---\n",
        "if df_filtrado is not None:\n",
        "    print(\"\\n--- Análise Detalhada do DataFrame Final ---\")\n",
        "    if df_filtrado.empty:\n",
        "        print(\"O DataFrame foi criado, mas está vazio (não contém linhas).\")\n",
        "    else:\n",
        "        num_rows, num_cols = df_filtrado.shape\n",
        "        print(f\"Dimensões: {num_rows:,} linhas e {num_cols} colunas.\")\n",
        "        print(\"\\nEstrutura e Tipos de Dados:\")\n",
        "        df_filtrado.info()\n",
        "else:\n",
        "    print(\"\\nO DataFrame 'df_filtrado' não foi criado. Verifique os erros nas etapas anteriores.\")"
      ],
      "metadata": {
        "id": "K6ZtJpH0rPJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Listagem e análise de valores únicos para as colunas indicadas em `df_filtrado`**\n",
        "if df_filtrado is not None:\n",
        "    columns_to_list = ['Ano', 'Unidade de Ensino', 'Eixo Tecnológico', 'Subeixo Tecnológico', 'Matrícula Atendida',\n",
        "                       'Forma de ingresso', 'Faixa Etária', 'Fonte de Financiamento',\n",
        "                       'Tipo de Oferta', 'Tipo de Curso', 'Situação de Matrícula', 'Categoria da Situação',\n",
        "                       ]\n",
        "\n",
        "    for col in columns_to_list:\n",
        "        if col in df_filtrado.columns:\n",
        "            # Converte a coluna para o tipo 'string' antes de classificar valores exclusivos\n",
        "            unique_values = sorted(df_filtrado[col].astype(str).unique())\n",
        "            print(f\"Valores únicos em ordem alfabética na coluna '{col}':\")\n",
        "            for value in unique_values:\n",
        "                print(f\"- {value}\")\n",
        "            print(\"-\" * 20) # Separador\n",
        "        else:\n",
        "            print(f\"A coluna '{col}' não foi encontrada no DataFrame 'df_filtrado'.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "nns9fkpAmej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Visualização dos valores únicos da coluna 'Nome de Curso' em `df_filtrado`**\n",
        "\n",
        "if 'df_filtrado' in locals() and df_filtrado is not None and 'Nome de Curso' in df_filtrado.columns:\n",
        "    unique_nomes_curso = sorted(df_filtrado['Nome de Curso'].unique())\n",
        "    print(\"Valores únicos na coluna 'Nome de Curso' (df_filtrado):\")\n",
        "    print(f\"Total de valores únicos: {len(unique_nomes_curso)}\")\n",
        "    for nome in unique_nomes_curso:\n",
        "        print(f\"- {nome}\")\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Nome de Curso' não encontrada no DataFrame 'df_filtrado'.\")"
      ],
      "metadata": {
        "id": "JQANcTG-l40Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Função para aplicar os prefixos ---\n",
        "def renomear_cursos(row):\n",
        "    \"\"\"\n",
        "    Verifica o 'Tipo de Curso' em uma linha do DataFrame e adiciona o\n",
        "    prefixo correspondente ao 'Nome de Curso'.\n",
        "    \"\"\"\n",
        "    tipo_curso = row['Tipo de Curso']\n",
        "    nome_curso = row['Nome de Curso']\n",
        "\n",
        "    if tipo_curso == 'Licenciatura':\n",
        "        return f'Licenciatura em {nome_curso}'\n",
        "    elif tipo_curso == 'Qualificação Profissional (FIC)':\n",
        "        # Adiciona o prefixo 'FIC em' em vez de 'Qualificação Profissional (FIC) em'\n",
        "        return f'FIC {nome_curso}'\n",
        "    elif tipo_curso == 'Tecnologia':\n",
        "        return f'Tecnologia em {nome_curso}'\n",
        "    else:\n",
        "        # Se não for nenhum dos tipos especificados, retorna o nome original\n",
        "        return nome_curso\n",
        "\n",
        "# --- Criação da nova coluna 'Cursos' ---\n",
        "# A função 'renomear_cursos' é aplicada a cada linha (axis=1) do DataFrame.\n",
        "# O resultado é armazenado na nova coluna 'Cursos'.\n",
        "df_filtrado['Cursos'] = df_filtrado.apply(renomear_cursos, axis=1)\n",
        "\n",
        "# --- Exibição do DataFrame final com a nova coluna ---\n",
        "print(\"--- DataFrame com a Nova Coluna 'Cursos' ---\")\n",
        "display(df_filtrado)"
      ],
      "metadata": {
        "id": "lddPlmDplP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir valores únicos da coluna 'Cursos' em df_filtrado\n",
        "if 'df_filtrado' in locals() and df_filtrado is not None and 'Cursos' in df_filtrado.columns:\n",
        "    unique_cursos = sorted(df_filtrado['Cursos'].unique())\n",
        "    print(\"Valores únicos na coluna 'Cursos' (df_filtrado):\")\n",
        "    print(f\"Total de valores únicos: {len(unique_cursos)}\")\n",
        "    for nome in unique_cursos:\n",
        "        print(f\"- {nome}\")\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Cursos' não encontrada no DataFrame 'df_filtrado'.\")"
      ],
      "metadata": {
        "id": "dXbDqNUfwjhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular e exibir a diferença em valores únicos dos cursos antes e depois do tratamento\n",
        "if ('df_filtrado' in locals() and df_filtrado is not None and\n",
        "    'Nome de Curso' in df_filtrado.columns and 'Cursos' in df_filtrado.columns):\n",
        "\n",
        "    unique_nomes_curso = df_filtrado['Nome de Curso'].unique()\n",
        "    unique_cursos = df_filtrado['Cursos'].unique()\n",
        "\n",
        "    num_unique_nomes_curso = len(unique_nomes_curso)\n",
        "    num_unique_cursos = len(unique_cursos)\n",
        "    difference = num_unique_nomes_curso - num_unique_cursos\n",
        "\n",
        "    print(f\"Total de valores únicos na coluna 'Nome de Curso' (df_filtrado): {num_unique_nomes_curso}\")\n",
        "    print(f\"Total de valores únicos na coluna 'Cursos' (df_filtrado): {num_unique_cursos}\")\n",
        "    print(f\"Diferença no número de valores únicos: {difference}\")\n",
        "\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None:\n",
        "    print(\"DataFrame 'df_filtrado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"As colunas 'Nome de Curso' ou 'Cursos' não foram encontradas no DataFrame 'df_filtrado'.\")"
      ],
      "metadata": {
        "id": "5QolQ07hC6ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 6: Script para tratamento detalhado dos parâmetros da coluna 'Cursos'**\n",
        "\n",
        "def padronizar_nome_curso(df, nome_coluna='Cursos'):\n",
        "    \"\"\"\n",
        "    Aplica uma série de regras de padronização a uma coluna de nomes de cursos em um DataFrame do pandas.\n",
        "\n",
        "    As etapas de limpeza incluem:\n",
        "    1. Remoção de informações redundantes (prefixos, sufixos de modalidade, etc.).\n",
        "    2. Normalização de variações complexas e padronização de nomes específicos.\n",
        "    3. Correção de erros de digitação e variações simples.\n",
        "    4. Padronização da capitalização para Title Case, preservando conectivos em minúsculo.\n",
        "    5. Remoção de espaços em branco extras.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): O DataFrame contendo os dados.\n",
        "        nome_coluna (str): O nome da coluna com os nomes dos cursos a serem padronizados.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: O DataFrame com a coluna de nomes de cursos padronizada.\n",
        "    \"\"\"\n",
        "    # Copia a coluna para evitar alterar o DataFrame original durante o processo\n",
        "    cursos_padronizados = df[nome_coluna].copy().astype(str)\n",
        "\n",
        "    # --- Etapa 6.1: Remover informações redundantes (prefixos e sufixos) ---\n",
        "    prefixos_para_remover = [\n",
        "        r'^fic\\s*-\\s*',\n",
        "        r'^pós graduação em\\s*',\n",
        "        r'^pós-graduação em\\s*',\n",
        "        r'^especialização \\(lato sensu\\)\\s*-\\s*',\n",
        "        r'^especialização\\s*-\\s*',\n",
        "        r'^mestrado\\s*-\\s*',\n",
        "        r'^doutorado\\s*-\\s*',\n",
        "        r'^mestrado profissional\\s*-\\s*',\n",
        "        r'^fic qualificação profissional\\s*-\\s*',\n",
        "        r'curso de Pós-graduação Lato Sensu '\n",
        "    ]\n",
        "    for prefixo in prefixos_para_remover:\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(prefixo, '', regex=True, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove sufixos de modalidade\n",
        "    sufixos_para_remover = [\n",
        "        r',\\s*na modalidade ead$',\n",
        "        r',\\s*na modalidade semipresencial$'\n",
        "    ]\n",
        "    for sufixo in sufixos_para_remover:\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(sufixo, '', regex=True, flags=re.IGNORECASE)\n",
        "\n",
        "    # --- Etapa 6.2: Normalização de variações complexas ---\n",
        "    # Este dicionário usa regex para encontrar padrões e substituir pelo nome padronizado.\n",
        "    # A ordem é importante: regras mais específicas devem vir antes.\n",
        "    regras_de_normalizacao = {\n",
        "        # --- Doutorados ---\n",
        "        # --- AJUSTE ---: Adicionada regra para Doutorado Acadêmico em Educação Profissional.\n",
        "        r'.*Doutorado Acad[êe]mico em Educaç[ãa]o Profissional.*': 'Doutorado em Educação Profissional e Tecnológica',\n",
        "        # --- AJUSTE ---: Adicionada regra para Doutorado em Desenvolvimento Educacional e Social.\n",
        "        # r'.*Doutorado.*Desenvolvimento Educacional e Social.*': 'Doutorado em Desenvolvimento Educacional e Social',\n",
        "        r'.*Doutorado em Ensino.*(Rede Nordeste|renoen).*': 'Doutorado em Ensino (RENOEN)',\n",
        "        r'Rede Nordeste de Ensino - RENOEN': 'Doutorado em Ensino (RENOEN)',\n",
        "\n",
        "        # --- Mestrados ---\n",
        "        r'.*Mestrado.*Educação Profissional e Tecnológica.*': 'Mestrado em Educação Profissional e Tecnológica (ProfEPT)',\n",
        "        r'Mestrado Acadêmico em Educação Profissional': 'Mestrado em Educação Profissional e Tecnológica (ProfEPT)',\n",
        "        r'Mestrado em Ensino': 'Mestrado em Ensino (Posensino)',\n",
        "        r'Mestrado Profissional - Recursos Naturais': 'Mestrado Profissional em Uso Sustentável dos Recursos Naturais',\n",
        "        r'Uso Sustentável dos Recursos Naturais': 'Mestrado Profissional em Uso Sustentável dos Recursos Naturais',\n",
        "\n",
        "        # --- Especializações ---\n",
        "        r'Especialização em Educação Ambiental e Geografia do Semi-Árido': 'Especialização em Educação Ambiental e Geografia do Semiárido',\n",
        "        r'Educação Ambiental e Geografia do Semiarido': 'Especialização em Educação Ambiental e Geografia do Semiárido',\n",
        "        r'.*Ci[êe]ncias Humanas e Saberes Contempor[âa]neos.*': 'Especialização em Ciências Humanas e Saberes Contemporâneos para a Educação',\n",
        "        r'Ciências Humanas e Competências Contemporâneas para a Educação': 'Especialização em Ciências Humanas e Competências Contemporâneas para a Educação',\n",
        "        r'Docência para a Educação Profissional e Tecnológica (DocentEPT)': 'Especialização em Docência na Educação Profissional e Tecnológica (DocentEPT)',\n",
        "        r'Especialização em Eja No Contexto da Diversidade': 'Especialização em Educação de Jovens e Adultos no Contexto da Diversidade',\n",
        "        r'Engenharia de Segurança do Trabalho': 'Especialização em Engenharia de Segurança do Trabalho',\n",
        "        r'Ensino de Ciências Biológicas': 'Especialização em Ensino de Ciências Biológicas',\n",
        "        r'Ensino de Ciências Naturais e Matemática': 'Especialização em Ensino de Ciências Naturais e Matemática',\n",
        "        r'Ensino de Geociências': 'Especialização em Ensino de Geociências',\n",
        "        r'Ensino de Teatro': 'Especialização em Ensino de Teatro',\n",
        "        r'Especialização em Ciência e Tecnologia de Alimentos, Na Modalidade Ead': 'Especialização em Ciência e Tecnologia de Alimentos',\n",
        "        r'Especialização em Ensino de Língua Portuguesa e Matemática Transdisciplinar': 'Especialização em Ensino da Língua Portuguesa e Matemática numa Abordagem Transdisciplinar',\n",
        "        r'Especialização em Docência para a Educação Profissional e Tecnológica – Docentept - Uab': 'Especialização em Docência na Educação Profissional e Tecnológica (DocentEPT)',\n",
        "        r'Estudos Linguísticos e Literários': 'Especialização em Estudos Linguísticos e Literários',\n",
        "        r'Especialização em Práticas Assertivas da Educação Profissional Integrada À Educação de Jovens e Adultos - Com Ênfase em Didática': 'Especialização em Práticas Assertivas em Didática e Gestão da Educação Profissional integrada à EJA',\n",
        "        r'^Gestão Ambiental$': 'Especialização em Gestão Ambiental',\n",
        "        r'Licenciatura em Licenciatura para a Educação Profissional, Científica e Tecnológica': 'Licenciatura em Formação Pedagógica para a Educação Básica, Profissional e Tecnológica'\n",
        "     }\n",
        "\n",
        "    for padrao, nome_correto in regras_de_normalizacao.items():\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(padrao, nome_correto, regex=True, flags=re.IGNORECASE)\n",
        "\n",
        "    # --- Etapa 6.3: Corrigir erros de digitação e variações simples ---\n",
        "    correcoes_simples = {\n",
        "        'Conteporaneidade': 'Contemporaneidade',\n",
        "        'À': 'à', # Corrige capitalização de crase\n",
        "        'Semi-Árido': 'Semiárido',\n",
        "        'Eja No Contexto da Diversidade': 'EJA no Contexto da Diversidade',\n",
        "        'para O Ensino Médio': 'para o Ensino Médio',\n",
        "        'desportiva e de Lazer': 'Desportiva e de Lazer'\n",
        "    }\n",
        "    for erro, correcao in correcoes_simples.items():\n",
        "        cursos_padronizados = cursos_padronizados.str.replace(erro, correcao, regex=False)\n",
        "\n",
        "    # --- Etapa 6.4: Padronização da capitalização ---\n",
        "    # Converte para Title Case\n",
        "    cursos_padronizados = cursos_padronizados.str.title()\n",
        "\n",
        "    # Lista de conectivos e palavras curtas que devem ser minúsculas\n",
        "    conectivos_minusculos = ['de', 'da', 'do', 'dos', 'das', 'em', 'e', 'para', 'o', 'a', 'à', 'os', 'as', 'no', 'na', 'nos', 'nas', 'com']\n",
        "\n",
        "    def apply_title_case_with_exceptions(name):\n",
        "        # Garante que siglas como IFRN, EJA, EAD, etc., fiquem em maiúsculas\n",
        "        name = re.sub(r'\\b(If(rn)?|Eja|Ead|Fic|Ppi|Pcd|Renoen|Profept|Docentept|Posensino)\\b', lambda m: m.group().upper(), name, flags=re.IGNORECASE)\n",
        "        words = name.split()\n",
        "        processed_words = [word.lower() if word.lower() in conectivos_minusculos and i > 0 else word for i, word in enumerate(words)]\n",
        "        return ' '.join(processed_words)\n",
        "\n",
        "    cursos_padronizados = cursos_padronizados.apply(apply_title_case_with_exceptions)\n",
        "\n",
        "    # --- Etapa 6.5: Remover espaços em branco extras ---\n",
        "    cursos_padronizados = cursos_padronizados.str.strip()\n",
        "    cursos_padronizados = cursos_padronizados.str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    # Atribui a coluna padronizada de volta ao DataFrame\n",
        "    df_tratado = df.copy()\n",
        "    df_tratado[nome_coluna] = cursos_padronizados\n",
        "\n",
        "    return df_tratado\n",
        "\n",
        "# Aplica a função de padronização à coluna 'Cursos'\n",
        "df_tratado = padronizar_nome_curso(df_filtrado.copy(), 'Cursos')\n",
        "\n",
        "if df_tratado is not None:\n",
        "    num_rows, num_cols = df_tratado.shape\n",
        "    print(f\"DataFrame 'df_tratado' criado como uma cópia de 'df_filtrado' tem {num_rows:,} linhas e {num_cols} colunas.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")\n",
        "\n",
        "# Exibir valores únicos em ordem alfabética na coluna 'Cursos'\n",
        "if df_tratado is not None and 'Cursos' in df_tratado.columns:\n",
        "    unique_cursos = sorted(df_tratado['Cursos'].unique())\n",
        "    print(\"Valores únicos na coluna 'Cursos':\")\n",
        "    print(f\"Total de valores únicos na coluna 'Cursos' após limpeza: {len(unique_cursos)}\")\n",
        "    for curso in unique_cursos:\n",
        "        print(f\"- {curso}\")\n",
        "elif df_tratado is None:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Cursos' não encontrada no DataFrame.\")\n"
      ],
      "metadata": {
        "id": "KRIFNHQ80BVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Calcular e exibir a diferença entre os valores únicos dos 'Cursos' antes e depois do tratamento**\n",
        "if ('df_filtrado' in locals() and df_filtrado is not None and 'Nome de Curso' in df_filtrado.columns and\n",
        "    'df_tratado' in locals() and df_tratado is not None and 'Cursos' in df_tratado.columns):\n",
        "\n",
        "    unique_nomes_curso_filtrado = df_filtrado['Nome de Curso'].unique()\n",
        "    unique_cursos_tratado = df_tratado['Cursos'].unique()\n",
        "\n",
        "    num_unique_nomes_curso_filtrado = len(unique_nomes_curso_filtrado)\n",
        "    num_unique_cursos_tratado = len(unique_cursos_tratado)\n",
        "    difference = num_unique_nomes_curso_filtrado - num_unique_cursos_tratado\n",
        "\n",
        "    print(f\"Total de valores únicos na coluna 'Nome de Curso' (df_filtrado): {num_unique_nomes_curso_filtrado}\")\n",
        "    print(f\"Total de valores únicos na coluna 'Cursos' (df_tratado): {num_unique_cursos_tratado}\")\n",
        "    print(f\"Diferença no número de valores únicos: {difference}\")\n",
        "\n",
        "elif 'df_filtrado' not in locals() or df_filtrado is None or 'Nome de Curso' not in df_filtrado.columns:\n",
        "    print(\"DataFrame 'df_filtrado' ou a coluna 'Nome de Curso' não encontrada. Execute as etapas anteriores.\")\n",
        "elif 'df_tratado' not in locals() or df_tratado is None or 'Cursos' not in df_tratado.columns:\n",
        "     print(\"DataFrame 'df_tratado' ou a coluna 'Cursos' não encontrada. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "cY4IteLXDnDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Mapear os valores em 'Matrícula Atendida' e converter para `booleano`**\n",
        "if 'df_tratado' in locals() and df_tratado is not None and 'Matrícula Atendida' in df_tratado.columns:\n",
        "    print(\"Convertendo a coluna 'Matrícula Atendida' para booleano...\")\n",
        "    mapping = {\"Sim\": 1, \"Y\": 1, \"Não Informado\": 0}\n",
        "    # Certifica-se de que a coluna é do tipo objeto para lidar com tipos mistos antes do mapeamento\n",
        "    df_tratado['Matrícula Atendida'] = df_tratado['Matrícula Atendida'].astype(str).map(mapping).astype(bool)\n",
        "    print(\"Conversão concluída.\")\n",
        "    print(\"\\nContagem dos valores na coluna 'Matrícula Atendida' após conversão:\")\n",
        "    display(df_tratado['Matrícula Atendida'].value_counts())\n",
        "elif 'df_tratado' not in locals() or df_tratado is None:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")\n",
        "else:\n",
        "    print(\"Coluna 'Matrícula Atendida' não encontrada no DataFrame 'df_tratado'.\")"
      ],
      "metadata": {
        "id": "SMJGe5UGP883"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Ajuste/correção de parâmetros em diversas colunas, renomeia colunas em em `df_tratado` para o BigQuery**\n",
        "\n",
        "# Renomeia colunas adequadamente para o BigQuery primeiro para evitar KeyErrors\n",
        "df_tratado = df_tratado.rename(columns={\n",
        "    'Carga Horaria': 'Carga_Horaria',\n",
        "    'Carga Horaria Mínima': 'Carga_Horaria_Minima',\n",
        "    'Categoria da Situação': 'Categoria_da_Situacao',\n",
        "    'Co Inst': 'Co_Inst',\n",
        "    'Cod Unidade': 'Cod_Unidade',\n",
        "    'Cor / Raça': 'Cor_Raça',\n",
        "    'Código da Matricula': 'Codigo_da_Matricula',\n",
        "    'Código da Unidade de Ensino - SISTEC': 'Codigo_da_Unidade_de_Ensino_-_SISTEC',\n",
        "    'Código do Ciclo Matricula': 'Codigo_do_Ciclo_Matricula',\n",
        "    'Código do Município com DV': 'Codigo_do_Municipio_com_DV',\n",
        "    'Data de Fim Previsto do Ciclo': 'Data_de_Fim_Previsto_do_Ciclo',\n",
        "    'Data de Inicio do Ciclo': 'Data_de_Inicio_do_Ciclo',\n",
        "    'Data de Ocorrencia da Matricula': 'Data_de_Ocorrencia_da_Matricula',\n",
        "    'Situação de Matrícula': 'Situacao_de_Matricula',\n",
        "    'Subeixo Tecnológico': 'Subeixo_Tecnologico',\n",
        "    'Matrícula Atendida': 'Matricula_Atendida',\n",
        "    'Eixo Tecnológico': 'Eixo_Tecnologico',\n",
        "    'Faixa Etária': 'Faixa_Etaria',\n",
        "    'Fator Esforço Curso': 'Fator_Esforco_Curso',\n",
        "    'Fonte de Financiamento': 'Fonte_de_Financiamento',\n",
        "    'Forma de ingresso': 'Forma_de_Ingresso',\n",
        "    'Instituição': 'Instituicao',\n",
        "    'Mês De Ocorrência da Situação': 'Mes_De_Ocorrencia_da_Situacao',\n",
        "    'Município': 'Municipio',\n",
        "    'Modalidade de Ensino': 'Modalidade_Educacional',\n",
        "    'Tipo de Curso': 'Nivel_Educacional',\n",
        "    'Tipo de Oferta': 'Modalidade_de_Curso',\n",
        "    'Nome de Curso': 'Nome_de_Curso',\n",
        "    'Total de Inscritos': 'Total_de_Inscritos',\n",
        "    'Região': 'Regiao',\n",
        "    'Renda Familiar': 'Renda_Familiar',\n",
        "    'Sexo': 'Sexo',\n",
        "    'Unidade de Ensino': 'Campus_do_IFRN',\n",
        "    'Vagas Extraordinárias AC': 'Vagas_Extraordinarias_AC',\n",
        "    'Vagas Extraordinárias l1': 'Vagas_Extraordinarias_l1',\n",
        "    'Vagas Extraordinárias l10': 'Vagas_Extraordinarias_l10',\n",
        "    'Vagas Extraordinárias l13': 'Vagas_Extraordinarias_l13',\n",
        "    'Vagas Extraordinárias l14': 'Vagas_Extraordinarias_l14',\n",
        "    'Vagas Extraordinárias l2': 'Vagas_Extraordinarias_l2',\n",
        "    'Vagas Extraordinárias l5': 'Vagas_Extraordinarias_l5',\n",
        "    'Vagas Extraordinárias l6': 'Vagas_Extraordinarias_l6',\n",
        "    'Vagas Extraordinárias l9': 'Vagas_Extraordinarias_l9',\n",
        "    'Vagas Ofertadas': 'Vagas_Ofertadas',\n",
        "    'Vagas Regulares AC': 'Vagas_Regulares_AC',\n",
        "    'Vagas Regulares l1': 'Vagas_Regulares_l1',\n",
        "    'Vagas Regulares l10': 'Vagas_Regulares_l10',\n",
        "    'Vagas Regulares l13': 'Vagas_Regulares_l13',\n",
        "    'Vagas Regulares l14': 'Vagas_Regulares_l14',\n",
        "    'Vagas Regulares l2': 'Vagas_Regulares_l2',\n",
        "    'Vagas Regulares l5': 'Vagas_Regulares_l5',\n",
        "    'Vagas Regulares l6': 'Vagas_Regulares_l6',\n",
        "    'Vagas Regulares l9': 'Vagas_Regulares_l9',\n",
        "})\n",
        "\n",
        "\n",
        "# Correção de parâmetros na coluna 'Eixo Tecnológico'\n",
        "replacements_eixo = {\n",
        "    'GESTÃO E NEGÓCIOS': 'Gestão e Negócios',\n",
        "    'DESENVOLVIMENTO EDUCACIONAL E SOCIAL': 'Desenvolvimento Educacional e Social'\n",
        "}\n",
        "if 'Eixo_Tecnologico' in df_tratado.columns:\n",
        "  df_tratado['Eixo_Tecnologico'] = df_tratado['Eixo_Tecnologico'].replace(replacements_eixo)\n",
        "\n",
        "# Correção de parâmetros na coluna 'Subeixo Tecnológico'\n",
        "replacements_subeixo = {\n",
        "    'AMBIENTE E SAÚDE': 'Ambiente e Saúde'\n",
        "}\n",
        "if 'Subeixo_Tecnologico' in df_tratado.columns:\n",
        "  df_tratado['Subeixo_Tecnologico'] = df_tratado['Subeixo_Tecnologico'].replace(replacements_subeixo) # Changed from replacements_eixo\n",
        "\n",
        "# Correção de parâmetros em 'Campus_do_IFRN'\n",
        "replacements_unidade = {\n",
        "    'Campus Ceará-mirim': 'Ceará-Mirim',\n",
        "    'Campus Pau Dos Ferros': 'Pau dos Ferros'\n",
        "}\n",
        "if 'Campus_do_IFRN' in df_tratado.columns:\n",
        "    # Aplicação de regras\n",
        "    df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].replace(replacements_unidade)\n",
        "    # Remoção d palavras 'Campus ' e 'Campus Avançado ' de 'Campus_do_IFRN'\n",
        "    df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].str.replace('Campus Avançado ', '', regex=False)\n",
        "    df_tratado['Campus_do_IFRN'] = df_tratado['Campus_do_IFRN'].str.replace('Campus ', '', regex=False)\n",
        "\n",
        "# Renomeia parâmetros na coluna 'Forma de ingresso'\n",
        "replacements_ingresso = {\n",
        "    'AC': 'AC: Ampla Concorrência',\n",
        "    'l1': 'L1: Renda Baixa',\n",
        "    'l2': 'L2: Renda Baixa + PPI',\n",
        "    'l5': 'L5: Ensino Médio em Escolas Pública',\n",
        "    'l6': 'L6: PPI (Pretos, Pardos e Indígenas)',\n",
        "    'l9': 'L9: Renda Baixa + PcD',\n",
        "    'l10': 'L10: Renda Baixa + PPI + PcD',\n",
        "    'l13': 'L13: PcD (Independente de Renda)',\n",
        "    'l14': 'L14: PcD + PPI (Independente de Renda)',\n",
        "    'LB_EP': 'LB_EP: Baixa Renda + Ensino Médio em Escola Pública',\n",
        "    'LB_PCD': 'LB_PCD: Baixa Renda + PcD',\n",
        "    'LB_PPI': 'LB_PPI: Baixa Renda + PPI',\n",
        "    'LB_Q': 'LB_Q: Baixa Renda + Quilombolas + Ensino Médio em Escola Pública',\n",
        "    'LI_EP': 'LI_EP: Ensino Médio em Escola Pública (Independentemente da Renda)',\n",
        "    'LI_PCD': 'LI_PCD: PcD (Independente de Renda)',\n",
        "    'LI_PPI': 'LI_PPI: PPI (Independente de Renda)',\n",
        "    'LI_Q': 'LI_Q: Quilombolas + Ensino Médio em Escola Pública (Independente da Renda)',\n",
        "    'nan': 'NI: Não informado'\n",
        "}\n",
        "\n",
        "if 'Forma_de_Ingresso' in df_tratado.columns:\n",
        "    # Certifica que a coluna seja do tipo objeto para lidar com substituições de strings, incluindo 'nan'\n",
        "    df_tratado['Forma_de_Ingresso'] = df_tratado['Forma_de_Ingresso'].astype(str).replace(replacements_ingresso)\n",
        "\n",
        "# # Adiciona padronização em parâmetro da coluna 'Categoria da Situação'\n",
        "replacements_categoria_situacao = {\n",
        "    'Em curso': 'Em Curso',\n",
        "}\n",
        "if 'Categoria_da_Situacao' in df_tratado.columns:\n",
        "    df_tratado['Categoria_da_Situacao'] = df_tratado['Categoria_da_Situacao'].replace(replacements_categoria_situacao)\n",
        "\n",
        "# Reposiona 'Cursos' depois de 'Nome_de_Curso'\n",
        "if 'Cursos' in df_tratado.columns and 'Nome_de_Curso' in df_tratado.columns:\n",
        "    cols = df_tratado.columns.tolist()\n",
        "    # Find the index of 'Cursos' and 'Nome_de_Curso'\n",
        "    cursos_index = cols.index('Cursos')\n",
        "    nome_curso_index = cols.index('Nome_de_Curso')\n",
        "\n",
        "    # Remove 'Cursos' from its current position\n",
        "    cursos_col = cols.pop(cursos_index)\n",
        "\n",
        "    # Insert 'Cursos' after 'Nome_de_Curso'\n",
        "    cols.insert(nome_curso_index + 1, cursos_col)\n",
        "\n",
        "    # Reindex the DataFrame with the new column order\n",
        "    df_tratado = df_tratado[cols]\n",
        "    print(\"Coluna 'Cursos' reposicionada após 'Nome_de_Curso'.\")\n",
        "elif 'Cursos' not in df_tratado.columns:\n",
        "    print(\"Aviso: Coluna 'Cursos' não encontrada para reposicionamento.\")\n",
        "elif 'Nome_de_Curso' not in df_tratado.columns:\n",
        "    print(\"Aviso: Coluna 'Nome_de_Curso' não encontrada para servir de referência.\")\n",
        "\n",
        "\n",
        "# Exibir informações sobre as colunas e tipos de dados de 'df_tratado'\n",
        "if df_tratado is not None:\n",
        "    print(\"Informações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "sbLxpB5GtpqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Verificação de valores únicos para as colunas indicadas em `df_tratado`**\n",
        "if df_tratado is not None:\n",
        "    columns_to_list = ['Campus_do_IFRN', 'Categoria_da_Situacao', 'Eixo_Tecnologico', 'Forma_de_Ingresso']\n",
        "\n",
        "    for col in columns_to_list:\n",
        "        if col in df_tratado.columns:\n",
        "            # Converte a coluna para o tipo 'string' antes de classificar valores exclusivos\n",
        "            unique_values = sorted(df_tratado[col].astype(str).unique())\n",
        "            print(f\"Valores únicos em ordem alfabética na coluna '{col}':\")\n",
        "            for value in unique_values:\n",
        "                print(f\"- {value}\")\n",
        "            print(\"-\" * 20) # Separador\n",
        "        else:\n",
        "            print(f\"A coluna '{col}' não foi encontrada no DataFrame 'df_tratado'.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "_So1hD4dltnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Tratamento dos tipos de dados de colunas selecionadas em `df_tratado`**\n",
        "def converter_tipos_de_dados(df: pd.DataFrame, colunas_para_int: list, colunas_para_str: list, colunas_para_data: dict, colunas_para_float: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converte os tipos de dados de colunas especificadas em um DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): O DataFrame a ser modificado.\n",
        "        colunas_para_int (list): Uma lista de nomes de colunas para converter para o tipo Int64 (inteiro que suporta nulos).\n",
        "        colunas_para_str (list): Uma lista de nomes de colunas para converter para o tipo string.\n",
        "        colunas_para_data (dict): Um dicionário onde as chaves são os nomes das colunas\n",
        "                                  para converter para data e os valores são os formatos esperados (ex: {'coluna': '%d/%m/%Y'}).\n",
        "        colunas_para_float (dict): Um dicionário onde as chaves são os nomes das colunas\n",
        "                                   para converter para float e os valores são os caracteres a serem substituídos\n",
        "                                   antes da conversão (ex: {'coluna': ','}).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Uma cópia do DataFrame com os tipos de dados convertidos.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"Erro: O objeto fornecido não é um DataFrame do pandas.\")\n",
        "        return None\n",
        "\n",
        "    # Cria uma cópia para evitar modificar o DataFrame original\n",
        "    df_processado = df.copy()\n",
        "\n",
        "    print(\"--- Iniciando conversão de tipos de dados ---\")\n",
        "\n",
        "    # --- Conversão para Inteiro (Int64) ---\n",
        "    print(\"\\nConvertendo colunas para Inteiro:\")\n",
        "    for col in colunas_para_int:\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                # Converte para float primeiro para lidar com NaN, depois para Int64 (que suporta NaN)\n",
        "                df_processado[col] = pd.to_numeric(df_processado[col], errors='coerce').astype('Int64')\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para Int64.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}': {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    # --- Conversão para String ---\n",
        "    print(\"\\nConvertendo colunas para String:\")\n",
        "    for col in colunas_para_str:\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                # Converte valores nulos para uma string vazia antes de mudar o tipo\n",
        "                df_processado[col] = df_processado[col].fillna('').astype(str)\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para string.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}': {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    # --- Conversão para Data ---\n",
        "    print(\"\\nConvertendo colunas para Data:\")\n",
        "    for col, fmt in colunas_para_data.items():\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                df_processado[col] = pd.to_datetime(df_processado[col], format=fmt, errors='coerce')\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para data com formato '{fmt}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}' para data: {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    # --- Conversão para Float (float64) ---\n",
        "    print(\"\\nConvertendo colunas para Float:\")\n",
        "    for col, char_to_replace in colunas_para_float.items():\n",
        "        if col in df_processado.columns:\n",
        "            try:\n",
        "                # Substitui o caractere e converte para numérico\n",
        "                df_processado[col] = df_processado[col].astype(str).str.replace(char_to_replace, '.', regex=False)\n",
        "                df_processado[col] = pd.to_numeric(df_processado[col], errors='coerce').astype('float64')\n",
        "                print(f\"  ✔ Coluna '{col}' convertida para float64.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao converter a coluna '{col}' para float64: {e}\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "    print(\"\\n--- Conversão concluída ---\")\n",
        "    return df_processado\n",
        "\n",
        "# Define as listas de colunas\n",
        "colunas_int = [\n",
        "    'Carga_Horaria_Minima', 'Idade', 'Total_de_Inscritos',\n",
        "    'Vagas_Extraordinarias_AC', 'Vagas_Extraordinarias_l1', 'Vagas_Extraordinarias_l10',\n",
        "    'Vagas_Extraordinarias_l13', 'Vagas_Extraordinarias_l14', 'Vagas_Extraordinarias_l2',\n",
        "    'Vagas_Extraordinarias_l5', 'Vagas_Extraordinarias_l6', 'Vagas_Extraordinarias_l9',\n",
        "    'Vagas_Ofertadas', 'Vagas_Regulares_AC', 'Vagas_Regulares_l1', 'Vagas_Regulares_l10',\n",
        "    'Vagas_Regulares_l13', 'Vagas_Regulares_l14', 'Vagas_Regulares_l2', 'Vagas_Regulares_l5',\n",
        "    'Vagas_Regulares_l6', 'Vagas_Regulares_l9'\n",
        "]\n",
        "colunas_str = [\n",
        "    'Cod_Unidade',\n",
        "    'Co_Inst',\n",
        "    'Codigo_da_Matricula',\n",
        "    'Codigo_da_Unidade_de_Ensino_-_SISTEC',\n",
        "    'Codigo_do_Ciclo_Matricula',\n",
        "    'Codigo_do_Municipio_com_DV',\n",
        "    'Eixo_Tecnologico',\n",
        "    'Forma_de_Ingresso',\n",
        "    'Instituicao',\n",
        "    'Modalidade_Educacional',\n",
        "    'Nivel_Educacional',\n",
        "    'Modalidade_de_Curso',\n",
        "    'Matricula_Atendida',\n",
        "    'Renda_Familiar',\n",
        "    'Subeixo_Tecnologico',\n",
        "    'Categoria_da_Situacao',\n",
        "]\n",
        "colunas_data = {\n",
        "    'Data_de_Fim_Previsto_do_Ciclo': '%d/%m/%Y',\n",
        "    'Data_de_Inicio_do_Ciclo': '%d/%m/%Y',\n",
        "    'Data_de_Ocorrencia_da_Matricula': '%d/%m/%Y'\n",
        "}\n",
        "\n",
        "colunas_float = {\n",
        "    'Fator_Esforco_Curso': ',' # Substituir vírgula por ponto\n",
        "}\n",
        "\n",
        "# Chama a função e armazena o resultado em um novo DataFrame\n",
        "df_tratado = converter_tipos_de_dados(df_tratado, colunas_int, colunas_str, colunas_data, colunas_float)\n",
        "\n",
        "# Verifica os tipos de dados após a conversão (opcional)\n",
        "if df_tratado is not None:\n",
        "  print(\"\\nTipos de dados após a conversão:\")\n",
        "  colunas_convertidas = colunas_int + colunas_str + list(colunas_data.keys()) + list(colunas_float.keys())\n",
        "  print(df_tratado[colunas_convertidas].dtypes)"
      ],
      "metadata": {
        "id": "UOm4infX6cc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Exibe informações sobre as colunas e os tipos de dados de `df_tratado`**\n",
        "if df_tratado is not None:\n",
        "    print(\"Informações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "YsF-NGGwRT_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Exibe estatísticas básicas para colunas selecionadas de `df_tratado`**\n",
        "if df_tratado is not None and not df_tratado.empty:\n",
        "    columns_to_describe = [\n",
        "        'Idade',\n",
        "        'Total_de_Inscritos',\n",
        "        'Vagas_Extraordinarias_AC',\n",
        "        'Vagas_Extraordinarias_l1',\n",
        "        'Vagas_Extraordinarias_l10',\n",
        "        'Vagas_Extraordinarias_l13',\n",
        "        'Vagas_Extraordinarias_l14',\n",
        "        'Vagas_Extraordinarias_l2',\n",
        "        'Vagas_Extraordinarias_l5',\n",
        "        'Vagas_Extraordinarias_l6',\n",
        "        'Vagas_Extraordinarias_l9',\n",
        "        'Vagas_Ofertadas',\n",
        "        'Vagas_Regulares_AC',\n",
        "        'Vagas_Regulares_l1',\n",
        "        'Vagas_Regulares_l10',\n",
        "        'Vagas_Regulares_l13',\n",
        "        'Vagas_Regulares_l14',\n",
        "        'Vagas_Regulares_l2',\n",
        "        'Vagas_Regulares_l5',\n",
        "        'Vagas_Regulares_l6',\n",
        "        'Vagas_Regulares_l9',\n",
        "    ]\n",
        "\n",
        "    # Filtrar apenas colunas existentes no DataFrame\n",
        "    existing_columns = [col for col in columns_to_describe if col in df_tratado.columns]\n",
        "\n",
        "    if existing_columns:\n",
        "        print(\"Estatísticas Descritivas para Colunas Selecionadas:\")\n",
        "        # Tente descrever colunas numéricas\n",
        "        try:\n",
        "            # Definir opção de formato 'float' para exibição\n",
        "            pd.options.display.float_format = '{:,.4f}'.format\n",
        "            display(df_tratado[existing_columns].describe())\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao gerar estatísticas para colunas numéricas selecionadas: {e}\")\n",
        "\n",
        "        # Tente descrever colunas de objetos\n",
        "        try:\n",
        "            object_columns = df_tratado[existing_columns].select_dtypes(include='object').columns.tolist()\n",
        "            if object_columns:\n",
        "                 print(\"\\nEstatísticas Descritivas para Colunas de Objeto Selecionadas:\")\n",
        "                 display(df_tratado[object_columns].describe())\n",
        "        except Exception as e:\n",
        "             print(f\"Erro ao gerar estatísticas para colunas de objetos selecionadas: {e}\")\n",
        "\n",
        "        # Remova a opção de formato float após a exibição, se desejar, para não afetar outras exibições\n",
        "        pd.options.display.float_format = None\n",
        "\n",
        "    else:\n",
        "        print(\"Nenhuma das colunas selecionadas foi encontrada no DataFrame.\")\n",
        "\n",
        "elif df_tratado is not None and df_tratado.empty:\n",
        "    print(\"O DataFrame 'df_tratado' está vazio, nenhuma estatística para exibir.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado, execute os passos anteriores.\")"
      ],
      "metadata": {
        "id": "qWHcOrazU0ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Análise Estatística de _Outliers_ em `df_tratado`**\n",
        "\n",
        "def analyze_outliers_iqr(df: pd.DataFrame, columns: list):\n",
        "    \"\"\"\n",
        "    Calcula e exibe os limites superior e inferior para identificação de outliers\n",
        "    usando o método IQR para as colunas numéricas especificadas.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): O DataFrame a ser analisado.\n",
        "        columns (list): Uma lista de nomes de colunas numéricas para análise.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(\"DataFrame vazio ou não encontrado. Nenhuma análise de outlier realizada.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Análise de Outliers (Método IQR) ---\")\n",
        "\n",
        "    for col in columns:\n",
        "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # Remove valores nulos para o cálculo do IQR\n",
        "            data = df[col].dropna()\n",
        "\n",
        "            if len(data) < 4: # Precisa de pelo menos 4 pontos para calcular Q1, Q3 e IQR\n",
        "                print(f\"  - Coluna '{col}': Não há dados suficientes para análise de outlier.\")\n",
        "                continue\n",
        "\n",
        "            Q1 = data.quantile(0.25)\n",
        "            Q3 = data.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            print(f\"\\n--- Coluna: '{col}' ---\")\n",
        "            print(f\"  Q1 (25º Percentil): {Q1:,.4f}\")\n",
        "            print(f\"  Q3 (75º Percentil): {Q3:,.4f}\")\n",
        "            print(f\"  IQR (Intervalo Interquartil): {IQR:,.4f}\")\n",
        "            print(f\"  Limite Inferior (Q1 - 1.5*IQR): {lower_bound:,.4f}\")\n",
        "            print(f\"  Limite Superior (Q3 + 1.5*IQR): {upper_bound:,.4f}\")\n",
        "\n",
        "            # Opcional: Contar e exibir outliers\n",
        "            outliers_lower = data[data < lower_bound]\n",
        "            outliers_upper = data[data > upper_bound]\n",
        "            total_outliers = len(outliers_lower) + len(outliers_upper)\n",
        "\n",
        "            print(f\"  Número de potenciais outliers abaixo do limite inferior: {len(outliers_lower):,}\")\n",
        "            print(f\"  Número de potenciais outliers acima do limite superior: {len(outliers_upper):,}\")\n",
        "            print(f\"  Total de potenciais outliers: {total_outliers:,}\")\n",
        "\n",
        "        elif col in df.columns:\n",
        "             print(f\"  - Coluna '{col}': Não é um tipo de dado numérico. Pulando análise de outlier.\")\n",
        "        else:\n",
        "            print(f\"  - AVISO: Coluna '{col}' não encontrada no DataFrame.\")\n",
        "\n",
        "# Lista de colunas para analisar outliers\n",
        "columns_for_outlier_analysis = [\n",
        "    'Idade',\n",
        "    'Total_de_Inscritos',\n",
        "    'Vagas_Extraordinarias_AC',\n",
        "    'Vagas_Extraordinarias_l1',\n",
        "    'Vagas_Extraordinarias_l10',\n",
        "    'Vagas_Extraordinarias_l13',\n",
        "    'Vagas_Extraordinarias_l14',\n",
        "    'Vagas_Extraordinarias_l2',\n",
        "    'Vagas_Extraordinarias_l5',\n",
        "    'Vagas_Extraordinarias_l6',\n",
        "    'Vagas_Extraordinarias_l9',\n",
        "    'Vagas_Ofertadas',\n",
        "    'Vagas_Regulares_AC',\n",
        "    'Vagas_Regulares_l1',\n",
        "    'Vagas_Regulares_l10',\n",
        "    'Vagas_Regulares_l13',\n",
        "    'Vagas_Regulares_l14',\n",
        "    'Vagas_Regulares_l2',\n",
        "    'Vagas_Regulares_l5',\n",
        "    'Vagas_Regulares_l6',\n",
        "    'Vagas_Regulares_l9',\n",
        "]\n",
        "\n",
        "# Executa a análise\n",
        "analyze_outliers_iqr(df_tratado, columns_for_outlier_analysis)"
      ],
      "metadata": {
        "id": "03Z99XBZFpVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir informações sobre as colunas e tipos de dados de 'df_tratado'\n",
        "if df_tratado is not None:\n",
        "    print(\"Informações sobre as colunas e tipos de dados do DataFrame 'df_tratado':\")\n",
        "    df_tratado.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_tratado' não encontrado. Execute as etapas anteriores.\")"
      ],
      "metadata": {
        "id": "_mUaL9ExVh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **ETAPA 8: Exportação de em `df_tratado` para tabela no BigQuery**\n",
        "\n",
        "# --- Configurações de Destino do BigQuery ---\n",
        "import pandas_gbq\n",
        "\n",
        "project_id = \"pnp-data-extraction\" # Substitua pelo ID do seu projeto\n",
        "dataset_id = \"pnp_dados_IFRN\"      # Nome do conjunto de dados\n",
        "\n",
        "# O nome da tabela no BigQuery será o mesmo nome da tabela da PNP\n",
        "table_id = f\"df_{table_name}\"\n",
        "destination_table = f\"{dataset_id}.{table_id}\"\n",
        "\n",
        "# --- Execução da Exportação ---\n",
        "if df_tratado is not None and not df_tratado.empty:\n",
        "    print(f\"Iniciando a exportação de {len(df_tratado):,} linhas para o BigQuery...\")\n",
        "    print(f\"Destino: {project_id}.{destination_table}\")\n",
        "\n",
        "    # --- Renomear colunas para serem compatíveis com BigQuery ---\n",
        "    # Substitui espaços e '/' por '_'\n",
        "    df_tratado_bq = df_tratado.copy()\n",
        "    df_tratado_bq.columns = (df_tratado_bq.columns\n",
        "            .str.replace(' ', '_', regex=False)\n",
        "            .str.replace('/', '_', regex=False)\n",
        "            .str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "        )\n",
        "    print(\"Nomes das colunas padronizados para o BigQuery.\")\n",
        "\n",
        "    # --- Definição do Esquema da Tabela ---\n",
        "    # Defina o nome e o tipo de cada coluna que você quer controlar.\n",
        "    # Tipos comuns: 'STRING', 'INTEGER', 'FLOAT', 'NUMERIC', 'BOOLEAN', 'TIMESTAMP', 'DATE'.\n",
        "    table_schema = [\n",
        "        {\"name\": \"Co_Inst\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Instituicao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Cod_Unidade\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Campus_do_IFRN\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_da_Unidade_de_Ensino_-_SISTEC\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_do_Ciclo_Matricula\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_da_Matricula\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Municipio\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Codigo_do_Municipio_com_DV\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"UF\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Regiao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Modalidade_Educacional\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Nivel_Educacional\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Modalidade_de_Curso\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Eixo_Tecnologico\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Subeixo_Tecnologico\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Nome_de_Curso\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Cursos\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Turno\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Carga_Horaria\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Carga_Horaria_Minima\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Fonte_de_Financiamento\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Fator_Esforco_Curso\", \"type\": \"FLOAT\"},\n",
        "        {\"name\": \"Sexo\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Cor_Raça\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Idade\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Faixa_Etaria\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Renda_Familiar\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Ano\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Matricula_Atendida\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Situacao_de_Matricula\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Categoria_da_Situacao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Data_de_Inicio_do_Ciclo\", \"type\": \"DATETIME\"},\n",
        "        {\"name\": \"Data_de_Fim_Previsto_do_Ciclo\", \"type\": \"DATETIME\"},\n",
        "        {\"name\": \"Data_de_Ocorrencia_da_Matricula\", \"type\": \"DATETIME\"},\n",
        "        {\"name\": \"Mes_De_Ocorrencia_da_Situacao\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Total_de_Inscritos\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Forma_de_Ingresso\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"Vagas_Ofertadas\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_AC\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l1\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l2\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l5\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l6\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l9\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l10\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l13\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Regulares_l14\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_AC\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l1\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l2\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l5\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l6\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l9\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l10\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l13\", \"type\": \"INTEGER\"},\n",
        "        {\"name\": \"Vagas_Extraordinarias_l14\", \"type\": \"INTEGER\"}\n",
        "    ]\n",
        "    # -----------------------------------------------------------------\n",
        "\n",
        "    # Envia o DataFrame para o BigQuery com o esquema especificado\n",
        "    try:\n",
        "        # Usando a função recomendada pandas_gbq.to_gbq\n",
        "        pandas_gbq.to_gbq(\n",
        "            df_tratado_bq, # Exporta o DataFrame com colunas renomeadas\n",
        "            destination_table=destination_table,\n",
        "            project_id=project_id,\n",
        "            if_exists='replace',  # Opções: 'fail', 'replace', 'append'\n",
        "            progress_bar=True\n",
        "        )\n",
        "        print(f\"\\n✔ DataFrame exportado com sucesso para o BigQuery!\")\n",
        "        print(f\"Link para a tabela: https://console.cloud.google.com/bigquery?project={project_id}&ws=!1m5!1m4!4m3!1s{project_id}!2s{dataset_id}!3s{table_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERRO durante a exportação para o BigQuery: {e}\")\n",
        "\n",
        "elif df_tratado is not None and df_tratado.empty:\n",
        "    print(\"AVISO: O DataFrame final está vazio. Nenhuma exportação foi realizada.\")\n",
        "else:\n",
        "    print(\"❌ ERRO: O DataFrame a ser exportado não foi encontrado. A exportação foi cancelada.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "H7wYuE0Dqco4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}